#!/usr/bin/env python3
"""
MLOps Sentiment Analysis - Comprehensive Report Generator
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞
"""

import json
import yaml
import argparse
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import numpy as np
from jinja2 import Template
import base64
from io import BytesIO

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BenchmarkReportGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –ø–æ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥—É"""
    
    def __init__(self, results_dir: str):
        self.results_dir = Path(results_dir)
        self.benchmark_data = []
        self.cost_data = []
        self.resource_data = []
        
    def load_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        consolidated_file = self.results_dir / "consolidated_results.json"
        if consolidated_file.exists():
            with open(consolidated_file, 'r', encoding='utf-8') as f:
                self.benchmark_data = json.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        cost_file = self.results_dir / "cost_analysis.json"
        if cost_file.exists():
            with open(cost_file, 'r', encoding='utf-8') as f:
                cost_analysis = json.load(f)
                self.cost_data = cost_analysis.get('analyses', [])
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ä–µ—Å—É—Ä—Å–∞—Ö
        for resource_file in self.results_dir.glob("resource_metrics_*.json"):
            with open(resource_file, 'r', encoding='utf-8') as f:
                resource_data = json.load(f)
                instance_name = resource_file.stem.replace('resource_metrics_', '')
                self.resource_data.append({
                    'instance': instance_name,
                    'metrics': resource_data
                })
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(self.benchmark_data)} –±–µ–Ω—á–º–∞—Ä–∫–æ–≤, "
                   f"{len(self.cost_data)} –∞–Ω–∞–ª–∏–∑–æ–≤ —Å—Ç–æ–∏–º–æ—Å—Ç–∏, "
                   f"{len(self.resource_data)} –Ω–∞–±–æ—Ä–æ–≤ –º–µ—Ç—Ä–∏–∫ —Ä–µ—Å—É—Ä—Å–æ–≤")
    
    def create_performance_comparison_chart(self) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        if not self.benchmark_data:
            return ""
        
        df = pd.DataFrame(self.benchmark_data)
        
        # –°–æ–∑–¥–∞–µ–º subplot —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('RPS –ø–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞–º', '–õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∏–Ω—Å—Ç–∞–Ω—Å–∞–º', 
                          '–ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏', '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: RPS
        fig.add_trace(
            go.Bar(
                x=df['instance_type'],
                y=df['requests_per_second'],
                name='RPS',
                marker_color='lightblue'
            ),
            row=1, col=1
        )
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        fig.add_trace(
            go.Bar(
                x=df['instance_type'],
                y=df['avg_latency'],
                name='Avg Latency (ms)',
                marker_color='lightcoral'
            ),
            row=1, col=2
        )
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        latency_percentiles = ['p50_latency', 'p90_latency', 'p95_latency', 'p99_latency']
        for percentile in latency_percentiles:
            if percentile in df.columns:
                fig.add_trace(
                    go.Scatter(
                        x=df['instance_type'],
                        y=df[percentile],
                        mode='lines+markers',
                        name=percentile.replace('_latency', '').upper(),
                        line=dict(width=2)
                    ),
                    row=2, col=1
                )
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (RPS/—Å—Ç–æ–∏–º–æ—Å—Ç—å)
        if self.cost_data:
            cost_df = pd.DataFrame(self.cost_data)
            efficiency = cost_df['requests_per_second'] / cost_df['cost_per_hour']
            fig.add_trace(
                go.Bar(
                    x=cost_df['instance_name'],
                    y=efficiency,
                    name='RPS/Cost Efficiency',
                    marker_color='lightgreen'
                ),
                row=2, col=2
            )
        
        fig.update_layout(
            height=800,
            title_text="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤",
            showlegend=True
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ HTML
        html_str = fig.to_html(include_plotlyjs='cdn')
        return html_str
    
    def create_cost_analysis_chart(self) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
        if not self.cost_data:
            return ""
        
        df = pd.DataFrame(self.cost_data)
        
        # –°–æ–∑–¥–∞–µ–º bubble chart
        fig = go.Figure()
        
        # –†–∞–∑–º–µ—Ä –ø—É–∑—ã—Ä—å–∫–æ–≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω –æ–±—â–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        sizes = df['total_efficiency_score'] * 50
        
        fig.add_trace(go.Scatter(
            x=df['cost_per_1000_predictions'],
            y=df['requests_per_second'],
            mode='markers',
            marker=dict(
                size=sizes,
                color=df['avg_latency_ms'],
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="–õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (ms)"),
                line=dict(width=2, color='DarkSlateGrey')
            ),
            text=df['instance_name'],
            textposition="middle center",
            hovertemplate='<b>%{text}</b><br>' +
                         '–°—Ç–æ–∏–º–æ—Å—Ç—å 1000 –ø—Ä–µ–¥.: $%{x:.4f}<br>' +
                         'RPS: %{y:.1f}<br>' +
                         '–õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: %{marker.color:.1f}ms<br>' +
                         '<extra></extra>'
        ))
        
        fig.update_layout(
            title='–ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏<br><sub>–†–∞–∑–º–µ—Ä –ø—É–∑—ã—Ä—å–∫–∞ = –æ–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</sub>',
            xaxis_title='–°—Ç–æ–∏–º–æ—Å—Ç—å 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (USD)',
            yaxis_title='–ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É (RPS)',
            width=800,
            height=600
        )
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def create_resource_utilization_chart(self) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        if not self.resource_data:
            return ""
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('CPU Utilization', 'Memory Utilization', 
                          'GPU Utilization', 'Resource Summary'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )
        
        colors = px.colors.qualitative.Set1
        
        for i, resource_set in enumerate(self.resource_data):
            instance = resource_set['instance']
            metrics = resource_set['metrics']
            
            if not metrics:
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
            timestamps = [m.get('timestamp', 0) for m in metrics]
            cpu_usage = [m.get('cpu_percent', 0) for m in metrics]
            memory_usage = [m.get('memory_percent', 0) for m in metrics]
            gpu_usage = [m.get('gpu_utilization', 0) for m in metrics if m.get('gpu_utilization') is not None]
            
            color = colors[i % len(colors)]
            
            # CPU –≥—Ä–∞—Ñ–∏–∫
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(cpu_usage))),
                    y=cpu_usage,
                    mode='lines',
                    name=f'{instance} CPU',
                    line=dict(color=color, width=2)
                ),
                row=1, col=1
            )
            
            # Memory –≥—Ä–∞—Ñ–∏–∫
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(memory_usage))),
                    y=memory_usage,
                    mode='lines',
                    name=f'{instance} Memory',
                    line=dict(color=color, width=2, dash='dash')
                ),
                row=1, col=2
            )
            
            # GPU –≥—Ä–∞—Ñ–∏–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ)
            if gpu_usage:
                fig.add_trace(
                    go.Scatter(
                        x=list(range(len(gpu_usage))),
                        y=gpu_usage,
                        mode='lines',
                        name=f'{instance} GPU',
                        line=dict(color=color, width=2, dash='dot')
                    ),
                    row=2, col=1
                )
        
        # –°–≤–æ–¥–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        avg_data = []
        for resource_set in self.resource_data:
            instance = resource_set['instance']
            metrics = resource_set['metrics']
            
            if metrics:
                avg_cpu = np.mean([m.get('cpu_percent', 0) for m in metrics])
                avg_memory = np.mean([m.get('memory_percent', 0) for m in metrics])
                gpu_values = [m.get('gpu_utilization', 0) for m in metrics if m.get('gpu_utilization') is not None]
                avg_gpu = np.mean(gpu_values) if gpu_values else 0
                
                avg_data.append({
                    'instance': instance,
                    'cpu': avg_cpu,
                    'memory': avg_memory,
                    'gpu': avg_gpu
                })
        
        if avg_data:
            avg_df = pd.DataFrame(avg_data)
            
            fig.add_trace(
                go.Bar(
                    x=avg_df['instance'],
                    y=avg_df['cpu'],
                    name='Avg CPU %',
                    marker_color='lightblue'
                ),
                row=2, col=2
            )
            
            fig.add_trace(
                go.Bar(
                    x=avg_df['instance'],
                    y=avg_df['memory'],
                    name='Avg Memory %',
                    marker_color='lightcoral'
                ),
                row=2, col=2
            )
            
            # GPU —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            if avg_df['gpu'].sum() > 0:
                fig.add_trace(
                    go.Bar(
                        x=avg_df['instance'],
                        y=avg_df['gpu'],
                        name='Avg GPU %',
                        marker_color='lightgreen'
                    ),
                    row=2, col=2
                )
        
        fig.update_layout(
            height=800,
            title_text="–£—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–æ –≤—Ä–µ–º—è –±–µ–Ω—á–º–∞—Ä–∫–∞",
            showlegend=True
        )
        
        return fig.to_html(include_plotlyjs='cdn')
    
    def generate_summary_statistics(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        stats = {
            'total_benchmarks': len(self.benchmark_data),
            'total_instances_tested': len(set(b['instance_type'] for b in self.benchmark_data)),
            'best_performance': {},
            'cost_efficiency': {},
            'resource_usage': {}
        }
        
        if self.benchmark_data:
            # –õ—É—á—à–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            best_rps = max(self.benchmark_data, key=lambda x: x['requests_per_second'])
            best_latency = min(self.benchmark_data, key=lambda x: x['avg_latency'])
            
            stats['best_performance'] = {
                'highest_rps': {
                    'instance': best_rps['instance_type'],
                    'value': best_rps['requests_per_second']
                },
                'lowest_latency': {
                    'instance': best_latency['instance_type'],
                    'value': best_latency['avg_latency']
                }
            }
        
        if self.cost_data:
            # –°—Ç–æ–∏–º–æ—Å—Ç–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            best_cost = min(self.cost_data, key=lambda x: x['cost_per_1000_predictions'])
            best_efficiency = max(self.cost_data, key=lambda x: x['total_efficiency_score'])
            
            stats['cost_efficiency'] = {
                'most_cost_effective': {
                    'instance': best_cost['instance_name'],
                    'cost_per_1000': best_cost['cost_per_1000_predictions']
                },
                'best_overall_efficiency': {
                    'instance': best_efficiency['instance_name'],
                    'score': best_efficiency['total_efficiency_score']
                }
            }
        
        return stats
    
    def generate_html_report(self, output_path: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞"""
        logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞...")
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏
        performance_chart = self.create_performance_comparison_chart()
        cost_chart = self.create_cost_analysis_chart()
        resource_chart = self.create_resource_utilization_chart()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = self.generate_summary_statistics()
        
        # HTML —à–∞–±–ª–æ–Ω
        html_template = """
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLOps Sentiment Analysis - Benchmark Report</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
        }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .stat-card {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }
        .stat-value {
            font-size: 2em;
            font-weight: bold;
            margin: 10px 0;
        }
        .chart-container {
            margin: 30px 0;
            border: 1px solid #ddd;
            border-radius: 5px;
            overflow: hidden;
        }
        .recommendations {
            background-color: #e8f5e8;
            border-left: 5px solid #27ae60;
            padding: 20px;
            margin: 20px 0;
        }
        .warning {
            background-color: #fff3cd;
            border-left: 5px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        .footer {
            text-align: center;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üöÄ MLOps Sentiment Analysis<br>Benchmark Report</h1>
        
        <div class="stats-grid">
            <div class="stat-card">
                <div>–í—Å–µ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤</div>
                <div class="stat-value">{{ stats.total_benchmarks }}</div>
            </div>
            <div class="stat-card">
                <div>–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤</div>
                <div class="stat-value">{{ stats.total_instances_tested }}</div>
            </div>
            {% if stats.best_performance.highest_rps %}
            <div class="stat-card">
                <div>–õ—É—á—à–∏–π RPS</div>
                <div class="stat-value">{{ "%.1f"|format(stats.best_performance.highest_rps.value) }}</div>
                <div>{{ stats.best_performance.highest_rps.instance }}</div>
            </div>
            {% endif %}
            {% if stats.cost_efficiency.most_cost_effective %}
            <div class="stat-card">
                <div>–°–∞–º—ã–π —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π</div>
                <div class="stat-value">${{ "%.4f"|format(stats.cost_efficiency.most_cost_effective.cost_per_1000) }}</div>
                <div>{{ stats.cost_efficiency.most_cost_effective.instance }}</div>
            </div>
            {% endif %}
        </div>
        
        <h2>üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>
        <div class="chart-container">
            {{ performance_chart|safe }}
        </div>
        
        {% if cost_chart %}
        <h2>üí∞ –ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏</h2>
        <div class="chart-container">
            {{ cost_chart|safe }}
        </div>
        {% endif %}
        
        {% if resource_chart %}
        <h2>üñ•Ô∏è –£—Ç–∏–ª–∏–∑–∞—Ü–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤</h2>
        <div class="chart-container">
            {{ resource_chart|safe }}
        </div>
        {% endif %}
        
        <h2>üìã –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã</h2>
        <table>
            <thead>
                <tr>
                    <th>–ò–Ω—Å—Ç–∞–Ω—Å</th>
                    <th>–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏</th>
                    <th>RPS</th>
                    <th>–°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (ms)</th>
                    <th>P95 –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (ms)</th>
                    <th>–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤</th>
                    <th>–û—à–∏–±–æ–∫ (%)</th>
                </tr>
            </thead>
            <tbody>
                {% for benchmark in benchmark_data %}
                <tr>
                    <td>{{ benchmark.instance_type }}</td>
                    <td>{{ benchmark.concurrent_users }}</td>
                    <td>{{ "%.2f"|format(benchmark.requests_per_second) }}</td>
                    <td>{{ "%.2f"|format(benchmark.avg_latency) }}</td>
                    <td>{{ "%.2f"|format(benchmark.p95_latency) }}</td>
                    <td>{{ benchmark.successful_requests }}</td>
                    <td>{{ "%.2f"|format(benchmark.error_rate) }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
        
        {% if cost_data %}
        <h2>üíµ –ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏</h2>
        <table>
            <thead>
                <tr>
                    <th>–ò–Ω—Å—Ç–∞–Ω—Å</th>
                    <th>–°—Ç–æ–∏–º–æ—Å—Ç—å/—á–∞—Å ($)</th>
                    <th>–°—Ç–æ–∏–º–æ—Å—Ç—å 1000 –ø—Ä–µ–¥. ($)</th>
                    <th>RPS</th>
                    <th>–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å</th>
                </tr>
            </thead>
            <tbody>
                {% for cost in cost_data %}
                <tr>
                    <td>{{ cost.instance_name }}</td>
                    <td>${{ "%.3f"|format(cost.cost_per_hour) }}</td>
                    <td>${{ "%.4f"|format(cost.cost_per_1000_predictions) }}</td>
                    <td>{{ "%.2f"|format(cost.requests_per_second) }}</td>
                    <td>{{ "%.2f"|format(cost.total_efficiency_score) }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
        {% endif %}
        
        <div class="recommendations">
            <h3>üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h3>
            <ul>
                {% if stats.best_performance.highest_rps %}
                <li><strong>–î–ª—è –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ {{ stats.best_performance.highest_rps.instance }} ({{ "%.1f"|format(stats.best_performance.highest_rps.value) }} RPS)</li>
                {% endif %}
                {% if stats.best_performance.lowest_latency %}
                <li><strong>–î–ª—è –Ω–∏–∑–∫–æ–π –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ {{ stats.best_performance.lowest_latency.instance }} ({{ "%.1f"|format(stats.best_performance.lowest_latency.value) }}ms)</li>
                {% endif %}
                {% if stats.cost_efficiency.most_cost_effective %}
                <li><strong>–î–ª—è —ç–∫–æ–Ω–æ–º–∏–∏:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ {{ stats.cost_efficiency.most_cost_effective.instance }} (${{ "%.4f"|format(stats.cost_efficiency.most_cost_effective.cost_per_1000) }} –∑–∞ 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)</li>
                {% endif %}
                <li><strong>–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:</strong> –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–ª–µ—Ä—Ç—ã –Ω–∞ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å >500ms –∏ error rate >5%</li>
                <li><strong>–ê–≤—Ç–æ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ:</strong> –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ HPA —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ CPU –∏ custom metrics</li>
            </ul>
        </div>
        
        <div class="footer">
            <p>–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {{ generation_time }}</p>
            <p>MLOps Sentiment Analysis Benchmarking Framework</p>
        </div>
    </div>
</body>
</html>
        """
        
        # –†–µ–Ω–¥–µ—Ä–∏–º —à–∞–±–ª–æ–Ω
        template = Template(html_template)
        html_content = template.render(
            stats=stats,
            benchmark_data=self.benchmark_data,
            cost_data=self.cost_data,
            performance_chart=performance_chart,
            cost_chart=cost_chart,
            resource_chart=resource_chart,
            generation_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"HTML –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='Comprehensive Benchmark Report Generator')
    parser.add_argument('--results-dir', default='results',
                       help='Directory containing benchmark results')
    parser.add_argument('--output', default='benchmark_comprehensive_report.html',
                       help='Output HTML file path')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—Ç—á–µ—Ç–æ–≤
    generator = BenchmarkReportGenerator(args.results_dir)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        generator.load_data()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º HTML –æ—Ç—á–µ—Ç
        generator.generate_html_report(args.output)
        
        logger.info(f"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {args.output}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")
        raise

if __name__ == "__main__":
    main()
