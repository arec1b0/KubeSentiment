#!/usr/bin/env python3
"""
MLOps Sentiment Analysis - Cost Calculator
–†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤
"""

import json
import yaml
import argparse
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class InstanceCost:
    """–°—Ç–æ–∏–º–æ—Å—Ç—å –∏–Ω—Å—Ç–∞–Ω—Å–∞"""
    name: str
    type: str
    cost_per_hour: float
    vcpu: int
    memory_gb: float
    gpu: Optional[str] = None
    gpu_memory_gb: Optional[float] = None

@dataclass
class BenchmarkCostAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    instance_name: str
    instance_type: str
    cost_per_hour: float
    requests_per_second: float
    avg_latency_ms: float
    predictions_per_hour: float
    cost_per_1000_predictions: float
    cost_efficiency_score: float  # RPS / cost_per_hour
    latency_efficiency_score: float  # 1000 / avg_latency_ms
    total_efficiency_score: float

@dataclass
class CostComparisonReport:
    """–û—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
    timestamp: str
    total_predictions: int
    analyses: List[BenchmarkCostAnalysis]
    best_cost_efficiency: str
    best_latency_efficiency: str
    best_overall_efficiency: str
    recommendations: List[str]

class CostCalculator:
    """–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"""
    
    def __init__(self, config_path: str):
        self.config = self._load_config(config_path)
        self.instance_costs = self._parse_instance_costs()
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _parse_instance_costs(self) -> Dict[str, InstanceCost]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        instance_costs = {}
        
        # CPU –∏–Ω—Å—Ç–∞–Ω—Å—ã
        for instance in self.config['instances']['cpu']:
            cost = InstanceCost(
                name=instance['name'],
                type=instance['type'],
                cost_per_hour=instance['cost_per_hour'],
                vcpu=instance['vcpu'],
                memory_gb=float(instance['memory'].replace('Gi', ''))
            )
            instance_costs[instance['name']] = cost
        
        # GPU –∏–Ω—Å—Ç–∞–Ω—Å—ã
        for instance in self.config['instances']['gpu']:
            cost = InstanceCost(
                name=instance['name'],
                type=instance['type'],
                cost_per_hour=instance['cost_per_hour'],
                vcpu=instance['vcpu'],
                memory_gb=float(instance['memory'].replace('Gi', '')),
                gpu=instance['gpu'],
                gpu_memory_gb=float(instance['gpu_memory'].replace('Gi', ''))
            )
            instance_costs[instance['name']] = cost
        
        return instance_costs
    
    def calculate_cost_analysis(self, benchmark_results_path: str) -> List[BenchmarkCostAnalysis]:
        """–†–∞—Å—á–µ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
        analyses = []
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
        with open(benchmark_results_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # –ï—Å–ª–∏ —ç—Ç–æ –æ–¥–∏–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫
        if isinstance(results, dict):
            results = [results]
        
        for result in results:
            instance_type = result['instance_type']
            
            if instance_type not in self.instance_costs:
                logger.warning(f"Instance type {instance_type} not found in cost configuration")
                continue
            
            instance_cost = self.instance_costs[instance_type]
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            rps = result['requests_per_second']
            avg_latency = result['avg_latency']
            predictions_per_hour = rps * 3600  # RPS * —Å–µ–∫—É–Ω–¥ –≤ —á–∞—Å–µ
            
            # –°—Ç–æ–∏–º–æ—Å—Ç—å 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            if predictions_per_hour > 0:
                cost_per_1000_predictions = (instance_cost.cost_per_hour * 1000) / predictions_per_hour
            else:
                cost_per_1000_predictions = float('inf')
            
            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            cost_efficiency = rps / instance_cost.cost_per_hour if instance_cost.cost_per_hour > 0 else 0
            latency_efficiency = 1000 / avg_latency if avg_latency > 0 else 0
            total_efficiency = (cost_efficiency * latency_efficiency) / cost_per_1000_predictions if cost_per_1000_predictions > 0 else 0
            
            analysis = BenchmarkCostAnalysis(
                instance_name=instance_cost.name,
                instance_type=instance_cost.type,
                cost_per_hour=instance_cost.cost_per_hour,
                requests_per_second=rps,
                avg_latency_ms=avg_latency,
                predictions_per_hour=predictions_per_hour,
                cost_per_1000_predictions=cost_per_1000_predictions,
                cost_efficiency_score=cost_efficiency,
                latency_efficiency_score=latency_efficiency,
                total_efficiency_score=total_efficiency
            )
            
            analyses.append(analysis)
        
        return analyses
    
    def generate_cost_comparison_report(self, analyses: List[BenchmarkCostAnalysis], 
                                      total_predictions: int = 1000) -> CostComparisonReport:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
        
        if not analyses:
            raise ValueError("No analyses provided")
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        best_cost = min(analyses, key=lambda x: x.cost_per_1000_predictions)
        best_latency = min(analyses, key=lambda x: x.avg_latency_ms)
        best_overall = max(analyses, key=lambda x: x.total_efficiency_score)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = self._generate_recommendations(analyses)
        
        report = CostComparisonReport(
            timestamp=datetime.now().isoformat(),
            total_predictions=total_predictions,
            analyses=analyses,
            best_cost_efficiency=best_cost.instance_name,
            best_latency_efficiency=best_latency.instance_name,
            best_overall_efficiency=best_overall.instance_name,
            recommendations=recommendations
        )
        
        return report
    
    def _generate_recommendations(self, analyses: List[BenchmarkCostAnalysis]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        recommendations = []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        cost_sorted = sorted(analyses, key=lambda x: x.cost_per_1000_predictions)
        latency_sorted = sorted(analyses, key=lambda x: x.avg_latency_ms)
        efficiency_sorted = sorted(analyses, key=lambda x: x.total_efficiency_score, reverse=True)
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        cheapest = cost_sorted[0]
        most_expensive = cost_sorted[-1]
        
        cost_diff = most_expensive.cost_per_1000_predictions / cheapest.cost_per_1000_predictions
        
        recommendations.append(
            f"üí∞ –°–∞–º—ã–π —ç–∫–æ–Ω–æ–º–∏—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: {cheapest.instance_name} "
            f"(${cheapest.cost_per_1000_predictions:.4f} –∑–∞ 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)"
        )
        
        if cost_diff > 2:
            recommendations.append(
                f"‚ö†Ô∏è –†–∞–∑–Ω–∏—Ü–∞ –≤ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å–∞–º—ã–º –¥–µ—à–µ–≤—ã–º –∏ –¥–æ—Ä–æ–≥–∏–º –∏–Ω—Å—Ç–∞–Ω—Å–æ–º —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {cost_diff:.1f}x"
            )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        fastest = latency_sorted[0]
        slowest = latency_sorted[-1]
        
        recommendations.append(
            f"‚ö° –°–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: {fastest.instance_name} "
            f"({fastest.avg_latency_ms:.2f}ms —Å—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å)"
        )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–±—â–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        most_efficient = efficiency_sorted[0]
        recommendations.append(
            f"üéØ –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Ü–µ–Ω–∞/–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {most_efficient.instance_name}"
        )
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        gpu_analyses = [a for a in analyses if 'gpu' in a.instance_name.lower()]
        cpu_analyses = [a for a in analyses if 'cpu' in a.instance_name.lower()]
        
        if gpu_analyses and cpu_analyses:
            best_gpu = min(gpu_analyses, key=lambda x: x.cost_per_1000_predictions)
            best_cpu = min(cpu_analyses, key=lambda x: x.cost_per_1000_predictions)
            
            if best_cpu.cost_per_1000_predictions < best_gpu.cost_per_1000_predictions:
                savings = ((best_gpu.cost_per_1000_predictions - best_cpu.cost_per_1000_predictions) / 
                          best_gpu.cost_per_1000_predictions) * 100
                recommendations.append(
                    f"üí° CPU –∏–Ω—Å—Ç–∞–Ω—Å—ã —ç–∫–æ–Ω–æ–º–∏—á–Ω–µ–µ GPU –Ω–∞ {savings:.1f}% –¥–ª—è –¥–∞–Ω–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏"
                )
            else:
                performance_gain = (best_gpu.requests_per_second / best_cpu.requests_per_second - 1) * 100
                recommendations.append(
                    f"üöÄ GPU –∏–Ω—Å—Ç–∞–Ω—Å—ã –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—Ç –ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ {performance_gain:.1f}%"
                )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é
        high_rps_instances = [a for a in analyses if a.requests_per_second > 100]
        if high_rps_instances:
            recommendations.append(
                "üìà –î–ª—è –≤—ã—Å–æ–∫–∏—Ö –Ω–∞–≥—Ä—É–∑–æ–∫ (>100 RPS) —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: " +
                ", ".join([a.instance_name for a in high_rps_instances[:3]])
            )
        
        return recommendations
    
    def save_cost_report(self, report: CostComparisonReport, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)
        
        logger.info(f"Cost report saved to {output_path}")
    
    def generate_cost_visualization(self, analyses: List[BenchmarkCostAnalysis], 
                                  output_dir: str):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = pd.DataFrame([asdict(a) for a in analyses])
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≥—Ä–∞—Ñ–∏–∫–∞–º–∏
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞ 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        ax1 = axes[0, 0]
        bars1 = ax1.bar(df['instance_name'], df['cost_per_1000_predictions'])
        ax1.set_title('–°—Ç–æ–∏–º–æ—Å—Ç—å 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –ø–æ —Ç–∏–ø–∞–º –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤')
        ax1.set_xlabel('–¢–∏–ø –∏–Ω—Å—Ç–∞–Ω—Å–∞')
        ax1.set_ylabel('–°—Ç–æ–∏–º–æ—Å—Ç—å (USD)')
        ax1.tick_params(axis='x', rotation=45)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(bars1, df['cost_per_1000_predictions']):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'${value:.4f}', ha='center', va='bottom', fontsize=9)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: RPS vs –°—Ç–æ–∏–º–æ—Å—Ç—å –≤ —á–∞—Å
        ax2 = axes[0, 1]
        scatter = ax2.scatter(df['cost_per_hour'], df['requests_per_second'], 
                             s=100, alpha=0.7, c=df['avg_latency_ms'], cmap='viridis')
        ax2.set_title('–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å vs –°—Ç–æ–∏–º–æ—Å—Ç—å')
        ax2.set_xlabel('–°—Ç–æ–∏–º–æ—Å—Ç—å –≤ —á–∞—Å (USD)')
        ax2.set_ylabel('–ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏ —Ç–æ—á–µ–∫
        for i, row in df.iterrows():
            ax2.annotate(row['instance_name'], 
                        (row['cost_per_hour'], row['requests_per_second']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞ –¥–ª—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        cbar = plt.colorbar(scatter, ax=ax2)
        cbar.set_label('–°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (ms)')
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        ax3 = axes[1, 0]
        efficiency_bars = ax3.bar(df['instance_name'], df['total_efficiency_score'])
        ax3.set_title('–û–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å/—Å—Ç–æ–∏–º–æ—Å—Ç—å)')
        ax3.set_xlabel('–¢–∏–ø –∏–Ω—Å—Ç–∞–Ω—Å–∞')
        ax3.set_ylabel('–û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏')
        ax3.tick_params(axis='x', rotation=45)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        ax4 = axes[1, 1]
        latency_bars = ax4.bar(df['instance_name'], df['avg_latency_ms'])
        ax4.set_title('–°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤')
        ax4.set_xlabel('–¢–∏–ø –∏–Ω—Å—Ç–∞–Ω—Å–∞')
        ax4.set_ylabel('–õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (ms)')
        ax4.tick_params(axis='x', rotation=45)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
        for bar, value in zip(latency_bars, df['avg_latency_ms']):
            ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                    f'{value:.1f}ms', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(output_dir / 'cost_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫: –°—Ç–æ–∏–º–æ—Å—Ç—å vs –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (bubble chart)
        plt.figure(figsize=(12, 8))
        
        # –†–∞–∑–º–µ—Ä –ø—É–∑—ã—Ä—å–∫–æ–≤ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª–µ–Ω —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        sizes = [max(50, a.total_efficiency_score * 100) for a in analyses]
        
        scatter = plt.scatter(df['cost_per_1000_predictions'], df['requests_per_second'],
                             s=sizes, alpha=0.6, c=df['avg_latency_ms'], cmap='RdYlBu_r')
        
        plt.xlabel('–°—Ç–æ–∏–º–æ—Å—Ç—å 1000 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (USD)')
        plt.ylabel('–ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É')
        plt.title('–ê–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n(—Ä–∞–∑–º–µ—Ä –ø—É–∑—ã—Ä—å–∫–∞ = –æ–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å)')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–ø–∏—Å–∏
        for i, row in df.iterrows():
            plt.annotate(row['instance_name'], 
                        (row['cost_per_1000_predictions'], row['requests_per_second']),
                        xytext=(5, 5), textcoords='offset points')
        
        # –¶–≤–µ—Ç–æ–≤–∞—è —à–∫–∞–ª–∞
        cbar = plt.colorbar(scatter)
        cbar.set_label('–°—Ä–µ–¥–Ω—è—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (ms)')
        
        plt.grid(True, alpha=0.3)
        plt.savefig(output_dir / 'cost_performance_bubble.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Cost visualization saved to {output_dir}")
    
    def print_cost_summary(self, report: CostComparisonReport):
        """–í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"""
        print(f"\n{'='*80}")
        print("–ê–ù–ê–õ–ò–ó –°–¢–û–ò–ú–û–°–¢–ò –ë–ï–ù–ß–ú–ê–†–ö–ê")
        print(f"{'='*80}")
        print(f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {report.timestamp}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {report.total_predictions}")
        print(f"\n{'–ò–Ω—Å—Ç–∞–Ω—Å':<15} {'–¢–∏–ø':<15} {'$/—á–∞—Å':<10} {'RPS':<8} {'–õ–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å':<12} {'$/1000 –ø—Ä–µ–¥.':<12}")
        print("-" * 80)
        
        for analysis in sorted(report.analyses, key=lambda x: x.cost_per_1000_predictions):
            print(f"{analysis.instance_name:<15} "
                  f"{analysis.instance_type:<15} "
                  f"${analysis.cost_per_hour:<9.3f} "
                  f"{analysis.requests_per_second:<7.1f} "
                  f"{analysis.avg_latency_ms:<11.1f}ms "
                  f"${analysis.cost_per_1000_predictions:<11.4f}")
        
        print(f"\n{'='*80}")
        print("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"{'='*80}")
        
        for i, recommendation in enumerate(report.recommendations, 1):
            print(f"{i}. {recommendation}")
        
        print(f"\nüèÜ –õ—É—á—à–∏–π –ø–æ —Å—Ç–æ–∏–º–æ—Å—Ç–∏: {report.best_cost_efficiency}")
        print(f"‚ö° –õ—É—á—à–∏–π –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏: {report.best_latency_efficiency}")
        print(f"üéØ –õ—É—á—à–∏–π –æ–±—â–∏–π –±–∞–ª–∞–Ω—Å: {report.best_overall_efficiency}")

def main():
    parser = argparse.ArgumentParser(description='Cost Calculator for MLOps Benchmarking')
    parser.add_argument('--config', default='configs/benchmark-config.yaml',
                       help='Path to benchmark configuration file')
    parser.add_argument('--results', required=True,
                       help='Path to benchmark results JSON file(s)')
    parser.add_argument('--predictions', type=int, default=1000,
                       help='Number of predictions for cost calculation')
    parser.add_argument('--output', default='results/cost_analysis.json',
                       help='Output file for cost analysis')
    parser.add_argument('--report-dir', default='results/cost_reports',
                       help='Directory for generated reports')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–µ–º –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä
    calculator = CostCalculator(args.config)
    
    try:
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        analyses = calculator.calculate_cost_analysis(args.results)
        
        if not analyses:
            logger.error("No valid analyses generated")
            return
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = calculator.generate_cost_comparison_report(analyses, args.predictions)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        calculator.save_cost_report(report, args.output)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        calculator.generate_cost_visualization(analyses, args.report_dir)
        
        # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
        calculator.print_cost_summary(report)
        
    except Exception as e:
        logger.error(f"Cost calculation failed: {e}")
        raise

if __name__ == "__main__":
    main()
