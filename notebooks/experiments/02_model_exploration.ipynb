{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Model Exploration: Understanding DistilBERT Sentiment Analysis\n",
    "\n",
    "This notebook provides an in-depth exploration of the sentiment analysis model used in KubeSentiment. We'll examine the DistilBERT model architecture, understand its capabilities, and analyze its performance characteristics.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the DistilBERT model architecture\n",
    "2. Learn about the SST-2 dataset and fine-tuning\n",
    "3. Explore model performance characteristics\n",
    "4. Analyze confidence scores and decision boundaries\n",
    "5. Understand model limitations and edge cases\n",
    "6. Compare with baseline approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Dependencies\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this notebook\n",
    "# Note: This cell might take a few minutes to run\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Version Check\n",
    "Let's check the versions of the installed libraries to ensure our environment is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List installed packages to ensure reproducibility\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ DistilBERT: The Model Behind KubeSentiment\n",
    "\n",
    "### What is DistilBERT?\n",
    "\n",
    "DistilBERT is a **distilled version of BERT** (Bidirectional Encoder Representations from Transformers) that:\n",
    "- **Maintains 97% of BERT's performance** while being 40% smaller\n",
    "- **Runs 60% faster** than the base BERT model\n",
    "- **Reduces carbon footprint** by requiring less computational resources\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "```\n",
    "Input Text\n",
    "    ‚Üì\n",
    "Tokenization (WordPiece)\n",
    "    ‚Üì\n",
    "Embedding Layer\n",
    "    ‚Üì\n",
    "6 Transformer Blocks (vs 12 in BERT-base)\n",
    "    ‚Üì\n",
    "Classification Head (2 classes: POSITIVE/NEGATIVE)\n",
    "    ‚Üì\n",
    "Softmax Probabilities\n",
    "```\n",
    "\n",
    "### SST-2 Dataset\n",
    "\n",
    "The model is fine-tuned on the **Stanford Sentiment Treebank (SST-2)**:\n",
    "- **67,349 training examples**\n",
    "- **Binary classification**: Positive vs Negative sentiment\n",
    "- **High-quality annotations** from human labelers\n",
    "- **~91% accuracy** on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
    "print(f\"üñ•Ô∏è CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Loading the Model\n",
    "\n",
    "Let's load the same model used in KubeSentiment and explore its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(f\"üì• Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create pipeline (same as used in the service)\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"üèóÔ∏è Model architecture: {model.__class__.__name__}\")\n",
    "print(f\"üìä Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"üè∑Ô∏è Labels: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Model Architecture Analysis\n",
    "\n",
    "Let's examine the model's internal structure and understand how it processes text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model architecture\n",
    "print(\"üèóÔ∏è Model Architecture Breakdown:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model configuration\n",
    "config = model.config\n",
    "print(f\"üìè Maximum sequence length: {config.max_position_embeddings}\")\n",
    "print(f\"üèóÔ∏è Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"üîç Hidden size: {config.hidden_size}\")\n",
    "print(f\"üë• Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"üìä Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"üè∑Ô∏è Number of labels: {config.num_labels}\")\n",
    "\n",
    "# Model size analysis\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in MB.\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_mb = (param_size + buffer_size) / 1024 / 1024\n",
    "    return size_mb\n",
    "\n",
    "model_size_mb = get_model_size(model)\n",
    "print(f\"üíæ Model size: {model_size_mb:.1f} MB\")\n",
    "\n",
    "# Layer analysis\n",
    "print(\"\\nüìã Layer Structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if len(name.split('.')) <= 2:  # Top-level modules only\n",
    "        print(f\"  ‚Ä¢ {name}: {module.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî§ Tokenization Deep Dive\n",
    "\n",
    "Understanding how text is tokenized is crucial for understanding model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore tokenization\n",
    "def analyze_tokenization(text: str):\n",
    "    \"\"\"Analyze how text is tokenized by the model.\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "\n",
    "    print(f\"üìù Original text: {text}\")\n",
    "    print(f\"üî§ Tokens: {tokens}\")\n",
    "    print(f\"üÜî Token IDs: {token_ids}\")\n",
    "    print(f\"üìä Number of tokens: {len(tokens)}\")\n",
    "    print(f\"üîÑ Decoded: {decoded}\")\n",
    "    print(f\"‚ö° Special tokens: {[tokenizer.cls_token, tokenizer.sep_token]}\")\n",
    "\n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"token_ids\": token_ids,\n",
    "        \"num_tokens\": len(tokens)\n",
    "    }\n",
    "\n",
    "# Test different text examples\n",
    "test_texts = [\n",
    "    \"I love this!\",\n",
    "    \"This is absolutely terrible.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning and artificial intelligence are transforming our world.\",\n",
    "    \"Hello, world! How are you doing today?\"\n",
    "]\n",
    "\n",
    "print(\"üî§ Tokenization Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenization_results = []\n",
    "for text in test_texts:\n",
    "    result = analyze_tokenization(text)\n",
    "    tokenization_results.append({\"text\": text, **result})\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Sentiment Analysis Examples\n",
    "\n",
    "Let's test the model with various types of text to understand its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive sentiment analysis test\n",
    "sentiment_test_cases = [\n",
    "    # Clearly positive\n",
    "    \"This is absolutely amazing! I love it so much!\",\n",
    "    \"Outstanding performance and excellent quality.\",\n",
    "    \"Best purchase I've ever made. Highly recommended!\",\n",
    "\n",
    "    # Clearly negative\n",
    "    \"This is terrible. Complete waste of money.\",\n",
    "    \"Awful experience. Never buying again.\",\n",
    "    \"Worst product I've ever used. Total disappointment.\",\n",
    "\n",
    "    # Neutral/ambiguous\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"The product works as expected.\",\n",
    "    \"Average performance for the price.\",\n",
    "\n",
    "    # Sarcasm and complex cases\n",
    "    \"Oh great, another meeting that could have been an email.\",\n",
    "    \"Thanks for the helpful error message that tells me nothing.\",\n",
    "    \"I'm so excited to spend my weekend debugging this code.\",\n",
    "\n",
    "    # Very short texts\n",
    "    \"Great!\",\n",
    "    \"Terrible.\",\n",
    "    \"Okay.\",\n",
    "\n",
    "    # Emojis and special characters\n",
    "    \"This is awesome! üòç‚ú®\",\n",
    "    \"So disappointed üòûüíî\",\n",
    "    \"Mixed feelings ü§∑‚Äç‚ôÇÔ∏è\"\n",
    "]\n",
    "\n",
    "def analyze_sentiments_batch(texts: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Analyze sentiment for multiple texts.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for text in texts:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Get predictions\n",
    "        predictions = classifier(text)[0]  # Get all scores\n",
    "\n",
    "        inference_time = (time.time() - start_time) * 1000\n",
    "\n",
    "        # Find the winning prediction\n",
    "        winner = max(predictions, key=lambda x: x['score'])\n",
    "        loser = min(predictions, key=lambda x: x['score'])\n",
    "\n",
    "        result = {\n",
    "            \"text\": text,\n",
    "            \"label\": winner[\"label\"],\n",
    "            \"score\": winner[\"score\"],\n",
    "            \"confidence_margin\": winner[\"score\"] - loser[\"score\"],\n",
    "            \"inference_time_ms\": round(inference_time, 2),\n",
    "            \"all_scores\": {pred[\"label\"]: pred[\"score\"] for pred in predictions}\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run analysis\n",
    "print(\"üéØ Sentiment Analysis Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sentiment_results = analyze_sentiments_batch(sentiment_test_cases)\n",
    "\n",
    "# Display results\n",
    "for result in sentiment_results:\n",
    "    print(f\"üìù {result['text'][:50]}...\")\n",
    "    print(f\"   üòä Label: {result['label']}\")\n",
    "    print(f\"   üìä Confidence: {result['score']:.3f}\")\n",
    "    print(f\"   üìè Margin: {result['confidence_margin']:.3f}\")\n",
    "    print(f\"   ‚ö° Time: {result['inference_time_ms']:.2f}ms\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Confidence Analysis\n",
    "\n",
    "Let's analyze the model's confidence patterns and decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for analysis\n",
    "df_results = pd.DataFrame(sentiment_results)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('DistilBERT Sentiment Analysis - Confidence Analysis', fontsize=16)\n",
    "\n",
    "# 1. Confidence distribution\n",
    "axes[0, 0].hist(df_results['score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Confidence Score Distribution')\n",
    "axes[0, 0].set_xlabel('Confidence Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].axvline(df_results['score'].mean(), color='red', linestyle='--', label=f'Mean: {df_results[\"score\"].mean():.3f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Confidence by label\n",
    "positive_scores = df_results[df_results['label'] == 'POSITIVE']['score']\n",
    "negative_scores = df_results[df_results['label'] == 'NEGATIVE']['score']\n",
    "\n",
    "axes[0, 1].hist(positive_scores, alpha=0.7, label='POSITIVE', color='green', bins=10)\n",
    "axes[0, 1].hist(negative_scores, alpha=0.7, label='NEGATIVE', color='red', bins=10)\n",
    "axes[0, 1].set_title('Confidence by Sentiment Label')\n",
    "axes[0, 1].set_xlabel('Confidence Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Confidence margin distribution\n",
    "axes[0, 2].hist(df_results['confidence_margin'], bins=15, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0, 2].set_title('Confidence Margin Distribution')\n",
    "axes[0, 2].set_xlabel('Confidence Margin')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].axvline(df_results['confidence_margin'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {df_results[\"confidence_margin\"].mean():.3f}')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Inference time distribution\n",
    "axes[1, 0].hist(df_results['inference_time_ms'], bins=10, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[1, 0].set_title('Inference Time Distribution')\n",
    "axes[1, 0].set_xlabel('Time (ms)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].axvline(df_results['inference_time_ms'].mean(), color='red', linestyle='--',\n",
    "                   label=f'Mean: {df_results[\"inference_time_ms\"].mean():.2f}ms')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 5. Low confidence predictions\n",
    "low_confidence = df_results[df_results['score'] < 0.7]\n",
    "axes[1, 1].barh(range(len(low_confidence)), low_confidence['score'])\n",
    "axes[1, 1].set_yticks(range(len(low_confidence)))\n",
    "axes[1, 1].set_yticklabels([text[:30] + \"...\" for text in low_confidence['text']])\n",
    "axes[1, 1].set_title('Low Confidence Predictions (< 0.7)')\n",
    "axes[1, 1].set_xlabel('Confidence Score')\n",
    "\n",
    "# 6. Performance summary\n",
    "axes[1, 2].axis('off')\n",
    "summary_text = f\"\"\"Performance Summary:\n",
    "\n",
    "Total Predictions: {len(df_results)}\n",
    "Positive: {len(positive_scores)}\n",
    "Negative: {len(negative_scores)}\n",
    "\n",
    "Avg Confidence: {df_results['score'].mean():.3f}\n",
    "Avg Margin: {df_results['confidence_margin'].mean():.3f}\n",
    "Avg Time: {df_results['inference_time_ms'].mean():.2f}ms\n",
    "\n",
    "Low Conf (<0.7): {len(low_confidence)}\n",
    "High Conf (>0.9): {len(df_results[df_results['score'] > 0.9])}\n",
    "\"\"\"\n",
    "axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,\n",
    "                fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display detailed results table\n",
    "print(\"\\nüìä Detailed Results:\")\n",
    "display(df_results[['text', 'label', 'score', 'confidence_margin', 'inference_time_ms']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Edge Cases and Model Limitations\n",
    "\n",
    "Let's explore some edge cases and understand the model's limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test edge cases\n",
    "edge_cases = [\n",
    "    # Very short texts\n",
    "    \"Good\",\n",
    "    \"Bad\",\n",
    "    \"!\",\n",
    "    \"\",\n",
    "\n",
    "    # Very long texts (truncated)\n",
    "    \"This is an extremely long text that goes on and on and on with lots of words that might confuse the model because it's way longer than the typical training examples and could potentially cause issues with tokenization and attention mechanisms. \" * 10,\n",
    "\n",
    "    # Neutral statements\n",
    "    \"The sky is blue.\",\n",
    "    \"Water is wet.\",\n",
    "    \"2 + 2 = 4.\",\n",
    "\n",
    "    # Sarcasm\n",
    "    \"Oh wow, another software update that breaks everything. Just what I needed.\",\n",
    "    \"Thanks for the amazing customer service that took 3 days to respond.\",\n",
    "\n",
    "    # Mixed sentiment\n",
    "    \"The food was excellent but the service was terrible.\",\n",
    "    \"Great product, awful packaging.\",\n",
    "\n",
    "    # Questions\n",
    "    \"Is this any good?\",\n",
    "    \"Why is this so bad?\",\n",
    "\n",
    "    # Emojis only\n",
    "    \"üòÄüòÄüòÄ\",\n",
    "    \"üò¢üò¢üò¢\",\n",
    "    \"ü§∑‚Äç‚ôÇÔ∏è\",\n",
    "\n",
    "    # Numbers and symbols\n",
    "    \"12345\",\n",
    "    \"!@#$%^&*()\",\n",
    "\n",
    "    # Foreign languages\n",
    "    \"C'est excellent!\",\n",
    "    \"Muy malo.\",\n",
    "    \"Á¥†Êô¥„Çâ„Åó„ÅÑ„Åß„Åô\",\n",
    "]\n",
    "\n",
    "print(\"üîç Edge Cases Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "edge_results = []\n",
    "for text in edge_cases:\n",
    "    try:\n",
    "        result = analyze_sentiments_batch([text])[0]\n",
    "        edge_results.append(result)\n",
    "\n",
    "        print(f\"üìù Text: {text[:40]}{'...' if len(text) > 40 else ''}\")\n",
    "        print(f\"   üè∑Ô∏è Prediction: {result['label']} ({result['score']:.3f})\")\n",
    "        print(f\"   üìä Margin: {result['confidence_margin']:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing: {text[:30]}... - {e}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Analyze edge case results\n",
    "edge_df = pd.DataFrame(edge_results)\n",
    "\n",
    "print(\"\\nüìà Edge Cases Summary:\")\n",
    "print(f\"Total edge cases tested: {len(edge_cases)}\")\n",
    "print(f\"Successfully processed: {len(edge_results)}\")\n",
    "print(f\"Errors encountered: {len(edge_cases) - len(edge_results)}\")\n",
    "\n",
    "if len(edge_results) > 0:\n",
    "    print(f\"\\nAverage confidence on edge cases: {edge_df['score'].mean():.3f}\")\n",
    "    print(f\"Edge cases with low confidence (<0.6): {len(edge_df[edge_df['score'] < 0.6])}\")\n",
    "    print(f\"Most confident edge case prediction: {edge_df.loc[edge_df['score'].idxmax(), 'text'][:30]}... ({edge_df['score'].max():.3f})\")\n",
    "    print(f\"Least confident edge case prediction: {edge_df.loc[edge_df['score'].idxmin(), 'text'][:30]}... ({edge_df['score'].min():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üÜö Model Comparison\n",
    "\n",
    "Let's compare DistilBERT with simpler baseline approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baseline models for comparison\n",
    "def keyword_baseline(text: str) -> str:\n",
    "    \"\"\"Simple keyword-based sentiment classifier.\"\"\"\n",
    "    positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'best']\n",
    "    negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'disappointing']\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    pos_count = sum(1 for word in positive_words if word in text_lower)\n",
    "    neg_count = sum(1 for word in negative_words if word in text_lower)\n",
    "\n",
    "    if pos_count > neg_count:\n",
    "        return 'POSITIVE'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'NEGATIVE'\n",
    "    else:\n",
    "        return 'POSITIVE'  # Default to positive\n",
    "\n",
    "def length_baseline(text: str) -> str:\n",
    "    \"\"\"Length-based classifier (joke baseline).\"\"\"\n",
    "    return 'POSITIVE' if len(text) > 20 else 'NEGATIVE'\n",
    "\n",
    "# Test texts for comparison\n",
    "comparison_texts = [\n",
    "    \"This product is amazing!\",\n",
    "    \"Terrible quality.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "    \"I absolutely love this wonderful product!\",\n",
    "    \"This is the worst purchase I've ever made.\",\n",
    "    \"Good value for money.\",\n",
    "    \"Awful customer service.\",\n",
    "    \"Fantastic experience!\",\n",
    "    \"Complete disappointment.\",\n",
    "    \"Great!\"\n",
    "]\n",
    "\n",
    "# Compare models\n",
    "comparison_results = []\n",
    "\n",
    "for text in comparison_texts:\n",
    "    # DistilBERT prediction\n",
    "    distilbert_result = analyze_sentiments_batch([text])[0]\n",
    "\n",
    "    # Baseline predictions\n",
    "    keyword_pred = keyword_baseline(text)\n",
    "    length_pred = length_baseline(text)\n",
    "\n",
    "    comparison_results.append({\n",
    "        \"text\": text,\n",
    "        \"distilbert\": distilbert_result[\"label\"],\n",
    "        \"distilbert_confidence\": distilbert_result[\"score\"],\n",
    "        \"keyword_baseline\": keyword_pred,\n",
    "        \"length_baseline\": length_pred\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comp_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "# Calculate agreement\n",
    "keyword_agreement = (comp_df['distilbert'] == comp_df['keyword_baseline']).mean()\n",
    "length_agreement = (comp_df['distilbert'] == comp_df['length_baseline']).mean()\n",
    "\n",
    "print(\"üÜö Model Comparison Results:\")\n",
    "print(\"=\" * 70)\n",
    "display(comp_df)\n",
    "\n",
    "print(f\"\\nüìä Agreement Analysis:\")\n",
    "print(f\"DistilBERT vs Keyword Baseline: {keyword_agreement:.1%}\")\n",
    "print(f\"DistilBERT vs Length Baseline: {length_agreement:.1%}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Agreement comparison\n",
    "models = ['Keyword Baseline', 'Length Baseline']\n",
    "agreements = [keyword_agreement, length_agreement]\n",
    "\n",
    "bars = axes[0].bar(models, agreements, color=['skyblue', 'lightcoral'])\n",
    "axes[0].set_title('Agreement with DistilBERT')\n",
    "axes[0].set_ylabel('Agreement Rate')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, agreement in zip(bars, agreements):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{agreement:.1%}', ha='center', va='bottom')\n",
    "\n",
    "# Confidence vs baseline performance\n",
    "colors = ['green' if row['distilbert'] == row['keyword_baseline'] else 'red'\n",
    "          for _, row in comp_df.iterrows()]\n",
    "\n",
    "axes[1].scatter(comp_df['distilbert_confidence'],\n",
    "               [1 if row['distilbert'] == row['keyword_baseline'] else 0 for _, row in comp_df.iterrows()],\n",
    "               c=colors, s=100, alpha=0.7)\n",
    "axes[1].set_title('Confidence vs Keyword Baseline Agreement')\n",
    "axes[1].set_xlabel('DistilBERT Confidence')\n",
    "axes[1].set_ylabel('Agrees with Keyword (1=Yes, 0=No)')\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_yticklabels(['Disagree', 'Agree'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Integration with KubeSentiment API\n",
    "\n",
    "Let's connect to the actual KubeSentiment service and compare local vs API results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare local model vs API\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "def compare_local_vs_api(texts: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Compare local model predictions with API predictions.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for text in texts:\n",
    "        # Local prediction\n",
    "        local_result = analyze_sentiments_batch([text])[0]\n",
    "\n",
    "        # API prediction\n",
    "        api_result = None\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{API_BASE_URL}/predict\",\n",
    "                json={\"text\": text},\n",
    "                timeout=10\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                api_data = response.json()\n",
    "                api_result = {\n",
    "                    \"label\": api_data[\"label\"],\n",
    "                    \"score\": api_data[\"score\"],\n",
    "                    \"inference_time_ms\": api_data[\"inference_time_ms\"]\n",
    "                }\n",
    "        except Exception as e:\n",
    "            api_result = {\"error\": str(e)}\n",
    "\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"local_label\": local_result[\"label\"],\n",
    "            \"local_score\": local_result[\"score\"],\n",
    "            \"local_time\": local_result[\"inference_time_ms\"],\n",
    "            \"api_result\": api_result\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test comparison\n",
    "test_texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"It's okay.\",\n",
    "    \"Outstanding quality!\",\n",
    "    \"Complete disaster.\"\n",
    "]\n",
    "\n",
    "print(\"üîó Local vs API Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "comparison_results = compare_local_vs_api(test_texts)\n",
    "\n",
    "matches = 0\n",
    "api_available = 0\n",
    "\n",
    "for result in comparison_results:\n",
    "    print(f\"üìù Text: {result['text']}\")\n",
    "    print(f\"   üè† Local: {result['local_label']} ({result['local_score']:.3f}) - {result['local_time']:.2f}ms\")\n",
    "\n",
    "    if result['api_result'] and 'error' not in result['api_result']:\n",
    "        api_available += 1\n",
    "        api_label = result['api_result']['label']\n",
    "        api_score = result['api_result']['score']\n",
    "        api_time = result['api_result']['inference_time_ms']\n",
    "\n",
    "        print(f\"   üåê API:   {api_label} ({api_score:.3f}) - {api_time:.2f}ms\")\n",
    "\n",
    "        # Check if predictions match\n",
    "        if result['local_label'] == api_label:\n",
    "            matches += 1\n",
    "            print(\"   ‚úÖ Match\")\n",
    "        else:\n",
    "            print(\"   ‚ùå Different\")\n",
    "    else:\n",
    "        print(\"   ‚ùå API unavailable\")\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "if api_available > 0:\n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"API available for {api_available}/{len(test_texts)} tests\")\n",
    "    print(f\"Predictions match: {matches}/{api_available} ({matches/api_available:.1%})\")\n",
    "\n",
    "    # Extract timing data for successful API calls\n",
    "    api_times = [r['api_result']['inference_time_ms'] for r in comparison_results\n",
    "                if r['api_result'] and 'error' not in r['api_result']]\n",
    "    local_times = [r['local_time'] for r in comparison_results]\n",
    "\n",
    "    if api_times:\n",
    "        print(f\"Average local time: {sum(local_times)/len(local_times):.2f}ms\")\n",
    "        print(f\"Average API time: {sum(api_times)/len(api_times):.2f}ms\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è API not available - make sure the KubeSentiment service is running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Automated Testing\n",
    "\n",
    "We can integrate automated tests directly into our notebooks using `pytest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test file\n",
    "test_code = \"\"\"\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "def test_model_loading():\n",
    "    # Test that the model can be loaded successfully\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "    assert model is not None, \\\"Model should not be None\\\"\n",
    "\n",
    "def test_positive_sentiment():\n",
    "    # Test that the model correctly identifies positive sentiment\n",
    "    classifier = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "    result = classifier('I love this product!')\n",
    "    assert result[0]['label'] == 'POSITIVE', \\\"Expected POSITIVE sentiment\\\"\n",
    "\"\"\"\n",
    "with open(\"test_model.py\", \"w\") as f:\n",
    "    f.write(test_code)\n",
    "\n",
    "# Run pytest\n",
    "!pytest test_model.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Key Insights and Takeaways\n",
    "\n",
    "### üéØ Model Performance\n",
    "- **DistilBERT shows strong performance** on clear sentiment cases\n",
    "- **High confidence scores** (>0.9) for obviously positive/negative text\n",
    "- **Lower confidence** for neutral, sarcastic, or ambiguous content\n",
    "- **Consistent results** between local and API implementations\n",
    "\n",
    "### üîç Model Limitations\n",
    "- **Struggles with sarcasm** and complex linguistic patterns\n",
    "- **Limited to binary classification** (positive vs negative)\n",
    "- **English-only training** affects non-English text performance\n",
    "- **Context-dependent** - may miss nuanced sentiment\n",
    "\n",
    "### ‚ö° Performance Characteristics\n",
    "- **Fast inference** (~20-50ms per prediction)\n",
    "- **Efficient resource usage** (67MB model size)\n",
    "- **Good for real-time applications**\n",
    "- **Scalable** for production workloads\n",
    "\n",
    "### üÜö Comparison Insights\n",
    "- **Significantly outperforms** simple keyword and rule-based baselines\n",
    "- **Handles complex language patterns** that rule-based systems miss\n",
    "- **Provides confidence scores** for decision-making\n",
    "- **Robust to variations** in text length and structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you understand the model deeply, explore:\n",
    "\n",
    "- **[../production/03_api_testing.ipynb](../production/03_api_testing.ipynb)**: Comprehensive API testing and load testing\n",
    "- **[../production/04_benchmarking_analysis.ipynb](../production/04_benchmarking_analysis.ipynb)**: Performance benchmarking across different hardware\n",
    "- **[../production/05_monitoring_metrics.ipynb](../production/05_monitoring_metrics.ipynb)**: Real-time monitoring and alerting\n",
    "- **[../tutorials/06_development_workflow.ipynb](../tutorials/06_development_workflow.ipynb)**: Development and testing workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Further Reading\n",
    "\n",
    "- **DistilBERT Paper**: [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)\n",
    "- **SST-2 Dataset**: [The Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/)\n",
    "- **Hugging Face Documentation**: [DistilBERT Model Card](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "- **Transformers Library**: [Sentiment Analysis Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline)\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Ready to explore the API testing and benchmarking capabilities?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_exporter": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
