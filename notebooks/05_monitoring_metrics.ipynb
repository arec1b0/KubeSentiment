{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Monitoring & Metrics Analysis: KubeSentiment Observability\n",
    "\n",
    "This notebook explores the comprehensive monitoring and observability stack of KubeSentiment, including metrics collection, alerting, and performance analysis.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the three pillars of observability (metrics, logs, traces)\n",
    "2. Explore Prometheus metrics and Grafana dashboards\n",
    "3. Analyze alerting rules and incident management\n",
    "4. Perform data drift detection and model monitoring\n",
    "5. Understand health checks and readiness probes\n",
    "6. Create custom monitoring dashboards and alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Three Pillars of Observability\n",
    "\n",
    "### 1. **Metrics** - Quantitative Measurements\n",
    "- **System Metrics**: CPU, memory, disk, network\n",
    "- **Application Metrics**: Response times, error rates, throughput\n",
    "- **Business Metrics**: Prediction accuracy, user satisfaction\n",
    "\n",
    "### 2. **Logs** - Discrete Events\n",
    "- **Structured Logging**: JSON format with correlation IDs\n",
    "- **Log Levels**: DEBUG, INFO, WARNING, ERROR, CRITICAL\n",
    "- **Centralized Collection**: Elasticsearch + Kibana or Loki\n",
    "\n",
    "### 3. **Traces** - Request Flow\n",
    "- **Distributed Tracing**: OpenTelemetry integration\n",
    "- **Request Correlation**: Track requests across services\n",
    "- **Performance Analysis**: Identify bottlenecks\n",
    "\n",
    "### KubeSentiment Monitoring Stack\n",
    "```\n",
    "Application Layer\n",
    "    ‚Üì (Metrics)\n",
    "Prometheus ‚Üê Grafana\n",
    "    ‚Üì (Alerts)\n",
    "Alertmanager ‚Üí Slack/Email\n",
    "    ‚Üì (Logs)\n",
    "Loki/Elasticsearch ‚Üê Kibana\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "PROMETHEUS_URL = \"http://localhost:9090\"  # Default Prometheus port\n",
    "API_BASE_URL = \"http://localhost:8000\"    # KubeSentiment API\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìä Prometheus URL: {PROMETHEUS_URL}\")\n",
    "print(f\"üåê API URL: {API_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Loading Monitoring Configuration\n",
    "\n",
    "Let's examine the monitoring configuration files and understand the alerting rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monitoring configurations\n",
    "def load_monitoring_configs():\n",
    "    \"\"\"Load monitoring configuration files.\"\"\"\n",
    "    configs = {}\n",
    "    config_dir = \"../config\"\n",
    "    \n",
    "    # Load Prometheus rules\n",
    "    prometheus_rules_path = f\"{config_dir}/prometheus-rules.yaml\"\n",
    "    if os.path.exists(prometheus_rules_path):\n",
    "        try:\n",
    "            import yaml\n",
    "            with open(prometheus_rules_path, 'r') as f:\n",
    "                configs['prometheus_rules'] = yaml.safe_load(f)\n",
    "            print(\"‚úÖ Prometheus rules loaded\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è PyYAML not available\")\n",
    "    \n",
    "    # Load Alertmanager config\n",
    "    alertmanager_path = f\"{config_dir}/alertmanager-config.yaml\"\n",
    "    if os.path.exists(alertmanager_path):\n",
    "        try:\n",
    "            with open(alertmanager_path, 'r') as f:\n",
    "                configs['alertmanager'] = yaml.safe_load(f)\n",
    "            print(\"‚úÖ Alertmanager config loaded\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Load environments config\n",
    "    environments_path = f\"{config_dir}/environments.yaml\"\n",
    "    if os.path.exists(environments_path):\n",
    "        try:\n",
    "            with open(environments_path, 'r') as f:\n",
    "                configs['environments'] = yaml.safe_load(f)\n",
    "            print(\"‚úÖ Environment configs loaded\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Load configurations\n",
    "monitoring_configs = load_monitoring_configs()\n",
    "\n",
    "print(\"\\nüìä Monitoring Configuration Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analyze Prometheus rules\n",
    "if 'prometheus_rules' in monitoring_configs:\n",
    "    rules = monitoring_configs['prometheus_rules']\n",
    "    print(\"üö® Prometheus Alerting Rules:\")\n",
    "    \n",
    "    if 'groups' in rules:\n",
    "        for group in rules['groups']:\n",
    "            print(f\"\\nüìã Group: {group.get('name', 'Unknown')}\")\n",
    "            \n",
    "            if 'rules' in group:\n",
    "                for rule in group['rules']:\n",
    "                    if 'alert' in rule:\n",
    "                        print(f\"   üö® {rule['alert']}\")\n",
    "                        if 'expr' in rule:\n",
    "                            expr = rule['expr']\n",
    "                            if len(expr) > 60:\n",
    "                                expr = expr[:57] + \"...\"\n",
    "                            print(f\"      Expression: {expr}\")\n",
    "                        if 'for' in rule:\n",
    "                            print(f\"      Duration: {rule['for']}\")\n",
    "                        if 'labels' in rule and 'severity' in rule['labels']:\n",
    "                            print(f\"      Severity: {rule['labels']['severity']}\")\n",
    "    else:\n",
    "        print(\"   No rule groups found\")\n",
    "\n",
    "# Analyze Alertmanager config\n",
    "if 'alertmanager' in monitoring_configs:\n",
    "    am_config = monitoring_configs['alertmanager']\n",
    "    print(\"\\nüì¢ Alertmanager Configuration:\")\n",
    "    \n",
    "    if 'route' in am_config:\n",
    "        route = am_config['route']\n",
    "        print(f\"   üìã Default receiver: {route.get('receiver', 'unknown')}\")\n",
    "        \n",
    "        if 'routes' in route:\n",
    "            print(f\"   üîÄ Routing rules: {len(route['routes'])}\")\n",
    "    \n",
    "    if 'receivers' in am_config:\n",
    "        receivers = am_config['receivers']\n",
    "        print(f\"   üìû Receivers configured: {len(receivers)}\")\n",
    "        \n",
    "        for receiver in receivers:\n",
    "            name = receiver.get('name', 'unknown')\n",
    "            if 'slack_configs' in receiver:\n",
    "                print(f\"      üí¨ Slack: {name}\")\n",
    "            if 'email_configs' in receiver:\n",
    "                print(f\"      üìß Email: {name}\")\n",
    "            if 'pagerduty_configs' in receiver:\n",
    "                print(f\"      üö® PagerDuty: {name}\")\n",
    "\n",
    "# Sample alerting rules for demonstration\n",
    "sample_alerts = {\n",
    "    \"HighErrorRate\": {\n",
    "        \"description\": \"Error rate above 5% for 5 minutes\",\n",
    "        \"severity\": \"critical\",\n",
    "        \"query\": \"rate(http_requests_total{status=~\\\"5..\\\"}[5m]) / rate(http_requests_total[5m]) > 0.05\"\n",
    "    },\n",
    "    \"HighLatency\": {\n",
    "        \"description\": \"95th percentile latency above 500ms for 10 minutes\",\n",
    "        \"severity\": \"warning\",\n",
    "        \"query\": \"histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[10m])) > 0.5\"\n",
    "    },\n",
    "    \"LowThroughput\": {\n",
    "        \"description\": \"Request rate below 10 RPS for 15 minutes\",\n",
    "        \"severity\": \"warning\",\n",
    "        \"query\": \"rate(http_requests_total[15m]) < 10\"\n",
    "    },\n",
    "    \"HighMemoryUsage\": {\n",
    "        \"description\": \"Memory usage above 90% for 5 minutes\",\n",
    "        \"severity\": \"critical\",\n",
    "        \"query\": \"(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) > 0.9\"\n",
    "    },\n",
    "    \"ModelAccuracyDrop\": {\n",
    "        \"description\": \"Model accuracy drops below 85%\",\n",
    "        \"severity\": \"critical\",\n",
    "        \"query\": \"ml_model_accuracy < 0.85\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüö® Sample Alerting Rules:\")\n",
    "for alert_name, alert_info in sample_alerts.items():\n",
    "    print(f\"\\nüì¢ {alert_name} ({alert_info['severity'].upper()})\")\n",
    "    print(f\"   üìù {alert_info['description']}\")\n",
    "    print(f\"   üîç Query: {alert_info['query'][:80]}...\" if len(alert_info['query']) > 80 else f\"   üîç Query: {alert_info['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Generating Sample Metrics Data\n",
    "\n",
    "Let's create realistic monitoring data to demonstrate analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample monitoring data\n",
    "def generate_monitoring_data(hours=24):\n",
    "    \"\"\"Generate sample monitoring data for analysis.\"\"\"\n",
    "    \n",
    "    # Create time series data\n",
    "    timestamps = pd.date_range(\n",
    "        start=datetime.now() - timedelta(hours=hours), \n",
    "        end=datetime.now(), \n",
    "        freq='5min'\n",
    "    )\n",
    "    \n",
    "    data = []\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    for ts in timestamps:\n",
    "        # Base metrics with some variability\n",
    "        base_load = 50 + 30 * np.sin(2 * np.pi * (ts.hour / 24))  # Daily pattern\n",
    "        \n",
    "        # Add some anomalies\n",
    "        anomaly_multiplier = 1.0\n",
    "        if ts.hour in [2, 3, 14, 15]:  # Low traffic periods\n",
    "            anomaly_multiplier = 0.3\n",
    "        elif np.random.random() < 0.05:  # 5% chance of spike\n",
    "            anomaly_multiplier = np.random.uniform(1.5, 3.0)\n",
    "        \n",
    "        # HTTP metrics\n",
    "        requests_per_second = base_load * anomaly_multiplier + np.random.normal(0, 5)\n",
    "        requests_per_second = max(0, requests_per_second)\n",
    "        \n",
    "        error_rate = np.random.beta(2, 98) * 100  # Low error rate, occasional spikes\n",
    "        if np.random.random() < 0.02:  # 2% chance of error spike\n",
    "            error_rate = np.random.uniform(5, 15)\n",
    "        \n",
    "        # Latency metrics (in milliseconds)\n",
    "        p50_latency = 45 + (requests_per_second / 10) + np.random.normal(0, 5)\n",
    "        p95_latency = p50_latency * (1.5 + np.random.exponential(0.5))\n",
    "        p99_latency = p95_latency * (1.3 + np.random.exponential(0.3))\n",
    "        \n",
    "        # System metrics\n",
    "        cpu_usage = 20 + (requests_per_second / 5) + np.random.normal(0, 3)\n",
    "        cpu_usage = min(95, max(5, cpu_usage))\n",
    "        \n",
    "        memory_usage = 45 + (requests_per_second / 8) + np.random.normal(0, 2)\n",
    "        memory_usage = min(90, max(30, memory_usage))\n",
    "        \n",
    "        # Model metrics\n",
    "        model_accuracy = 0.91 + np.random.normal(0, 0.005)\n",
    "        model_accuracy = max(0.85, min(0.95, model_accuracy))\n",
    "        \n",
    "        # Inference time\n",
    "        inference_time = 42 + (cpu_usage / 10) + np.random.normal(0, 3)\n",
    "        inference_time = max(20, inference_time)\n",
    "        \n",
    "        # Cache metrics\n",
    "        cache_hit_rate = 0.75 + np.random.normal(0, 0.05)\n",
    "        cache_hit_rate = max(0.6, min(0.9, cache_hit_rate))\n",
    "        \n",
    "        data.append({\n",
    "            \"timestamp\": ts,\n",
    "            \"requests_per_second\": round(requests_per_second, 2),\n",
    "            \"error_rate_percent\": round(error_rate, 3),\n",
    "            \"p50_latency_ms\": round(p50_latency, 2),\n",
    "            \"p95_latency_ms\": round(p95_latency, 2),\n",
    "            \"p99_latency_ms\": round(p99_latency, 2),\n",
    "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
    "            \"memory_usage_percent\": round(memory_usage, 2),\n",
    "            \"model_accuracy\": round(model_accuracy, 4),\n",
    "            \"inference_time_ms\": round(inference_time, 2),\n",
    "            \"cache_hit_rate\": round(cache_hit_rate, 3),\n",
    "            \"active_connections\": int(requests_per_second * 2 + np.random.normal(0, 5))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate sample data\n",
    "monitoring_df = generate_monitoring_data(hours=24)\n",
    "monitoring_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "print(\"üìä Sample Monitoring Data Generated:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìã Data points: {len(monitoring_df)}\")\n",
    "print(f\"‚è±Ô∏è Time range: {monitoring_df.index.min()} to {monitoring_df.index.max()}\")\n",
    "print(f\"üìä Metrics collected: {len(monitoring_df.columns)}\")\n",
    "\n",
    "print(\"\\nüîç Data Preview:\")\n",
    "display(monitoring_df.head())\n",
    "\n",
    "print(\"\\nüìà Summary Statistics:\")\n",
    "display(monitoring_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Time Series Analysis\n",
    "\n",
    "Let's analyze the monitoring data over time to understand system behavior and identify patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis and visualization\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 20))\n",
    "fig.suptitle('KubeSentiment Monitoring Dashboard - 24 Hour Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Request throughput over time\n",
    "axes[0, 0].plot(monitoring_df.index, monitoring_df['requests_per_second'], linewidth=2, color='blue')\n",
    "axes[0, 0].set_title('Request Throughput (RPS)')\n",
    "axes[0, 0].set_ylabel('Requests/sec')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].axhline(monitoring_df['requests_per_second'].mean(), color='red', linestyle='--', alpha=0.7,\n",
    "                   label=f'Mean: {monitoring_df[\"requests_per_second\"].mean():.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Error rate over time\n",
    "axes[0, 1].plot(monitoring_df.index, monitoring_df['error_rate_percent'], linewidth=2, color='red')\n",
    "axes[0, 1].set_title('Error Rate (%)')\n",
    "axes[0, 1].set_ylabel('Error Rate (%)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(5, color='orange', linestyle='--', alpha=0.7, label='Alert Threshold: 5%')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Latency percentiles\n",
    "axes[1, 0].plot(monitoring_df.index, monitoring_df['p50_latency_ms'], label='P50', linewidth=2, color='green')\n",
    "axes[1, 0].plot(monitoring_df.index, monitoring_df['p95_latency_ms'], label='P95', linewidth=2, color='orange')\n",
    "axes[1, 0].plot(monitoring_df.index, monitoring_df['p99_latency_ms'], label='P99', linewidth=2, color='red')\n",
    "axes[1, 0].set_title('Response Latency Percentiles')\n",
    "axes[1, 0].set_ylabel('Latency (ms)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].axhline(500, color='red', linestyle='--', alpha=0.7, label='Alert Threshold')\n",
    "\n",
    "# 4. System resource usage\n",
    "axes[1, 1].plot(monitoring_df.index, monitoring_df['cpu_usage_percent'], label='CPU', linewidth=2, color='purple')\n",
    "axes[1, 1].plot(monitoring_df.index, monitoring_df['memory_usage_percent'], label='Memory', linewidth=2, color='brown')\n",
    "axes[1, 1].set_title('System Resource Usage')\n",
    "axes[1, 1].set_ylabel('Usage (%)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(80, color='red', linestyle='--', alpha=0.7, label='High Usage Threshold')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# 5. Model performance metrics\n",
    "axes[2, 0].plot(monitoring_df.index, monitoring_df['model_accuracy'] * 100, linewidth=2, color='darkgreen')\n",
    "axes[2, 0].set_title('Model Accuracy Over Time')\n",
    "axes[2, 0].set_ylabel('Accuracy (%)')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "axes[2, 0].axhline(85, color='red', linestyle='--', alpha=0.7, label='Accuracy Threshold')\n",
    "axes[2, 0].legend()\n",
    "\n",
    "# 6. Inference performance\n",
    "axes[2, 1].plot(monitoring_df.index, monitoring_df['inference_time_ms'], linewidth=2, color='teal')\n",
    "axes[2, 1].set_title('Model Inference Time')\n",
    "axes[2, 1].set_ylabel('Time (ms)')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "axes[2, 1].axhline(100, color='red', linestyle='--', alpha=0.7, label='Latency Threshold')\n",
    "axes[2, 1].legend()\n",
    "\n",
    "# 7. Cache performance\n",
    "axes[3, 0].plot(monitoring_df.index, monitoring_df['cache_hit_rate'] * 100, linewidth=2, color='magenta')\n",
    "axes[3, 0].set_title('Cache Hit Rate')\n",
    "axes[3, 0].set_ylabel('Hit Rate (%)')\n",
    "axes[3, 0].grid(True, alpha=0.3)\n",
    "axes[3, 0].axhline(70, color='green', linestyle='--', alpha=0.7, label='Good Performance')\n",
    "axes[3, 0].legend()\n",
    "\n",
    "# 8. Active connections\n",
    "axes[3, 1].plot(monitoring_df.index, monitoring_df['active_connections'], linewidth=2, color='olive')\n",
    "axes[3, 1].set_title('Active Connections')\n",
    "axes[3, 1].set_ylabel('Connections')\n",
    "axes[3, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate key metrics\n",
    "print(\"üìä Key Performance Metrics (24h):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Average RPS\": f\"{monitoring_df['requests_per_second'].mean():.1f}\",\n",
    "    \"Peak RPS\": f\"{monitoring_df['requests_per_second'].max():.1f}\",\n",
    "    \"Average Error Rate\": f\"{monitoring_df['error_rate_percent'].mean():.2f}%\",\n",
    "    \"Max Error Rate\": f\"{monitoring_df['error_rate_percent'].max():.2f}%\",\n",
    "    \"Average P95 Latency\": f\"{monitoring_df['p95_latency_ms'].mean():.1f}ms\",\n",
    "    \"Max P95 Latency\": f\"{monitoring_df['p95_latency_ms'].max():.1f}ms\",\n",
    "    \"Average CPU Usage\": f\"{monitoring_df['cpu_usage_percent'].mean():.1f}%\",\n",
    "    \"Average Memory Usage\": f\"{monitoring_df['memory_usage_percent'].mean():.1f}%\",\n",
    "    \"Model Accuracy\": f\"{monitoring_df['model_accuracy'].mean():.1%}\",\n",
    "    \"Cache Hit Rate\": f\"{monitoring_df['cache_hit_rate'].mean():.1%}\",\n",
    "    \"Average Inference Time\": f\"{monitoring_df['inference_time_ms'].mean():.1f}ms\"\n",
    "}\n",
    "\n",
    "for metric, value in metrics_summary.items():\n",
    "    print(f\"{metric:<25}: {value}\")\n",
    "\n",
    "# Identify potential issues\n",
    "issues = []\n",
    "if monitoring_df['error_rate_percent'].max() > 5:\n",
    "    issues.append(\"High error rate detected\")\n",
    "if monitoring_df['p95_latency_ms'].max() > 500:\n",
    "    issues.append(\"High latency spikes detected\")\n",
    "if monitoring_df['cpu_usage_percent'].max() > 80:\n",
    "    issues.append(\"High CPU usage detected\")\n",
    "if monitoring_df['memory_usage_percent'].max() > 85:\n",
    "    issues.append(\"High memory usage detected\")\n",
    "if monitoring_df['model_accuracy'].min() < 0.85:\n",
    "    issues.append(\"Model accuracy degradation detected\")\n",
    "\n",
    "if issues:\n",
    "    print(f\"\\nüö® Potential Issues Detected ({len(issues)}):\")\n",
    "    for issue in issues:\n",
    "        print(f\"   ‚Ä¢ {issue}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No critical issues detected in the monitoring period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Alert Analysis and Incident Detection\n",
    "\n",
    "Let's analyze potential alerts and create an alerting dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alert analysis and incident detection\n",
    "def analyze_alerts(monitoring_df, alert_rules):\n",
    "    \"\"\"Analyze monitoring data against alert rules.\"\"\"\n",
    "    \n",
    "    alerts_triggered = []\n",
    "    \n",
    "    for alert_name, rule in alert_rules.items():\n",
    "        severity = rule['severity']\n",
    "        description = rule['description']\n",
    "        \n",
    "        # Simple rule evaluation (in real implementation, this would be PromQL)\n",
    "        violations = []\n",
    "        \n",
    "        if alert_name == \"HighErrorRate\":\n",
    "            violations = monitoring_df[monitoring_df['error_rate_percent'] > 5].index.tolist()\n",
    "        elif alert_name == \"HighLatency\":\n",
    "            violations = monitoring_df[monitoring_df['p95_latency_ms'] > 500].index.tolist()\n",
    "        elif alert_name == \"LowThroughput\":\n",
    "            violations = monitoring_df[monitoring_df['requests_per_second'] < 10].index.tolist()\n",
    "        elif alert_name == \"HighMemoryUsage\":\n",
    "            violations = monitoring_df[monitoring_df['memory_usage_percent'] > 90].index.tolist()\n",
    "        elif alert_name == \"ModelAccuracyDrop\":\n",
    "            violations = monitoring_df[monitoring_df['model_accuracy'] < 0.85].index.tolist()\n",
    "        \n",
    "        if violations:\n",
    "            alerts_triggered.append({\n",
    "                \"alert_name\": alert_name,\n",
    "                \"severity\": severity,\n",
    "                \"description\": description,\n",
    "                \"violations_count\": len(violations),\n",
    "                \"first_violation\": violations[0],\n",
    "                \"last_violation\": violations[-1],\n",
    "                \"violation_percentage\": len(violations) / len(monitoring_df) * 100\n",
    "            })\n",
    "    \n",
    "    return alerts_triggered\n",
    "\n",
    "# Analyze alerts\n",
    "alerts_detected = analyze_alerts(monitoring_df, sample_alerts)\n",
    "\n",
    "print(\"üö® Alert Analysis Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if alerts_detected:\n",
    "    print(f\"üì¢ {len(alerts_detected)} alerts would have been triggered:\")\n",
    "    \n",
    "    for alert in alerts_detected:\n",
    "        severity_color = \"üî¥\" if alert['severity'] == 'critical' else \"üü†\" if alert['severity'] == 'warning' else \"üü¢\"\n",
    "        print(f\"\\n{severity_color} {alert['alert_name']} ({alert['severity'].upper()})\")\n",
    "        print(f\"   üìù {alert['description']}\")\n",
    "        print(f\"   üî¢ Violations: {alert['violations_count']} times\")\n",
    "        print(f\"   üìä Violation rate: {alert['violation_percentage']:.1f}%\")\n",
    "        print(f\"   üïí First: {alert['first_violation']}\")\n",
    "        print(f\"   üïí Last: {alert['last_violation']}\")\n",
    "\n",
    "    # Alert severity distribution\n",
    "    severity_counts = {}\n",
    "    for alert in alerts_detected:\n",
    "        severity_counts[alert['severity']] = severity_counts.get(alert['severity'], 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìà Alert Severity Distribution:\")\n",
    "    for severity, count in severity_counts.items():\n",
    "        print(f\"   {severity.upper()}: {count} alerts\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úÖ No alerts would have been triggered in this period\")\n",
    "\n",
    "# Create alert timeline visualization\n",
    "if alerts_detected:\n",
    "    fig, axes = plt.subplots(len(alerts_detected), 1, figsize=(12, 4 * len(alerts_detected)))\n",
    "    fig.suptitle('Alert Timeline Analysis', fontsize=16)\n",
    "    \n",
    "    if len(alerts_detected) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, alert in enumerate(alerts_detected):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot the relevant metric\n",
    "        if alert['alert_name'] == \"HighErrorRate\":\n",
    "            metric_data = monitoring_df['error_rate_percent']\n",
    "            threshold = 5\n",
    "            ylabel = 'Error Rate (%)'\n",
    "        elif alert['alert_name'] == \"HighLatency\":\n",
    "            metric_data = monitoring_df['p95_latency_ms']\n",
    "            threshold = 500\n",
    "            ylabel = 'P95 Latency (ms)'\n",
    "        elif alert['alert_name'] == \"LowThroughput\":\n",
    "            metric_data = monitoring_df['requests_per_second']\n",
    "            threshold = 10\n",
    "            ylabel = 'Requests/sec'\n",
    "        elif alert['alert_name'] == \"HighMemoryUsage\":\n",
    "            metric_data = monitoring_df['memory_usage_percent']\n",
    "            threshold = 90\n",
    "            ylabel = 'Memory Usage (%)'\n",
    "        elif alert['alert_name'] == \"ModelAccuracyDrop\":\n",
    "            metric_data = monitoring_df['model_accuracy'] * 100\n",
    "            threshold = 85\n",
    "            ylabel = 'Model Accuracy (%)'\n",
    "        \n",
    "        ax.plot(monitoring_df.index, metric_data, linewidth=2, color='blue', label='Actual Value')\n",
    "        ax.axhline(threshold, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Threshold: {threshold}')\n",
    "        ax.fill_between(monitoring_df.index, metric_data, threshold, \n",
    "                       where=(metric_data > threshold), color='red', alpha=0.3, label='Alert Zone')\n",
    "        \n",
    "        ax.set_title(f\"{alert['alert_name']} - {alert['description']}\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "        \n",
    "        # Rotate x-axis labels\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Simulate real-time alerting\n",
    "def simulate_real_time_monitoring(monitoring_df, check_interval_minutes=5):\n",
    "    \"\"\"Simulate real-time monitoring and alerting.\"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Real-Time Monitoring Simulation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check last few data points\n",
    "    recent_data = monitoring_df.tail(12)  # Last hour (12 * 5min intervals)\n",
    "    \n",
    "    active_alerts = []\n",
    "    \n",
    "    # Check each alert rule\n",
    "    latest_values = recent_data.iloc[-1]\n",
    "    \n",
    "    if latest_values['error_rate_percent'] > 5:\n",
    "        active_alerts.append(\"High Error Rate\")\n",
    "    \n",
    "    if latest_values['p95_latency_ms'] > 500:\n",
    "        active_alerts.append(\"High Latency\")\n",
    "    \n",
    "    if latest_values['requests_per_second'] < 10:\n",
    "        active_alerts.append(\"Low Throughput\")\n",
    "    \n",
    "    if latest_values['memory_usage_percent'] > 90:\n",
    "        active_alerts.append(\"High Memory Usage\")\n",
    "    \n",
    "    if latest_values['model_accuracy'] < 0.85:\n",
    "        active_alerts.append(\"Model Accuracy Drop\")\n",
    "    \n",
    "    if active_alerts:\n",
    "        print(f\"üö® ACTIVE ALERTS ({len(active_alerts)}):\")\n",
    "        for alert in active_alerts:\n",
    "            print(f\"   ‚Ä¢ {alert}\")\n",
    "        \n",
    "        print(\"\\nüì¢ Recommended Actions:\")\n",
    "        if \"High Error Rate\" in active_alerts:\n",
    "            print(\"   ‚Ä¢ Check application logs for error patterns\")\n",
    "            print(\"   ‚Ä¢ Review recent deployments or configuration changes\")\n",
    "        if \"High Latency\" in active_alerts:\n",
    "            print(\"   ‚Ä¢ Check system resource utilization\")\n",
    "            print(\"   ‚Ä¢ Consider scaling up instance size\")\n",
    "        if \"Low Throughput\" in active_alerts:\n",
    "            print(\"   ‚Ä¢ Investigate potential service outages\")\n",
    "            print(\"   ‚Ä¢ Check network connectivity\")\n",
    "        if \"High Memory Usage\" in active_alerts:\n",
    "            print(\"   ‚Ä¢ Monitor for memory leaks\")\n",
    "            print(\"   ‚Ä¢ Consider increasing instance memory\")\n",
    "        if \"Model Accuracy Drop\" in active_alerts:\n",
    "            print(\"   ‚Ä¢ Check for data drift\")\n",
    "            print(\"   ‚Ä¢ Consider model retraining\")\n",
    "    else:\n",
    "        print(\"‚úÖ All systems operating normally\")\n",
    "        print(f\"   üìä Current RPS: {latest_values['requests_per_second']:.1f}\")\n",
    "        print(f\"   üìà Current P95 Latency: {latest_values['p95_latency_ms']:.1f}ms\")\n",
    "        print(f\"   üéØ Model Accuracy: {latest_values['model_accuracy']:.1%}\")\n",
    "\n",
    "simulate_real_time_monitoring(monitoring_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Data Drift Detection\n",
    "\n",
    "Let's implement data drift detection using statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data drift detection\n",
    "def detect_data_drift(reference_data, current_data, feature_name, threshold=0.05):\n",
    "    \"\"\"Detect data drift using statistical tests.\"\"\"\n",
    "    \n",
    "    from scipy.stats import ks_2samp, ttest_ind\n",
    "    \n",
    "    # Kolmogorov-Smirnov test\n",
    "    ks_stat, ks_p_value = ks_2samp(reference_data, current_data)\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, t_p_value = ttest_ind(reference_data, current_data)\n",
    "    \n",
    "    # Simple drift detection based on distribution change\n",
    "    ref_mean, ref_std = np.mean(reference_data), np.std(reference_data)\n",
    "    curr_mean, curr_std = np.mean(current_data), np.std(current_data)\n",
    "    \n",
    "    # Calculate drift score (normalized difference)\n",
    "    mean_drift = abs(curr_mean - ref_mean) / ref_std if ref_std > 0 else 0\n",
    "    std_drift = abs(curr_std - ref_std) / ref_std if ref_std > 0 else 0\n",
    "    \n",
    "    drift_score = (mean_drift + std_drift) / 2\n",
    "    \n",
    "    return {\n",
    "        \"feature\": feature_name,\n",
    "        \"drift_detected\": drift_score > threshold,\n",
    "        \"drift_score\": drift_score,\n",
    "        \"threshold\": threshold,\n",
    "        \"ks_p_value\": ks_p_value,\n",
    "        \"t_p_value\": t_p_value,\n",
    "        \"reference_mean\": ref_mean,\n",
    "        \"current_mean\": curr_mean,\n",
    "        \"reference_std\": ref_std,\n",
    "        \"current_std\": curr_std\n",
    "    }\n",
    "\n",
    "# Simulate data drift analysis\n",
    "def analyze_model_drift(monitoring_df, reference_window_hours=6):\n",
    "    \"\"\"Analyze model performance drift over time.\"\"\"\n",
    "    \n",
    "    # Split data into reference and current windows\n",
    "    midpoint = len(monitoring_df) // 2\n",
    "    reference_data = monitoring_df.iloc[:midpoint]\n",
    "    current_data = monitoring_df.iloc[midpoint:]\n",
    "    \n",
    "    # Features to monitor for drift\n",
    "    drift_features = {\n",
    "        \"model_accuracy\": \"Model Accuracy\",\n",
    "        \"inference_time_ms\": \"Inference Time\",\n",
    "        \"p95_latency_ms\": \"P95 Latency\",\n",
    "        \"error_rate_percent\": \"Error Rate\"\n",
    "    }\n",
    "    \n",
    "    drift_results = []\n",
    "    \n",
    "    for feature, display_name in drift_features.items():\n",
    "        if feature in monitoring_df.columns:\n",
    "            result = detect_data_drift(\n",
    "                reference_data[feature].values,\n",
    "                current_data[feature].values,\n",
    "                display_name\n",
    "            )\n",
    "            drift_results.append(result)\n",
    "    \n",
    "    return drift_results\n",
    "\n",
    "# Analyze drift\n",
    "drift_analysis = analyze_model_drift(monitoring_df)\n",
    "\n",
    "print(\"üîç Data Drift Detection Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "drift_detected = False\n",
    "for result in drift_analysis:\n",
    "    status = \"üö® DRIFT DETECTED\" if result['drift_detected'] else \"‚úÖ STABLE\"\n",
    "    print(f\"\\n{status} - {result['feature']}\")\n",
    "    print(f\"   üìä Drift Score: {result['drift_score']:.3f} (threshold: {result['threshold']})\")\n",
    "    print(f\"   üìà Reference Mean: {result['reference_mean']:.3f}\")\n",
    "    print(f\"   üìâ Current Mean: {result['current_mean']:.3f}\")\n",
    "    print(f\"   üìè Reference Std: {result['reference_std']:.3f}\")\n",
    "    print(f\"   üìê Current Std: {result['current_std']:.3f}\")\n",
    "    print(f\"   üî¨ KS Test p-value: {result['ks_p_value']:.4f}\")\n",
    "    \n",
    "    if result['drift_detected']:\n",
    "        drift_detected = True\n",
    "        print(\"   ‚ö†Ô∏è  Recommendation: Investigate data distribution changes\")\n",
    "\n",
    "if drift_detected:\n",
    "    print(f\"\\nüö® SUMMARY: Data drift detected in {sum(1 for r in drift_analysis if r['drift_detected'])} metrics\")\n",
    "    print(\"   üìã Recommended actions:\")\n",
    "    print(\"   ‚Ä¢ Review recent data collection changes\")\n",
    "    print(\"   ‚Ä¢ Check for changes in user behavior\")\n",
    "    print(\"   ‚Ä¢ Consider model retraining if drift persists\")\n",
    "    print(\"   ‚Ä¢ Update model monitoring thresholds\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ SUMMARY: No significant data drift detected\")\n",
    "    print(\"   üìã System operating within normal parameters\")\n",
    "\n",
    "# Visualize drift for key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Data Drift Analysis - Reference vs Current Distribution', fontsize=16)\n",
    "\n",
    "# Split data for visualization\n",
    "midpoint = len(monitoring_df) // 2\n",
    "reference_data = monitoring_df.iloc[:midpoint]\n",
    "current_data = monitoring_df.iloc[midpoint:]\n",
    "\n",
    "drift_features = ['model_accuracy', 'inference_time_ms', 'p95_latency_ms', 'error_rate_percent']\n",
    "feature_names = ['Model Accuracy', 'Inference Time (ms)', 'P95 Latency (ms)', 'Error Rate (%)']\n",
    "\n",
    "for i, (feature, name) in enumerate(zip(drift_features, feature_names)):\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    \n",
    "    # Plot distributions\n",
    "    ax.hist(reference_data[feature], alpha=0.7, label='Reference', bins=20, density=True)\n",
    "    ax.hist(current_data[feature], alpha=0.7, label='Current', bins=20, density=True)\n",
    "    \n",
    "    ax.set_title(f'{name} Distribution')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create drift monitoring dashboard\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot drift scores\n",
    "features = [r['feature'] for r in drift_analysis]\n",
    "drift_scores = [r['drift_score'] for r in drift_analysis]\n",
    "thresholds = [r['threshold'] for r in drift_analysis]\n",
    "\n",
    "x = np.arange(len(features))\n",
    "bars = ax.bar(x, drift_scores, alpha=0.7, color='skyblue', label='Drift Score')\n",
    "ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='Drift Threshold')\n",
    "\n",
    "# Color bars based on drift detection\n",
    "for i, (bar, score, threshold) in enumerate(zip(bars, drift_scores, thresholds)):\n",
    "    if score > threshold:\n",
    "        bar.set_color('red')\n",
    "        bar.set_alpha(0.9)\n",
    "\n",
    "ax.set_xlabel('Metrics')\n",
    "ax.set_ylabel('Drift Score')\n",
    "ax.set_title('Data Drift Detection Dashboard')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(features, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, drift_scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "            f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Custom Dashboard Creation\n",
    "\n",
    "Let's create a custom monitoring dashboard with key metrics and alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive monitoring dashboard\n",
    "def create_monitoring_dashboard(monitoring_df, alerts_detected, drift_analysis):\n",
    "    \"\"\"Create a comprehensive monitoring dashboard.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 20))\n",
    "    fig.suptitle('KubeSentiment - Comprehensive Monitoring Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Current status summary (top row)\n",
    "    latest = monitoring_df.iloc[-1]\n",
    "    \n",
    "    # Status indicators\n",
    "    status_data = {\n",
    "        'RPS': {'value': latest['requests_per_second'], 'threshold': 10, 'unit': 'req/s'},\n",
    "        'Error Rate': {'value': latest['error_rate_percent'], 'threshold': 5, 'unit': '%'},\n",
    "        'P95 Latency': {'value': latest['p95_latency_ms'], 'threshold': 500, 'unit': 'ms'}\n",
    "    }\n",
    "    \n",
    "    colors = []\n",
    "    for i, (metric, data) in enumerate(status_data.items()):\n",
    "        color = 'green' if data['value'] < data['threshold'] else 'red'\n",
    "        colors.append(color)\n",
    "        \n",
    "        axes[0, i].text(0.5, 0.7, f\"{data['value']:.1f} {data['unit']}\", \n",
    "                       ha='center', va='center', fontsize=24, color=color, fontweight='bold')\n",
    "        axes[0, i].text(0.5, 0.3, metric, ha='center', va='center', fontsize=12)\n",
    "        axes[0, i].set_xlim(0, 1)\n",
    "        axes[0, i].set_ylim(0, 1)\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Add threshold indicator\n",
    "        axes[0, i].text(0.5, 0.1, f\"Threshold: {data['threshold']} {data['unit']}\", \n",
    "                       ha='center', va='center', fontsize=8, color='gray')\n",
    "    \n",
    "    # Overall system health score\n",
    "    health_score = 100\n",
    "    if latest['error_rate_percent'] > 5:\n",
    "        health_score -= 30\n",
    "    if latest['p95_latency_ms'] > 500:\n",
    "        health_score -= 20\n",
    "    if latest['cpu_usage_percent'] > 80:\n",
    "        health_score -= 15\n",
    "    if latest['memory_usage_percent'] > 85:\n",
    "        health_score -= 15\n",
    "    if latest['model_accuracy'] < 0.85:\n",
    "        health_score -= 20\n",
    "    \n",
    "    health_color = 'green' if health_score > 80 else 'orange' if health_score > 60 else 'red'\n",
    "    axes[0, 2].text(0.5, 0.7, f\"{health_score}%\", ha='center', va='center', \n",
    "                    fontsize=24, color=health_color, fontweight='bold')\n",
    "    axes[0, 2].text(0.5, 0.3, 'System Health', ha='center', va='center', fontsize=12)\n",
    "    axes[0, 2].text(0.5, 0.1, 'Composite Score', ha='center', va='center', fontsize=8, color='gray')\n",
    "    \n",
    "    # Time series plots (second row)\n",
    "    time_window = monitoring_df.tail(60)  # Last 5 hours\n",
    "    \n",
    "    axes[1, 0].plot(time_window.index, time_window['requests_per_second'], color='blue', linewidth=2)\n",
    "    axes[1, 0].set_title('Throughput (Last 5h)')\n",
    "    axes[1, 0].set_ylabel('RPS')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 1].plot(time_window.index, time_window['p95_latency_ms'], color='orange', linewidth=2)\n",
    "    axes[1, 1].axhline(500, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1, 1].set_title('Latency (Last 5h)')\n",
    "    axes[1, 1].set_ylabel('P95 Latency (ms)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 2].plot(time_window.index, time_window['model_accuracy'] * 100, color='green', linewidth=2)\n",
    "    axes[1, 2].axhline(85, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[1, 2].set_title('Model Accuracy (Last 5h)')\n",
    "    axes[1, 2].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Resource utilization (third row)\n",
    "    axes[2, 0].plot(time_window.index, time_window['cpu_usage_percent'], label='CPU', color='purple')\n",
    "    axes[2, 0].plot(time_window.index, time_window['memory_usage_percent'], label='Memory', color='brown')\n",
    "    axes[2, 0].axhline(80, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[2, 0].set_title('Resource Utilization')\n",
    "    axes[2, 0].set_ylabel('Usage (%)')\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cache performance\n",
    "    axes[2, 1].plot(time_window.index, time_window['cache_hit_rate'] * 100, color='magenta')\n",
    "    axes[2, 1].axhline(70, color='green', linestyle='--', alpha=0.7)\n",
    "    axes[2, 1].set_title('Cache Performance')\n",
    "    axes[2, 1].set_ylabel('Hit Rate (%)')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Error rate\n",
    "    axes[2, 2].plot(time_window.index, time_window['error_rate_percent'], color='red')\n",
    "    axes[2, 2].axhline(5, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[2, 2].set_title('Error Rate')\n",
    "    axes[2, 2].set_ylabel('Error Rate (%)')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Alerts and drift summary (bottom row)\n",
    "    axes[3, 0].axis('off')\n",
    "    alert_summary = f\"\"\"Active Alerts: {len(alerts_detected)}\n",
    "\n",
    "\"\"\"\n",
    "    for alert in alerts_detected[:3]:  # Show top 3\n",
    "        alert_summary += f\"üö® {alert['alert_name']}\\n   {alert['severity'].upper()}\\n\\n\"\n",
    "    \n",
    "    axes[3, 0].text(0.1, 0.9, alert_summary, transform=axes[3, 0].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.5))\n",
    "    axes[3, 0].set_title('Alert Status')\n",
    "    \n",
    "    # Drift status\n",
    "    axes[3, 1].axis('off')\n",
    "    drift_summary = f\"\"\"Drift Detection\\n\\n\"\"\"\n",
    "    drift_issues = sum(1 for r in drift_analysis if r['drift_detected'])\n",
    "    drift_summary += f\"Issues Found: {drift_issues}/{len(drift_analysis)}\\n\\n\"\n",
    "    \n",
    "    for result in drift_analysis:\n",
    "        status = \"‚ùå\" if result['drift_detected'] else \"‚úÖ\"\n",
    "        drift_summary += f\"{status} {result['feature'][:15]}\\n   {result['drift_score']:.3f}\\n\\n\"\n",
    "    \n",
    "    axes[3, 1].text(0.1, 0.9, drift_summary, transform=axes[3, 1].transAxes, \n",
    "                    fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "    axes[3, 1].set_title('Drift Status')\n",
    "    \n",
    "    # Recommendations\n",
    "    axes[3, 2].axis('off')\n",
    "    recommendations = \"\"\"Recommendations:\\n\\n\"\"\"\n",
    "    \n",
    "    if alerts_detected:\n",
    "        recommendations += \"‚Ä¢ Review active alerts\\n‚Ä¢ Check system resources\\n\"\n",
    "    else:\n",
    "        recommendations += \"‚Ä¢ System operating normally\\n‚Ä¢ Monitor trends\\n\"\n",
    "    \n",
    "    if drift_issues > 0:\n",
    "        recommendations += \"‚Ä¢ Investigate data drift\\n‚Ä¢ Consider model retraining\\n\"\n",
    "    \n",
    "    if latest['cpu_usage_percent'] > 70:\n",
    "        recommendations += \"‚Ä¢ Monitor CPU utilization\\n\"\n",
    "    \n",
    "    if latest['cache_hit_rate'] < 0.7:\n",
    "        recommendations += \"‚Ä¢ Optimize cache performance\\n\"\n",
    "    \n",
    "    recommendations += \"\\n‚Ä¢ Regular maintenance\\n‚Ä¢ Performance monitoring\"\n",
    "    \n",
    "    axes[3, 2].text(0.1, 0.9, recommendations, transform=axes[3, 2].transAxes, \n",
    "                    fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.5))\n",
    "    axes[3, 2].set_title('Action Items')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the comprehensive dashboard\n",
    "create_monitoring_dashboard(monitoring_df, alerts_detected, drift_analysis)\n",
    "\n",
    "print(\"üéâ Monitoring Analysis Complete!\")\n",
    "print(\"\\nüìã Dashboard Summary:\")\n",
    "print(f\"   üìä Monitoring period: {len(monitoring_df)} data points\")\n",
    "print(f\"   üö® Alerts detected: {len(alerts_detected)}\")\n",
    "print(f\"   üîç Drift issues: {sum(1 for r in drift_analysis if r['drift_detected'])}\")\n",
    "print(f\"   üìà Average RPS: {monitoring_df['requests_per_second'].mean():.1f}\")\n",
    "print(f\"   ‚è±Ô∏è Average P95 latency: {monitoring_df['p95_latency_ms'].mean():.1f}ms\")\n",
    "print(f\"   üéØ Model accuracy: {monitoring_df['model_accuracy'].mean():.1%}\")\n",
    "print(f\"   üíæ Cache hit rate: {monitoring_df['cache_hit_rate'].mean():.1%}\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   ‚Ä¢ Set up real Prometheus + Grafana monitoring\")\n",
    "print(\"   ‚Ä¢ Configure alerting rules in production\")\n",
    "print(\"   ‚Ä¢ Implement automated drift detection\")\n",
    "print(\"   ‚Ä¢ Create custom Grafana dashboards\")\n",
    "print(\"   ‚Ä¢ Set up log aggregation with Loki/Elasticsearch\")\n",
    "print(\"\\nüí° Pro Tip: Use this notebook as a template for creating custom monitoring dashboards!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
