{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Deployment Guide: Production Infrastructure & Scaling\n",
    "\n",
    "This notebook provides comprehensive guidance for deploying KubeSentiment to production environments, covering infrastructure setup, scaling strategies, and operational best practices.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand deployment strategies and environments\n",
    "2. Learn infrastructure provisioning with Terraform\n",
    "3. Master Kubernetes deployment with Helm\n",
    "4. Implement scaling and high availability\n",
    "5. Configure monitoring and alerting in production\n",
    "6. Understand backup, recovery, and disaster recovery\n",
    "7. Optimize costs and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Deployment Architecture\n",
    "\n",
    "### Environment Strategy\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Development   ‚îÇ -> ‚îÇ    Staging      ‚îÇ -> ‚îÇ  Production     ‚îÇ\n",
    "‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ\n",
    "‚îÇ ‚Ä¢ Local testing ‚îÇ    ‚îÇ ‚Ä¢ Integration   ‚îÇ    ‚îÇ ‚Ä¢ Live traffic  ‚îÇ\n",
    "‚îÇ ‚Ä¢ Fast feedback ‚îÇ    ‚îÇ ‚Ä¢ Load testing  ‚îÇ    ‚îÇ ‚Ä¢ High avail.   ‚îÇ\n",
    "‚îÇ ‚Ä¢ Cost: Low     ‚îÇ    ‚îÇ ‚Ä¢ User acceptance‚îÇ    ‚îÇ ‚Ä¢ Auto-scaling  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Infrastructure Components\n",
    "\n",
    "- **Kubernetes Cluster**: Container orchestration\n",
    "- **Load Balancer**: Traffic distribution\n",
    "- **Monitoring Stack**: Prometheus + Grafana\n",
    "- **Logging**: ELK stack or Loki\n",
    "- **CI/CD Pipeline**: GitHub Actions\n",
    "- **Secret Management**: Kubernetes secrets or external vault\n",
    "- **Database**: For metadata and caching (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "PROJECT_ROOT = Path(\"..\")\n",
    "INFRA_DIR = PROJECT_ROOT / \"infrastructure\"\n",
    "HELM_DIR = PROJECT_ROOT / \"helm\"\n",
    "DOCKER_DIR = PROJECT_ROOT\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Infrastructure directory: {INFRA_DIR.absolute()}\")\n",
    "print(f\"‚öìÔ∏è Helm directory: {HELM_DIR.absolute()}\")\n",
    "print(f\"üê≥ Docker directory: {DOCKER_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Infrastructure as Code\n",
    "\n",
    "Let's examine the Terraform infrastructure configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze infrastructure configuration\n",
    "def analyze_infrastructure():\n",
    "    \"\"\"Analyze the infrastructure as code setup.\"\"\"\n",
    "    \n",
    "    infra_analysis = {}\n",
    "    \n",
    "    # Check Terraform files\n",
    "    terraform_files = list(INFRA_DIR.rglob(\"*.tf\"))\n",
    "    infra_analysis['terraform_files'] = []\n",
    "    \n",
    "    for tf_file in terraform_files:\n",
    "        try:\n",
    "            with open(tf_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                \n",
    "            infra_analysis['terraform_files'].append({\n",
    "                \"file\": str(tf_file.relative_to(PROJECT_ROOT)),\n",
    "                \"size\": len(content),\n",
    "                \"lines\": len(content.split('\\n')),\n",
    "                \"resources\": content.count('resource \"'),\n",
    "                \"variables\": content.count('variable \"'),\n",
    "                \"outputs\": content.count('output \"')\n",
    "            })\n",
    "        except Exception as e:\n",
    "            infra_analysis['terraform_files'].append({\n",
    "                \"file\": str(tf_file.relative_to(PROJECT_ROOT)),\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Check Helm charts\n",
    "    helm_files = list(HELM_DIR.rglob(\"*\")\n",
    "    helm_analysis = []\n",
    "    \n",
    "    for helm_file in helm_files:\n",
    "        if helm_file.is_file():\n",
    "            try:\n",
    "                with open(helm_file, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                helm_analysis.append({\n",
    "                    \"file\": str(helm_file.relative_to(PROJECT_ROOT)),\n",
    "                    \"size\": len(content),\n",
    "                    \"lines\": len(content.split('\\n')),\n",
    "                    \"type\": helm_file.suffix\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    infra_analysis['helm_files'] = helm_analysis\n",
    "    \n",
    "    # Check Docker files\n",
    "    docker_files = ['Dockerfile', 'docker-compose.yml', '.dockerignore']\n",
    "    docker_analysis = []\n",
    "    \n",
    "    for docker_file in docker_files:\n",
    "        file_path = PROJECT_ROOT / docker_file\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                docker_analysis.append({\n",
    "                    \"file\": docker_file,\n",
    "                    \"size\": len(content),\n",
    "                    \"lines\": len(content.split('\\n'))\n",
    "                })\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    infra_analysis['docker_files'] = docker_analysis\n",
    "    \n",
    "    return infra_analysis\n",
    "\n",
    "# Analyze infrastructure\n",
    "print(\"üèóÔ∏è Infrastructure Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "infra_analysis = analyze_infrastructure()\n",
    "\n",
    "# Display Terraform analysis\n",
    "if infra_analysis.get('terraform_files'):\n",
    "    print(\"\\nüìÑ Terraform Files:\")\n",
    "    for tf_file in infra_analysis['terraform_files']:\n",
    "        print(f\"   üìù {tf_file['file']}\")\n",
    "        print(f\"      Lines: {tf_file['lines']}, Resources: {tf_file['resources']}, Variables: {tf_file['variables']}\")\n",
    "\n",
    "# Display Helm analysis\n",
    "if infra_analysis.get('helm_files'):\n",
    "    print(\"\\n‚öìÔ∏è Helm Chart Files:\")\n",
    "    file_types = {}\n",
    "    for helm_file in infra_analysis['helm_files']:\n",
    "        file_type = helm_file['type'] or 'no extension'\n",
    "        file_types[file_type] = file_types.get(file_type, 0) + 1\n",
    "    \n",
    "    for file_type, count in file_types.items():\n",
    "        print(f\"   {file_type}: {count} files\")\n",
    "    \n",
    "    # Show key templates\n",
    "    templates = [f for f in infra_analysis['helm_files'] if 'templates' in f['file']]\n",
    "    if templates:\n",
    "        print(\"\\nüìã Key Templates:\")\n",
    "        for template in templates[:5]:  # Show first 5\n",
    "            template_name = Path(template['file']).name\n",
    "            print(f\"   ‚Ä¢ {template_name}\")\n",
    "\n",
    "# Display Docker analysis\n",
    "if infra_analysis.get('docker_files'):\n",
    "    print(\"\\nüê≥ Docker Files:\")\n",
    "    for docker_file in infra_analysis['docker_files']:\n",
    "        print(f\"   üì¶ {docker_file['file']}: {docker_file['lines']} lines\")\n",
    "\n",
    "# Create infrastructure complexity visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Terraform complexity\n",
    "if infra_analysis.get('terraform_files'):\n",
    "    tf_data = pd.DataFrame(infra_analysis['terraform_files'])\n",
    "    if 'resources' in tf_data.columns:\n",
    "        axes[0].bar(range(len(tf_data)), tf_data['resources'], color='skyblue')\n",
    "        axes[0].set_xticks(range(len(tf_data)))\n",
    "        axes[0].set_xticklabels([Path(f['file']).name for f in infra_analysis['terraform_files']], rotation=45, ha='right')\n",
    "        axes[0].set_title('Terraform Resources by File')\n",
    "        axes[0].set_ylabel('Number of Resources')\n",
    "\n",
    "# Helm chart complexity\n",
    "if infra_analysis.get('helm_files'):\n",
    "    helm_data = pd.DataFrame(infra_analysis['helm_files'])\n",
    "    if 'lines' in helm_data.columns:\n",
    "        file_types = helm_data.groupby('type')['lines'].sum()\n",
    "        axes[1].pie(file_types.values, labels=file_types.index, autopct='%1.1f%%')\n",
    "        axes[1].set_title('Helm Chart Complexity by File Type')\n",
    "\n",
    "# Docker complexity\n",
    "if infra_analysis.get('docker_files'):\n",
    "    docker_data = pd.DataFrame(infra_analysis['docker_files'])\n",
    "    if 'lines' in docker_data.columns:\n",
    "        axes[2].bar(range(len(docker_data)), docker_data['lines'], color='lightgreen')\n",
    "        axes[2].set_xticks(range(len(docker_data)))\n",
    "        axes[2].set_xticklabels(docker_data['file'], rotation=45, ha='right')\n",
    "        axes[2].set_title('Docker File Complexity')\n",
    "        axes[2].set_ylabel('Lines of Code')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Deployment strategy analysis\n",
    "deployment_strategies = {\n",
    "    \"development\": {\n",
    "        \"replicas\": 1,\n",
    "        \"resources\": {\"cpu\": \"250m\", \"memory\": \"512Mi\"},\n",
    "        \"scaling\": \"manual\",\n",
    "        \"cost_estimate\": \"low\"\n",
    "    },\n",
    "    \"staging\": {\n",
    "        \"replicas\": 2,\n",
    "        \"resources\": {\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n",
    "        \"scaling\": \"manual\",\n",
    "        \"cost_estimate\": \"medium\"\n",
    "    },\n",
    "    \"production\": {\n",
    "        \"replicas\": 3,\n",
    "        \"resources\": {\"cpu\": \"1000m\", \"memory\": \"2Gi\"},\n",
    "        \"scaling\": \"hpa\",\n",
    "        \"cost_estimate\": \"high\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüè¢ Deployment Strategy by Environment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for env, config in deployment_strategies.items():\n",
    "    print(f\"\\nüåç {env.upper()} Environment:\")\n",
    "    print(f\"   üî¢ Replicas: {config['replicas']}\")\n",
    "    print(f\"   üíæ CPU: {config['resources']['cpu']}, Memory: {config['resources']['memory']}\")\n",
    "    print(f\"   üìà Scaling: {config['scaling']}\")\n",
    "    print(f\"   üí∞ Cost: {config['cost_estimate']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Infrastructure Analysis Complete!\")\n",
    "print(\"\\nüí° Infrastructure as Code Benefits:\")\n",
    "print(\"   ‚Ä¢ Reproducible deployments\")\n",
    "print(\"   ‚Ä¢ Environment consistency\")\n",
    "print(\"   ‚Ä¢ Version-controlled infrastructure\")\n",
    "print(\"   ‚Ä¢ Automated provisioning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ò∏Ô∏è Kubernetes Deployment\n",
    "\n",
    "Let's examine the Kubernetes deployment configuration and Helm charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Kubernetes deployment\n",
    "def analyze_kubernetes_deployment():\n",
    "    \"\"\"Analyze Kubernetes deployment configuration.\"\"\"\n",
    "    \n",
    "    k8s_analysis = {}\n",
    "    \n",
    "    # Load Helm values files\n",
    "    values_files = ['values.yaml', 'values-dev.yaml', 'values-prod.yaml']\n",
    "    values_configs = {}\n",
    "    \n",
    "    for values_file in values_files:\n",
    "        file_path = HELM_DIR / \"mlops-sentiment\" / values_file\n",
    "        if file_path.exists():\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    values_configs[values_file] = yaml.safe_load(f)\n",
    "            except Exception as e:\n",
    "                values_configs[values_file] = {\"error\": str(e)}\n",
    "    \n",
    "    k8s_analysis['values_configs'] = values_configs\n",
    "    \n",
    "    # Analyze deployment templates\n",
    "    template_files = list((HELM_DIR / \"mlops-sentiment\" / \"templates\").glob(\"*.yaml\"))\n",
    "    templates_analysis = []\n",
    "    \n",
    "    for template_file in template_files:\n",
    "        try:\n",
    "            with open(template_file, 'r') as f:\n",
    "                template_content = yaml.safe_load(f)\n",
    "                \n",
    "            if template_content:\n",
    "                # Analyze different resource types\n",
    "                if template_content.get('kind') == 'Deployment':\n",
    "                    spec = template_content.get('spec', {})\n",
    "                    template_info = {\n",
    "                        \"file\": template_file.name,\n",
    "                        \"kind\": \"Deployment\",\n",
    "                        \"replicas\": spec.get('replicas', 'templated'),\n",
    "                        \"strategy\": spec.get('strategy', {}).get('type', 'RollingUpdate'),\n",
    "                        \"containers\": len(spec.get('template', {}).get('spec', {}).get('containers', []))\n",
    "                    }\n",
    "                elif template_content.get('kind') == 'Service':\n",
    "                    spec = template_content.get('spec', {})\n",
    "                    template_info = {\n",
    "                        \"file\": template_file.name,\n",
    "                        \"kind\": \"Service\",\n",
    "                        \"type\": spec.get('type', 'ClusterIP'),\n",
    "                        \"ports\": len(spec.get('ports', []))\n",
    "                    }\n",
    "                elif template_content.get('kind') == 'Ingress':\n",
    "                    spec = template_content.get('spec', {})\n",
    "                    template_info = {\n",
    "                        \"file\": template_file.name,\n",
    "                        \"kind\": \"Ingress\",\n",
    "                        \"rules\": len(spec.get('rules', []))\n",
    "                    }\n",
    "                else:\n",
    "                    template_info = {\n",
    "                        \"file\": template_file.name,\n",
    "                        \"kind\": template_content.get('kind', 'Unknown')\n",
    "                    }\n",
    "                \n",
    "                templates_analysis.append(template_info)\n",
    "                \n",
    "        except Exception as e:\n",
    "            templates_analysis.append({\n",
    "                \"file\": template_file.name,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    k8s_analysis['templates'] = templates_analysis\n",
    "    \n",
    "    return k8s_analysis\n",
    "\n",
    "# Analyze Kubernetes deployment\n",
    "print(\"‚ò∏Ô∏è Kubernetes Deployment Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "k8s_analysis = analyze_kubernetes_deployment()\n",
    "\n",
    "# Display values configurations\n",
    "if k8s_analysis.get('values_configs'):\n",
    "    print(\"\\n‚öôÔ∏è Helm Values Configurations:\")\n",
    "    for config_file, config_data in k8s_analysis['values_configs'].items():\n",
    "        print(f\"\\nüìÑ {config_file}:\")\n",
    "        if isinstance(config_data, dict) and 'error' not in config_data:\n",
    "            # Show key configuration items\n",
    "            if 'replicaCount' in config_data:\n",
    "                print(f\"   üî¢ Replica Count: {config_data['replicaCount']}\")\n",
    "            if 'image' in config_data:\n",
    "                print(f\"   üê≥ Image: {config_data['image'].get('repository', 'N/A')}:{config_data['image'].get('tag', 'N/A')}\")\n",
    "            if 'resources' in config_data:\n",
    "                resources = config_data['resources']\n",
    "                if 'limits' in resources:\n",
    "                    limits = resources['limits']\n",
    "                    print(f\"   üíæ Resource Limits: CPU {limits.get('cpu', 'N/A')}, Memory {limits.get('memory', 'N/A')}\")\n",
    "            if 'autoscaling' in config_data and config_data['autoscaling'].get('enabled'):\n",
    "                hpa = config_data['autoscaling']\n",
    "                print(f\"   üìà HPA: Min {hpa.get('minReplicas', 'N/A')}, Max {hpa.get('maxReplicas', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Error loading config: {config_data.get('error', 'Unknown error')}\")\n",
    "\n",
    "# Display templates analysis\n",
    "if k8s_analysis.get('templates'):\n",
    "    print(\"\\nüìã Kubernetes Templates:\")\n",
    "    templates_df = pd.DataFrame(k8s_analysis['templates'])\n",
    "    \n",
    "    # Group by kind\n",
    "    if 'kind' in templates_df.columns:\n",
    "        kind_counts = templates_df['kind'].value_counts()\n",
    "        print(\"Resource Types:\")\n",
    "        for kind, count in kind_counts.items():\n",
    "            print(f\"   ‚Ä¢ {kind}: {count} template(s)\")\n",
    "    \n",
    "    # Show detailed template info\n",
    "    print(\"\\nüìÑ Template Details:\")\n",
    "    for template in k8s_analysis['templates']:\n",
    "        print(f\"   üìù {template['file']} ({template.get('kind', 'Unknown')})\")\n",
    "        \n",
    "        if template.get('kind') == 'Deployment':\n",
    "            print(f\"      Replicas: {template.get('replicas', 'N/A')}\")\n",
    "            print(f\"      Strategy: {template.get('strategy', 'N/A')}\")\n",
    "            print(f\"      Containers: {template.get('containers', 'N/A')}\")\n",
    "        elif template.get('kind') == 'Service':\n",
    "            print(f\"      Type: {template.get('type', 'N/A')}\")\n",
    "            print(f\"      Ports: {template.get('ports', 'N/A')}\")\n",
    "        elif template.get('kind') == 'Ingress':\n",
    "            print(f\"      Rules: {template.get('rules', 'N/A')}\")\n",
    "\n",
    "# Create deployment architecture visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Kubernetes Deployment Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Environment configurations comparison\n",
    "if k8s_analysis.get('values_configs'):\n",
    "    env_configs = {}\n",
    "    for config_file, config_data in k8s_analysis['values_configs'].items():\n",
    "        if isinstance(config_data, dict) and 'error' not in config_data:\n",
    "            env_name = config_file.replace('values-', '').replace('.yaml', '')\n",
    "            if env_name == 'values':\n",
    "                env_name = 'default'\n",
    "            env_configs[env_name] = {\n",
    "                'replicas': config_data.get('replicaCount', 1),\n",
    "                'cpu_limit': config_data.get('resources', {}).get('limits', {}).get('cpu', '1000m'),\n",
    "                'memory_limit': config_data.get('resources', {}).get('limits', {}).get('memory', '1Gi')\n",
    "            }\n",
    "    \n",
    "    if env_configs:\n",
    "        env_names = list(env_configs.keys())\n",
    "        replicas = [env_configs[env]['replicas'] for env in env_names]\n",
    "        \n",
    "        axes[0, 0].bar(env_names, replicas, color=['lightblue', 'lightgreen', 'red'])\n",
    "        axes[0, 0].set_title('Replica Count by Environment')\n",
    "        axes[0, 0].set_ylabel('Number of Replicas')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Resource allocation\n",
    "resource_data = {\n",
    "    'CPU Limits': ['250m', '500m', '1000m', '2000m'],\n",
    "    'Memory Limits': ['512Mi', '1Gi', '2Gi', '4Gi'],\n",
    "    'Environments': ['dev', 'staging', 'prod-small', 'prod-large']\n",
    "}\n",
    "\n",
    "# Convert CPU to cores for visualization\n",
    "cpu_cores = [0.25, 0.5, 1.0, 2.0]\n",
    "memory_gb = [0.5, 1.0, 2.0, 4.0]\n",
    "\n",
    "axes[0, 1].plot(resource_data['Environments'], cpu_cores, marker='o', linewidth=2, label='CPU')\n",
    "axes[0, 1].plot(resource_data['Environments'], memory_gb, marker='s', linewidth=2, label='Memory')\n",
    "axes[0, 1].set_title('Resource Allocation by Environment')\n",
    "axes[0, 1].set_ylabel('Resources')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scaling comparison\n",
    "scaling_data = {\n",
    "    'Environment': ['Development', 'Staging', 'Production'],\n",
    "    'Min Replicas': [1, 2, 3],\n",
    "    'Max Replicas': [1, 3, 10],\n",
    "    'Target CPU %': [70, 70, 70]\n",
    "}\n",
    "\n",
    "x = np.arange(len(scaling_data['Environment']))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x - width/2, scaling_data['Min Replicas'], width, label='Min Replicas', alpha=0.8)\n",
    "axes[1, 0].bar(x + width/2, scaling_data['Max Replicas'], width, label='Max Replicas', alpha=0.8)\n",
    "axes[1, 0].set_title('Horizontal Pod Autoscaling Configuration')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(scaling_data['Environment'])\n",
    "axes[1, 0].set_ylabel('Number of Replicas')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Service mesh features\n",
    "service_features = {\n",
    "    'Load Balancing': 95,\n",
    "    'Circuit Breaking': 90,\n",
    "    'Traffic Splitting': 85,\n",
    "    'Observability': 95,\n",
    "    'Security': 90,\n",
    "    'Multi-cluster': 80\n",
    "}\n",
    "\n",
    "features = list(service_features.keys())\n",
    "scores = list(service_features.values())\n",
    "\n",
    "bars = axes[1, 1].barh(range(len(features)), scores, color='lightcoral', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(features)))\n",
    "axes[1, 1].set_yticklabels(features)\n",
    "axes[1, 1].set_xlabel('Maturity Score (%)')\n",
    "axes[1, 1].set_title('Service Mesh Capabilities')\n",
    "axes[1, 1].set_xlim(0, 100)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, scores):\n",
    "    width = bar.get_width()\n",
    "    axes[1, 1].text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{score}%', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Kubernetes Deployment Analysis Complete!\")\n",
    "print(\"\\nüí° Kubernetes Benefits:\")\n",
    "print(\"   ‚Ä¢ Automated scaling and self-healing\")\n",
    "print(\"   ‚Ä¢ Declarative configuration\")\n",
    "print(\"   ‚Ä¢ Service discovery and load balancing\")\n",
    "print(\"   ‚Ä¢ Rolling updates with zero downtime\")\n",
    "print(\"   ‚Ä¢ Resource optimization and cost efficiency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Scaling Strategies\n",
    "\n",
    "Let's analyze scaling strategies and performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling analysis and recommendations\n",
    "def analyze_scaling_strategies():\n",
    "    \"\"\"Analyze different scaling strategies and provide recommendations.\"\"\"\n",
    "    \n",
    "    scaling_analysis = {\n",
    "        \"horizontal_pod_autoscaling\": {\n",
    "            \"description\": \"Automatically scale pods based on CPU/memory usage\",\n",
    "            \"metrics\": [\"cpu\", \"memory\"],\n",
    "            \"advantages\": [\n",
    "                \"Automatic scaling based on resource usage\",\n",
    "                \"Handles traffic spikes automatically\",\n",
    "                \"Cost-effective - scales down during low traffic\"\n",
    "            ],\n",
    "            \"disadvantages\": [\n",
    "                \"Cold start latency for new pods\",\n",
    "                \"May not handle all traffic patterns optimally\",\n",
    "                \"Requires careful resource limit tuning\"\n",
    "            ],\n",
    "            \"use_case\": \"Variable traffic patterns\",\n",
    "            \"configuration\": {\n",
    "                \"min_replicas\": 3,\n",
    "                \"max_replicas\": 10,\n",
    "                \"target_cpu_utilization\": 70,\n",
    "                \"target_memory_utilization\": 80\n",
    "            }\n",
    "        },\n",
    "        \"cluster_autoscaling\": {\n",
    "            \"description\": \"Automatically scale the number of nodes in the cluster\",\n",
    "            \"metrics\": [\"node_cpu\", \"node_memory\", \"pending_pods\"],\n",
    "            \"advantages\": [\n",
    "                \"Handles pod scaling limits\",\n",
    "                \"Optimizes cluster resource utilization\",\n",
    "                \"Reduces infrastructure costs during low usage\"\n",
    "            ],\n",
    "            \"disadvantages\": [\n",
    "                \"Slower scaling compared to pod autoscaling\",\n",
    "                \"More complex configuration\",\n",
    "                \"May cause service disruptions during node changes\"\n",
    "            ],\n",
    "            \"use_case\": \"Highly variable or unpredictable workloads\",\n",
    "            \"configuration\": {\n",
    "                \"min_nodes\": 3,\n",
    "                \"max_nodes\": 20,\n",
    "                \"scale_down_delay\": \"10m\",\n",
    "                \"scale_up_delay\": \"1m\"\n",
    "            }\n",
    "        },\n",
    "        \"predictive_scaling\": {\n",
    "            \"description\": \"Scale based on predicted traffic patterns\",\n",
    "            \"metrics\": [\"historical_data\", \"time_patterns\", \"external_signals\"],\n",
    "            \"advantages\": [\n",
    "                \"Proactive scaling before traffic spikes\",\n",
    "                \"Better user experience with reduced latency\",\n",
    "                \"Can incorporate business metrics\"\n",
    "            ],\n",
    "            \"disadvantages\": [\n",
    "                \"Requires historical data and ML models\",\n",
    "                \"More complex to implement\",\n",
    "                \"May over-provision resources\"\n",
    "            ],\n",
    "            \"use_case\": \"Predictable traffic patterns with known seasonality\",\n",
    "            \"configuration\": {\n",
    "                \"prediction_window\": \"1h\",\n",
    "                \"scaling_buffer\": 20,\n",
    "                \"min_prediction_confidence\": 0.8\n",
    "            }\n",
    "        },\n",
    "        \"event_driven_scaling\": {\n",
    "            \"description\": \"Scale based on external events or custom metrics\",\n",
    "            \"metrics\": [\"queue_depth\", \"api_calls\", \"business_metrics\"],\n",
    "            \"advantages\": [\n",
    "                \"Responds to business logic rather than just resources\",\n",
    "                \"Can handle complex scaling scenarios\",\n",
    "                \"Integrates with external systems\"\n",
    "            ],\n",
    "            \"disadvantages\": [\n",
    "                \"Requires custom development\",\n",
    "                \"May be overkill for simple applications\",\n",
    "                \"Debugging can be complex\"\n",
    "            ],\n",
    "            \"use_case\": \"Complex applications with specific scaling requirements\",\n",
    "            \"configuration\": {\n",
    "                \"custom_metrics\": [\"sentiment_requests_per_minute\"],\n",
    "                \"scaling_policies\": [\"step_scaling\", \"target_tracking\"],\n",
    "                \"cooldown_period\": \"5m\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return scaling_analysis\n",
    "\n",
    "# Performance optimization recommendations\n",
    "def get_performance_optimization():\n",
    "    \"\"\"Get performance optimization recommendations.\"\"\"\n",
    "    \n",
    "    optimizations = {\n",
    "        \"model_optimization\": {\n",
    "            \"techniques\": [\n",
    "                \"Use ONNX Runtime for faster inference\",\n",
    "                \"Implement model quantization\",\n",
    "                \"Use model distillation for smaller models\",\n",
    "                \"Implement model versioning and A/B testing\"\n",
    "            ],\n",
    "            \"expected_improvement\": \"2-3x faster inference\",\n",
    "            \"complexity\": \"Medium\"\n",
    "        },\n",
    "        \"caching_strategies\": {\n",
    "            \"techniques\": [\n",
    "                \"Implement LRU cache for predictions\",\n",
    "                \"Use Redis for distributed caching\",\n",
    "                \"Cache model artifacts in memory\",\n",
    "                \"Implement response compression\"\n",
    "            ],\n",
    "            \"expected_improvement\": \"10-50% faster response times\",\n",
    "            \"complexity\": \"Low\"\n",
    "        },\n",
    "        \"infrastructure_optimization\": {\n",
    "            \"techniques\": [\n",
    "                \"Use GPU instances for inference\",\n",
    "                \"Implement horizontal pod autoscaling\",\n",
    "                \"Use spot instances for cost optimization\",\n",
    "                \"Implement multi-region deployment\"\n",
    "            ],\n",
    "            \"expected_improvement\": \"3-5x throughput improvement\",\n",
    "            \"complexity\": \"High\"\n",
    "        },\n",
    "        \"api_optimization\": {\n",
    "            \"techniques\": [\n",
    "                \"Implement async request processing\",\n",
    "                \"Use connection pooling\",\n",
    "                \"Implement rate limiting\",\n",
    "                \"Optimize JSON serialization\"\n",
    "            ],\n",
    "            \"expected_improvement\": \"20-40% reduced latency\",\n",
    "            \"complexity\": \"Medium\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return optimizations\n",
    "\n",
    "# Display scaling analysis\n",
    "print(\"üìä Scaling Strategies Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scaling_strategies = analyze_scaling_strategies()\n",
    "\n",
    "for strategy_name, strategy_info in scaling_strategies.items():\n",
    "    print(f\"\\nüîÑ {strategy_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   üìù {strategy_info['description']}\")\n",
    "    print(f\"   üéØ Use Case: {strategy_info['use_case']}\")\n",
    "    \n",
    "    print(f\"   ‚úÖ Advantages:\")\n",
    "    for advantage in strategy_info['advantages'][:2]:  # Show first 2\n",
    "        print(f\"      ‚Ä¢ {advantage}\")\n",
    "    \n",
    "    print(f\"   ‚öôÔ∏è Key Configuration:\")\n",
    "    for key, value in list(strategy_info['configuration'].items())[:3]:  # Show first 3\n",
    "        print(f\"      ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# Performance optimization\n",
    "print(\"\\n‚ö° Performance Optimization Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "optimizations = get_performance_optimization()\n",
    "\n",
    "for opt_name, opt_info in optimizations.items():\n",
    "    print(f\"\\nüîß {opt_name.replace('_', ' ').title()}:\")\n",
    "    print(f\"   üìà Expected Improvement: {opt_info['expected_improvement']}\")\n",
    "    print(f\"   üéöÔ∏è Complexity: {opt_info['complexity']}\")\n",
    "    \n",
    "    print(f\"   üõ†Ô∏è Key Techniques:\")\n",
    "    for technique in opt_info['techniques'][:3]:  # Show first 3\n",
    "        print(f\"      ‚Ä¢ {technique}\")\n",
    "\n",
    "# Create scaling strategy comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Scaling Strategy Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Scaling speed comparison\n",
    "strategies = list(scaling_strategies.keys())\n",
    "scaling_speed = [3, 8, 2, 1]  # Relative scaling speed (1-10 scale)\n",
    "cost_efficiency = [8, 6, 7, 5]  # Cost efficiency (1-10 scale)\n",
    "complexity = [3, 7, 9, 8]  # Implementation complexity (1-10 scale)\n",
    "reliability = [9, 8, 6, 7]  # Reliability (1-10 scale)\n",
    "\n",
    "axes[0, 0].bar(strategies, scaling_speed, color='skyblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Scaling Speed')\n",
    "axes[0, 0].set_ylabel('Speed Score (1-10)')\n",
    "axes[0, 0].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(strategies, cost_efficiency, color='lightgreen', alpha=0.7)\n",
    "axes[0, 1].set_title('Cost Efficiency')\n",
    "axes[0, 1].set_ylabel('Efficiency Score (1-10)')\n",
    "axes[0, 1].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].bar(strategies, complexity, color='orange', alpha=0.7)\n",
    "axes[1, 0].set_title('Implementation Complexity')\n",
    "axes[1, 0].set_ylabel('Complexity Score (1-10)')\n",
    "axes[1, 0].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(strategies, reliability, color='lightcoral', alpha=0.7)\n",
    "axes[1, 1].set_title('Reliability')\n",
    "axes[1, 1].set_ylabel('Reliability Score (1-10)')\n",
    "axes[1, 1].set_xticklabels(strategies, rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scaling recommendations\n",
    "scaling_recommendations = {\n",
    "    \"small_startup\": {\n",
    "        \"strategy\": \"horizontal_pod_autoscaling\",\n",
    "        \"justification\": \"Simple, cost-effective, handles basic scaling needs\",\n",
    "        \"expected_cost_savings\": \"20-30%\"\n",
    "    },\n",
    "    \"growing_business\": {\n",
    "        \"strategy\": \"cluster_autoscaling + hpa\",\n",
    "        \"justification\": \"Handles variable traffic, optimizes infrastructure costs\",\n",
    "        \"expected_cost_savings\": \"30-40%\"\n",
    "    },\n",
    "    \"enterprise_large_scale\": {\n",
    "        \"strategy\": \"predictive_scaling + cluster_autoscaling\",\n",
    "        \"justification\": \"Proactive scaling for predictable patterns, maximum efficiency\",\n",
    "        \"expected_cost_savings\": \"40-50%\"\n",
    "    },\n",
    "    \"specialized_use_case\": {\n",
    "        \"strategy\": \"event_driven_scaling\",\n",
    "        \"justification\": \"Custom business logic integration, maximum flexibility\",\n",
    "        \"expected_cost_savings\": \"35-45%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüéØ Scaling Strategy Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for use_case, recommendation in scaling_recommendations.items():\n",
    "    print(f\"\\nüè¢ {use_case.replace('_', ' ').title()}:\")\n",
    "    print(f\"   üîÑ Recommended Strategy: {recommendation['strategy']}\")\n",
    "    print(f\"   üí° Justification: {recommendation['justification']}\")\n",
    "    print(f\"   üí∞ Expected Cost Savings: {recommendation['expected_cost_savings']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Scaling Analysis Complete!\")\n",
    "print(\"\\nüí° Scaling Best Practices:\")\n",
    "print(\"   ‚Ä¢ Start with horizontal pod autoscaling\")\n",
    "print(\"   ‚Ä¢ Monitor resource utilization continuously\")\n",
    "print(\"   ‚Ä¢ Implement gradual rollout strategies\")\n",
    "print(\"   ‚Ä¢ Use multiple metrics for scaling decisions\")\n",
    "print(\"   ‚Ä¢ Test scaling behavior under load\")\n",
    "print(\"   ‚Ä¢ Monitor cost implications of scaling decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîí Production Readiness Checklist\n",
    "\n",
    "Let's create a comprehensive production readiness checklist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness checklist\n",
    "def create_production_readiness_checklist():\n",
    "    \"\"\"Create a comprehensive production readiness checklist.\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"infrastructure\": {\n",
    "            \"category\": \"Infrastructure\",\n",
    "            \"items\": {\n",
    "                \"kubernetes_cluster\": {\n",
    "                    \"description\": \"Production-ready Kubernetes cluster with proper networking\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl cluster-info && kubectl get nodes\"\n",
    "                },\n",
    "                \"monitoring_stack\": {\n",
    "                    \"description\": \"Prometheus + Grafana monitoring stack deployed\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get pods -n monitoring\"\n",
    "                },\n",
    "                \"load_balancer\": {\n",
    "                    \"description\": \"Load balancer configured for external access\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get svc -n ingress\"\n",
    "                },\n",
    "                \"backup_system\": {\n",
    "                    \"description\": \"Automated backup system for persistent data\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Recommended\",\n",
    "                    \"verification\": \"Check backup job schedules and test restores\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"security\": {\n",
    "            \"category\": \"Security\",\n",
    "            \"items\": {\n",
    "                \"secret_management\": {\n",
    "                    \"description\": \"Secrets stored securely (not in code/config)\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get secrets && check for hardcoded secrets\"\n",
    "                },\n",
    "                \"network_policies\": {\n",
    "                    \"description\": \"Kubernetes network policies restrict pod communication\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get networkpolicies\"\n",
    "                },\n",
    "                \"rbac_configured\": {\n",
    "                    \"description\": \"Role-based access control properly configured\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get clusterroles,clusterrolebindings\"\n",
    "                },\n",
    "                \"security_scanning\": {\n",
    "                    \"description\": \"Regular security scanning of containers and dependencies\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"Check CI/CD security scan results\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"reliability\": {\n",
    "            \"category\": \"Reliability\",\n",
    "            \"items\": {\n",
    "                \"health_checks\": {\n",
    "                    \"description\": \"Liveness and readiness probes configured\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl describe deployment | grep -A5 Probes\"\n",
    "                },\n",
    "                \"resource_limits\": {\n",
    "                    \"description\": \"CPU and memory limits set for all containers\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get pods -o=jsonpath='{.items[*].spec.containers[*].resources}'\"\n",
    "                },\n",
    "                \"autoscaling\": {\n",
    "                    \"description\": \"Horizontal Pod Autoscaler configured\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get hpa\"\n",
    "                },\n",
    "                \"disaster_recovery\": {\n",
    "                    \"description\": \"Disaster recovery plan documented and tested\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Recommended\",\n",
    "                    \"verification\": \"Check DR documentation and test results\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"category\": \"Performance\",\n",
    "            \"items\": {\n",
    "                \"load_testing\": {\n",
    "                    \"description\": \"Load testing completed with acceptable performance\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"Check load test results and performance metrics\"\n",
    "                },\n",
    "                \"caching_strategy\": {\n",
    "                    \"description\": \"Caching strategy implemented and tested\",\n",
    "                    \"priority\": \"Medium\",\n",
    "                    \"status\": \"Recommended\",\n",
    "                    \"verification\": \"Check cache hit rates and performance improvement\"\n",
    "                },\n",
    "                \"optimization_applied\": {\n",
    "                    \"description\": \"Performance optimizations applied (ONNX, etc.)\",\n",
    "                    \"priority\": \"Medium\",\n",
    "                    \"status\": \"Recommended\",\n",
    "                    \"verification\": \"Compare performance before/after optimization\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"observability\": {\n",
    "            \"category\": \"Observability\",\n",
    "            \"items\": {\n",
    "                \"structured_logging\": {\n",
    "                    \"description\": \"Structured JSON logging with correlation IDs\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"Check log format and correlation ID usage\"\n",
    "                },\n",
    "                \"alerting_rules\": {\n",
    "                    \"description\": \"Comprehensive alerting rules configured\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"kubectl get prometheusrules && check alert definitions\"\n",
    "                },\n",
    "                \"dashboards_created\": {\n",
    "                    \"description\": \"Grafana dashboards created and populated\",\n",
    "                    \"priority\": \"Medium\",\n",
    "                    \"status\": \"Recommended\",\n",
    "                    \"verification\": \"Access Grafana dashboards and verify metrics display\"\n",
    "                },\n",
    "                \"tracing_enabled\": {\n",
    "                    \"description\": \"Distributed tracing enabled for request tracking\",\n",
    "                    \"priority\": \"Medium\",\n",
    "                    \"status\": \"Optional\",\n",
    "                    \"verification\": \"Check OpenTelemetry configuration\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"operations\": {\n",
    "            \"category\": \"Operations\",\n",
    "            \"items\": {\n",
    "                \"ci_cd_pipeline\": {\n",
    "                    \"description\": \"Complete CI/CD pipeline with automated deployment\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"Check GitHub Actions workflow and deployment history\"\n",
    "                },\n",
    "                \"rollback_plan\": {\n",
    "                    \"description\": \"Rollback plan documented and tested\",\n",
    "                    \"priority\": \"Critical\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"Check rollback documentation and test results\"\n",
    "                },\n",
    "                \"documentation\": {\n",
    "                    \"description\": \"Complete operational documentation available\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Required\",\n",
    "                    \"verification\": \"Check docs/ directory and runbooks\"\n",
    "                },\n",
    "                \"on_call_rotation\": {\n",
    "                    \"description\": \"On-call rotation and incident response documented\",\n",
    "                    \"priority\": \"High\",\n",
    "                    \"status\": \"Recommended\",\n",
    "                    \"verification\": \"Check incident response documentation\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return checklist\n",
    "\n",
    "# Display production readiness checklist\n",
    "print(\"‚úÖ Production Readiness Checklist:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checklist = create_production_readiness_checklist()\n",
    "\n",
    "total_items = 0\n",
    "critical_items = 0\n",
    "required_items = 0\n",
    "\n",
    "for category_name, category_info in checklist.items():\n",
    "    print(f\"\\nüìã {category_info['category']}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for item_name, item_info in category_info['items'].items():\n",
    "        total_items += 1\n",
    "        \n",
    "        if item_info['priority'] == 'Critical':\n",
    "            critical_items += 1\n",
    "        \n",
    "        if item_info['status'] == 'Required':\n",
    "            required_items += 1\n",
    "        \n",
    "        priority_icon = \"üî¥\" if item_info['priority'] == 'Critical' else \"üü†\" if item_info['priority'] == 'High' else \"üü¢\"\n",
    "        status_icon = \"‚úÖ\" if item_info['status'] == 'Required' else \"‚ö†Ô∏è\" if item_info['status'] == 'Recommended' else \"‚ÑπÔ∏è\"\n",
    "        \n",
    "        print(f\"   {priority_icon}{status_icon} {item_name.replace('_', ' ').title()}\")\n",
    "        print(f\"      üìù {item_info['description']}\")\n",
    "        print(f\"      üîç Verification: {item_info['verification']}\")\n",
    "\n",
    "print(f\"\\nüìä Checklist Summary:\")\n",
    "print(f\"   üìã Total Items: {total_items}\")\n",
    "print(f\"   üî¥ Critical Items: {critical_items}\")\n",
    "print(f\"   ‚úÖ Required Items: {required_items}\")\n",
    "print(f\"   ‚ö†Ô∏è Recommended Items: {total_items - required_items}\")\n",
    "\n",
    "# Create checklist visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Count items by category and priority\n",
    "categories = []\n",
    "critical_counts = []\n",
    "high_counts = []\n",
    "medium_counts = []\n",
    "\n",
    "for category_name, category_info in checklist.items():\n",
    "    categories.append(category_info['category'])\n",
    "    \n",
    "    critical = sum(1 for item in category_info['items'].values() if item['priority'] == 'Critical')\n",
    "    high = sum(1 for item in category_info['items'].values() if item['priority'] == 'High')\n",
    "    medium = sum(1 for item in category_info['items'].values() if item['priority'] == 'Medium')\n",
    "    \n",
    "    critical_counts.append(critical)\n",
    "    high_counts.append(high)\n",
    "    medium_counts.append(medium)\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, critical_counts, width, label='Critical', color='red', alpha=0.7)\n",
    "ax.bar(x, high_counts, width, label='High', color='orange', alpha=0.7)\n",
    "ax.bar(x + width, medium_counts, width, label='Medium', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_ylabel('Number of Items')\n",
    "ax.set_title('Production Readiness Checklist by Category and Priority')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(categories, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (crit, high, med) in enumerate(zip(critical_counts, high_counts, medium_counts)):\n",
    "    if crit > 0:\n",
    "        ax.text(i - width, crit + 0.1, str(crit), ha='center', va='bottom')\n",
    "    if high > 0:\n",
    "        ax.text(i, high + 0.1, str(high), ha='center', va='bottom')\n",
    "    if med > 0:\n",
    "        ax.text(i + width, med + 0.1, str(med), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüöÄ Deployment Checklist Complete!\")\n",
    "print(\"\\nüí° Production Deployment Best Practices:\")\n",
    "print(\"   ‚Ä¢ Use this checklist before going live\")\n",
    "print(\"   ‚Ä¢ Automate verification where possible\")\n",
    "print(\"   ‚Ä¢ Have rollback plans for all changes\")\n",
    "print(\"   ‚Ä¢ Monitor closely after deployment\")\n",
    "print(\"   ‚Ä¢ Document all production procedures\")\n",
    "print(\"   ‚Ä¢ Regular security and performance audits\")\n",
    "print(\"\\nüéØ Remember: Production readiness is about confidence, not perfection!\")\n",
    "print(\"\\nüèÅ KubeSentiment Deployment Guide Complete!\")\n",
    "print(\"\\nüìö Additional Resources:\")\n",
    "print(\"   ‚Ä¢ Kubernetes documentation: https://kubernetes.io/docs/\")\n",
    "print(\"   ‚Ä¢ Helm charts: https://helm.sh/docs/\")\n",
    "print(\"   ‚Ä¢ Terraform: https://www.terraform.io/docs/\")\n",
    "print(\"   ‚Ä¢ Production readiness: https://landing.google.com/sre/books/\")\n",
    "print(\"\\nüéâ Ready to deploy KubeSentiment to production!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
