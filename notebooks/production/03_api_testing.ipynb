{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ API Testing: Comprehensive KubeSentiment API Testing\n",
    "\n",
    "This notebook provides comprehensive testing of the KubeSentiment API endpoints, including functional testing, load testing, error handling, and integration testing.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand all API endpoints and their functionality\n",
    "2. Perform comprehensive functional testing\n",
    "3. Conduct load and performance testing\n",
    "4. Test error handling and edge cases\n",
    "5. Validate API responses and schemas\n",
    "6. Test caching and performance optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Dependencies\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this notebook\n",
    "# Note: This cell might take a few minutes to run\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Version Check\n",
    "Let's check the versions of the installed libraries to ensure our environment is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List installed packages to ensure reproducibility\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã API Endpoints Overview\n",
    "\n",
    "### Core Endpoints\n",
    "\n",
    "| Endpoint | Method | Purpose | Expected Response Time |\n",
    "|----------|--------|---------|----------------------|\n",
    "| `/health` | GET | Service health check | <10ms |\n",
    "| `/predict` | POST | Sentiment analysis | <100ms |\n",
    "| `/model-info` | GET | Model information | <20ms |\n",
    "| `/metrics` | GET | Prometheus metrics | <50ms |\n",
    "| `/metrics-json` | GET | JSON metrics | <50ms |\n",
    "\n",
    "### Request/Response Formats\n",
    "\n",
    "**Health Check:**\n",
    "```json\n",
    "{\n",
    "  \"status\": \"healthy\",\n",
    "  \"model_status\": \"available\",\n",
    "  \"version\": \"1.0.0\",\n",
    "  \"timestamp\": 1703123456.789\n",
    "}\n",
    "```\n",
    "\n",
    "**Prediction:**\n",
    "```json\n",
    "{\n",
    "  \"text\": \"I love this product!\",\n",
    "  \"label\": \"POSITIVE\",\n",
    "  \"score\": 0.9998,\n",
    "  \"inference_time_ms\": 45.2,\n",
    "  \"model_name\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "  \"text_length\": 20\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import requests\n",
    "import httpx\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "TIMEOUT = 30\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üéØ Testing API at: {API_BASE_URL}\")\n",
    "print(f\"‚è±Ô∏è Timeout: {TIMEOUT} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Initial Health Check\n",
    "\n",
    "Let's start by checking if the API is available and healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Client class for organized testing\n",
    "class KubeSentimentAPIClient:\n",
    "    \"\"\"Client for testing KubeSentiment API endpoints.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str, timeout: int = 30):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.timeout = timeout\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    def health_check(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check service health.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.session.get(f\"{self.base_url}/health\", timeout=self.timeout)\n",
    "            response_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code == 200,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time_ms\": round(response_time, 2),\n",
    "                \"data\": response.json() if response.status_code == 200 else None,\n",
    "                \"error\": None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time_ms\": None,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def predict(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Make sentiment prediction.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.session.post(\n",
    "                f\"{self.base_url}/predict\",\n",
    "                json={\"text\": text},\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            response_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code == 200,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time_ms\": round(response_time, 2),\n",
    "                \"data\": response.json() if response.status_code == 200 else None,\n",
    "                \"error\": response.text if response.status_code != 200 else None,\n",
    "                \"text\": text\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time_ms\": None,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e),\n",
    "                \"text\": text\n",
    "            }\n",
    "    \n",
    "    def get_model_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get model information.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.session.get(f\"{self.base_url}/model-info\", timeout=self.timeout)\n",
    "            response_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code == 200,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time_ms\": round(response_time, 2),\n",
    "                \"data\": response.json() if response.status_code == 200 else None,\n",
    "                \"error\": None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time_ms\": None,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics in JSON format.\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.session.get(f\"{self.base_url}/metrics-json\", timeout=self.timeout)\n",
    "            response_time = (time.time() - start_time) * 1000\n",
    "            \n",
    "            return {\n",
    "                \"success\": response.status_code == 200,\n",
    "                \"status_code\": response.status_code,\n",
    "                \"response_time_ms\": round(response_time, 2),\n",
    "                \"data\": response.json() if response.status_code == 200 else None,\n",
    "                \"error\": None\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"status_code\": None,\n",
    "                \"response_time_ms\": None,\n",
    "                \"data\": None,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "# Initialize API client\n",
    "client = KubeSentimentAPIClient(API_BASE_URL, TIMEOUT)\n",
    "\n",
    "# Test basic connectivity\n",
    "print(\"üîç Initial API Health Check:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "health_result = client.health_check()\n",
    "\n",
    "if health_result[\"success\"]:\n",
    "    print(\"‚úÖ API is healthy!\")\n",
    "    print(f\"üìä Status: {health_result['data']['status']}\")\n",
    "    print(f\"ü§ñ Model Status: {health_result['data']['model_status']}\")\n",
    "    print(f\"üè∑Ô∏è Version: {health_result['data']['version']}\")\n",
    "    print(f\"‚ö° Response Time: {health_result['response_time_ms']}ms\")\n",
    "    \n",
    "    api_available = True\n",
    "else:\n",
    "    print(\"‚ùå API is not available\")\n",
    "    print(f\"üîç Error: {health_result['error']}\")\n",
    "    print(\"\\nüí° Make sure the service is running:\")\n",
    "    print(\"   docker run -d -p 8000:8000 sentiment-service:latest\")\n",
    "    print(\"   # or\")\n",
    "    print(\"   python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\")\n",
    "    \n",
    "    api_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Functional Testing\n",
    "\n",
    "Let's perform comprehensive functional testing of all API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional tests for all endpoints\n",
    "def run_functional_tests(client: KubeSentimentAPIClient) -> Dict[str, Any]:\n",
    "    \"\"\"Run comprehensive functional tests.\"\"\"\n",
    "    results = {\n",
    "        \"health_check\": client.health_check(),\n",
    "        \"model_info\": client.get_model_info(),\n",
    "        \"metrics\": client.get_metrics(),\n",
    "        \"predictions\": []\n",
    "    }\n",
    "    \n",
    "    # Test predictions with various inputs\n",
    "    test_texts = [\n",
    "        \"I love this amazing product!\",\n",
    "        \"This is absolutely terrible.\",\n",
    "        \"It's okay, nothing special.\",\n",
    "        \"Outstanding customer service!\",\n",
    "        \"Complete disaster. Never again.\"\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        results[\"predictions\"].append(client.predict(text))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run functional tests\n",
    "if api_available:\n",
    "    print(\"üß™ Running Functional Tests:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    functional_results = run_functional_tests(client)\n",
    "    \n",
    "    # Analyze results\n",
    "    tests_passed = 0\n",
    "    total_tests = 0\n",
    "    \n",
    "    # Health check\n",
    "    total_tests += 1\n",
    "    if functional_results[\"health_check\"][\"success\"]:\n",
    "        tests_passed += 1\n",
    "        print(\"‚úÖ Health Check: PASSED\")\n",
    "    else:\n",
    "        print(\"‚ùå Health Check: FAILED\")\n",
    "    \n",
    "    # Model info\n",
    "    total_tests += 1\n",
    "    if functional_results[\"model_info\"][\"success\"]:\n",
    "        tests_passed += 1\n",
    "        print(\"‚úÖ Model Info: PASSED\")\n",
    "    else:\n",
    "        print(\"‚ùå Model Info: FAILED\")\n",
    "    \n",
    "    # Metrics\n",
    "    total_tests += 1\n",
    "    if functional_results[\"metrics\"][\"success\"]:\n",
    "        tests_passed += 1\n",
    "        print(\"‚úÖ Metrics: PASSED\")\n",
    "    else:\n",
    "        print(\"‚ùå Metrics: FAILED\")\n",
    "    \n",
    "    # Predictions\n",
    "    for i, pred in enumerate(functional_results[\"predictions\"]):\n",
    "        total_tests += 1\n",
    "        if pred[\"success\"]:\n",
    "            tests_passed += 1\n",
    "            print(f\"‚úÖ Prediction {i+1}: PASSED\")\n",
    "        else:\n",
    "            print(f\"‚ùå Prediction {i+1}: FAILED\")\n",
    "    \n",
    "    print(f\"\\nüìä Functional Test Results: {tests_passed}/{total_tests} PASSED\")\n",
    "    \n",
    "    # Show sample prediction results\n",
    "    if functional_results[\"predictions\"]:\n",
    "        print(\"\\nüéØ Sample Prediction Results:\")\n",
    "        for i, pred in enumerate(functional_results[\"predictions\"][:3]):\n",
    "            if pred[\"success\"] and pred[\"data\"]:\n",
    "                data = pred[\"data\"]\n",
    "                print(f\"  {i+1}. '{pred['text'][:30]}...' ‚Üí {data['label']} ({data['score']:.3f}) - {data['inference_time_ms']:.1f}ms\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping functional tests - API not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Error Handling Testing\n",
    "\n",
    "Let's test how the API handles various error conditions and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling test cases\n",
    "error_test_cases = [\n",
    "    {\n",
    "        \"name\": \"Empty text\",\n",
    "        \"text\": \"\",\n",
    "        \"expected_error\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Whitespace only\",\n",
    "        \"text\": \"   \",\n",
    "        \"expected_error\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Text too long\",\n",
    "        \"text\": \"a\" * 10000,\n",
    "        \"expected_error\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Very short text\",\n",
    "        \"text\": \"Hi\",\n",
    "        \"expected_error\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Normal positive text\",\n",
    "        \"text\": \"This is great!\",\n",
    "        \"expected_error\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Normal negative text\",\n",
    "        \"text\": \"This is terrible.\",\n",
    "        \"expected_error\": False\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Special characters\",\n",
    "        \"text\": \"Hello! @#$%^&*() üåü\",\n",
    "        \"expected_error\": False\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run error handling tests\n",
    "if api_available:\n",
    "    print(\"üö® Error Handling Tests:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    error_results = []\n",
    "    \n",
    "    for test_case in error_test_cases:\n",
    "        result = client.predict(test_case[\"text\"])\n",
    "        \n",
    "        # Determine if this behaved as expected\n",
    "        actual_error = not result[\"success\"]\n",
    "        expected_error = test_case[\"expected_error\"]\n",
    "        test_passed = actual_error == expected_error\n",
    "        \n",
    "        error_results.append({\n",
    "            \"test_name\": test_case[\"name\"],\n",
    "            \"expected_error\": expected_error,\n",
    "            \"actual_error\": actual_error,\n",
    "            \"passed\": test_passed,\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"response_time_ms\": result[\"response_time_ms\"],\n",
    "            \"error_message\": result[\"error\"]\n",
    "        })\n",
    "        \n",
    "        status = \"‚úÖ PASSED\" if test_passed else \"‚ùå FAILED\"\n",
    "        print(f\"{status} {test_case['name']}: Expected error={expected_error}, Got error={actual_error}\")\n",
    "        \n",
    "        if not test_passed:\n",
    "            if result[\"error\"]:\n",
    "                print(f\"      Error: {result['error'][:100]}...\")\n",
    "    \n",
    "    # Summary\n",
    "    passed_tests = sum(1 for r in error_results if r[\"passed\"])\n",
    "    total_tests = len(error_results)\n",
    "    \n",
    "    print(f\"\\nüìä Error Handling Test Results: {passed_tests}/{total_tests} PASSED\")\n",
    "    \n",
    "    # Show detailed results\n",
    "    error_df = pd.DataFrame(error_results)\n",
    "    display(error_df)\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping error handling tests - API not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Testing\n",
    "\n",
    "Let's test the API performance under different loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing functions\n",
    "def run_load_test(client: KubeSentimentAPIClient, num_requests: int, concurrent_users: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"Run load test with specified parameters.\"\"\"\n",
    "    \n",
    "    test_texts = [\n",
    "        \"I love this product! It's amazing.\",\n",
    "        \"This is terrible. Complete waste of money.\",\n",
    "        \"It's okay, nothing special but it works.\",\n",
    "        \"Outstanding quality and great service!\",\n",
    "        \"Awful experience. Never again.\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    def single_request(text: str) -> Dict[str, Any]:\n",
    "        result = client.predict(text)\n",
    "        return {\n",
    "            \"success\": result[\"success\"],\n",
    "            \"response_time_ms\": result[\"response_time_ms\"],\n",
    "            \"status_code\": result[\"status_code\"],\n",
    "            \"text_length\": len(text)\n",
    "        }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if concurrent_users == 1:\n",
    "        # Sequential requests\n",
    "        for i in range(num_requests):\n",
    "            text = test_texts[i % len(test_texts)]\n",
    "            results.append(single_request(text))\n",
    "    else:\n",
    "        # Concurrent requests\n",
    "        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:\n",
    "            futures = []\n",
    "            for i in range(num_requests):\n",
    "                text = test_texts[i % len(test_texts)]\n",
    "                futures.append(executor.submit(single_request, text))\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                results.append(future.result())\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    successful_requests = [r for r in results if r[\"success\"]]\n",
    "    response_times = [r[\"response_time_ms\"] for r in successful_requests if r[\"response_time_ms\"]]\n",
    "    \n",
    "    return {\n",
    "        \"total_requests\": num_requests,\n",
    "        \"concurrent_users\": concurrent_users,\n",
    "        \"total_time_seconds\": round(total_time, 2),\n",
    "        \"successful_requests\": len(successful_requests),\n",
    "        \"success_rate\": len(successful_requests) / num_requests,\n",
    "        \"avg_response_time_ms\": round(np.mean(response_times), 2) if response_times else None,\n",
    "        \"min_response_time_ms\": round(min(response_times), 2) if response_times else None,\n",
    "        \"max_response_time_ms\": round(max(response_times), 2) if response_times else None,\n",
    "        \"p95_response_time_ms\": round(np.percentile(response_times, 95), 2) if response_times else None,\n",
    "        \"requests_per_second\": round(num_requests / total_time, 2)\n",
    "    }\n",
    "\n",
    "# Run performance tests\n",
    "if api_available:\n",
    "    print(\"‚ö° Performance Testing:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_scenarios = [\n",
    "        {\"name\": \"Light Load\", \"requests\": 10, \"concurrency\": 1},\n",
    "        {\"name\": \"Medium Load\", \"requests\": 50, \"concurrency\": 2},\n",
    "        {\"name\": \"Heavy Load\", \"requests\": 100, \"concurrency\": 5}\n",
    "    ]\n",
    "    \n",
    "    performance_results = []\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"üß™ Running {scenario['name']} Test...\")\n",
    "        result = run_load_test(client, scenario[\"requests\"], scenario[\"concurrency\"])\n",
    "        result[\"scenario\"] = scenario[\"name\"]\n",
    "        performance_results.append(result)\n",
    "        \n",
    "        print(f\"   ‚úÖ Success Rate: {result['success_rate']:.1%}\")\n",
    "        print(f\"   ‚ö° Requests/sec: {result['requests_per_second']:.2f}\")\n",
    "        print(f\"   üìä Avg Response Time: {result['avg_response_time_ms']}ms\")\n",
    "        print(f\"   üìà P95 Response Time: {result['p95_response_time_ms']}ms\")\n",
    "        print()\n",
    "    \n",
    "    # Create performance comparison chart\n",
    "    perf_df = pd.DataFrame(performance_results)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('API Performance Test Results', fontsize=16)\n",
    "    \n",
    "    # Success rates\n",
    "    axes[0, 0].bar(perf_df['scenario'], perf_df['success_rate'], color='skyblue')\n",
    "    axes[0, 0].set_title('Success Rate by Test Scenario')\n",
    "    axes[0, 0].set_ylabel('Success Rate')\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Response times\n",
    "    axes[0, 1].bar(perf_df['scenario'], perf_df['avg_response_time_ms'], color='lightcoral')\n",
    "    axes[0, 1].set_title('Average Response Time')\n",
    "    axes[0, 1].set_ylabel('Response Time (ms)')\n",
    "    \n",
    "    # Throughput\n",
    "    axes[1, 0].bar(perf_df['scenario'], perf_df['requests_per_second'], color='lightgreen')\n",
    "    axes[1, 0].set_title('Requests Per Second')\n",
    "    axes[1, 0].set_ylabel('Requests/sec')\n",
    "    \n",
    "    # P95 response times\n",
    "    axes[1, 1].bar(perf_df['scenario'], perf_df['p95_response_time_ms'], color='orange')\n",
    "    axes[1, 1].set_title('95th Percentile Response Time')\n",
    "    axes[1, 1].set_ylabel('Response Time (ms)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display detailed results\n",
    "    print(\"üìä Detailed Performance Results:\")\n",
    "    display(perf_df)\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping performance tests - API not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Caching Performance Test\n",
    "\n",
    "Let's test the prediction caching functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction caching\n",
    "def test_prediction_caching(client: KubeSentimentAPIClient, num_iterations: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Test prediction caching by making repeated requests.\"\"\"\n",
    "    \n",
    "    test_texts = [\n",
    "        \"This is an amazing product!\",\n",
    "        \"I absolutely love this service.\",\n",
    "        \"Outstanding quality and support.\",\n",
    "        \"Best purchase I've ever made.\",\n",
    "        \"Highly recommended to everyone.\"\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"üîÑ Iteration {iteration + 1}/{num_iterations}...\")\n",
    "        \n",
    "        for text in test_texts:\n",
    "            result = client.predict(text)\n",
    "            \n",
    "            all_results.append({\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"text\": text,\n",
    "                \"success\": result[\"success\"],\n",
    "                \"response_time_ms\": result[\"response_time_ms\"],\n",
    "                \"cached\": result[\"data\"].get(\"cached\", False) if result[\"data\"] else False\n",
    "            })\n",
    "    \n",
    "    # Analyze caching performance\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    cached_results = df[df[\"cached\"] == True]\n",
    "    uncached_results = df[df[\"cached\"] == False]\n",
    "    \n",
    "    return {\n",
    "        \"total_requests\": len(df),\n",
    "        \"cached_requests\": len(cached_results),\n",
    "        \"cache_hit_rate\": len(cached_results) / len(df),\n",
    "        \"avg_cached_time_ms\": cached_results[\"response_time_ms\"].mean() if len(cached_results) > 0 else None,\n",
    "        \"avg_uncached_time_ms\": uncached_results[\"response_time_ms\"].mean() if len(uncached_results) > 0 else None,\n",
    "        \"time_improvement_percent\": (\n",
    "            (uncached_results[\"response_time_ms\"].mean() - cached_results[\"response_time_ms\"].mean()) \n",
    "            / uncached_results[\"response_time_ms\"].mean() * 100\n",
    "        ) if len(cached_results) > 0 and len(uncached_results) > 0 else None,\n",
    "        \"detailed_results\": df\n",
    "    }\n",
    "\n",
    "# Run caching test\n",
    "if api_available:\n",
    "    print(\"üîÑ Testing Prediction Caching:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    caching_results = test_prediction_caching(client, num_iterations=3)\n",
    "    \n",
    "    print(f\"üìä Total Requests: {caching_results['total_requests']}\")\n",
    "    print(f\"üíæ Cached Requests: {caching_results['cached_requests']}\")\n",
    "    print(f\"üéØ Cache Hit Rate: {caching_results['cache_hit_rate']:.1%}\")\n",
    "    \n",
    "    if caching_results['avg_cached_time_ms']:\n",
    "        print(f\"‚ö° Avg Cached Response Time: {caching_results['avg_cached_time_ms']:.2f}ms\")\n",
    "    \n",
    "    if caching_results['avg_uncached_time_ms']:\n",
    "        print(f\"üêå Avg Uncached Response Time: {caching_results['avg_uncached_time_ms']:.2f}ms\")\n",
    "    \n",
    "    if caching_results['time_improvement_percent']:\n",
    "        print(f\"üöÄ Time Improvement: {caching_results['time_improvement_percent']:.1f}%\")\n",
    "    \n",
    "    # Visualize caching performance\n",
    "    if caching_results['detailed_results'] is not None:\n",
    "        df = caching_results['detailed_results']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Response time by cache status\n",
    "        cache_status_data = [\n",
    "            df[df['cached'] == False]['response_time_ms'],\n",
    "            df[df['cached'] == True]['response_time_ms']\n",
    "        ]\n",
    "        \n",
    "        axes[0].boxplot(cache_status_data, labels=['Uncached', 'Cached'])\n",
    "        axes[0].set_title('Response Time: Cached vs Uncached')\n",
    "        axes[0].set_ylabel('Response Time (ms)')\n",
    "        \n",
    "        # Cache hits over iterations\n",
    "        iteration_cache_hits = df.groupby('iteration')['cached'].sum()\n",
    "        axes[1].plot(iteration_cache_hits.index, iteration_cache_hits.values, marker='o')\n",
    "        axes[1].set_title('Cache Hits by Iteration')\n",
    "        axes[1].set_xlabel('Iteration')\n",
    "        axes[1].set_ylabel('Number of Cache Hits')\n",
    "        axes[1].set_xticks(range(1, len(iteration_cache_hits) + 1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping caching test - API not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Async Testing\n",
    "\n",
    "Let's test the API using async HTTP requests to simulate real-world usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async testing with httpx\n",
    "async def async_predict(client: httpx.AsyncClient, text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Make async prediction request.\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = await client.post(\n",
    "            f\"{API_BASE_URL}/predict\",\n",
    "            json={\"text\": text},\n",
    "            timeout=TIMEOUT\n",
    "        )\n",
    "        response_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return {\n",
    "            \"success\": response.status_code == 200,\n",
    "            \"status_code\": response.status_code,\n",
    "            \"response_time_ms\": round(response_time, 2),\n",
    "            \"data\": response.json() if response.status_code == 200 else None,\n",
    "            \"error\": response.text if response.status_code != 200 else None,\n",
    "            \"text\": text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"status_code\": None,\n",
    "            \"response_time_ms\": None,\n",
    "            \"data\": None,\n",
    "            \"error\": str(e),\n",
    "            \"text\": text\n",
    "        }\n",
    "\n",
    "async def run_async_load_test(num_requests: int, concurrent_requests: int) -> Dict[str, Any]:\n",
    "    \"\"\"Run async load test.\"\"\"\n",
    "    \n",
    "    test_texts = [\n",
    "        \"I love this amazing product!\",\n",
    "        \"This is absolutely terrible.\",\n",
    "        \"It's decent, does the job.\",\n",
    "        \"Outstanding customer service!\",\n",
    "        \"Complete waste of money.\"\n",
    "    ]\n",
    "    \n",
    "    async def make_requests():\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            tasks = []\n",
    "            for i in range(num_requests):\n",
    "                text = test_texts[i % len(test_texts)]\n",
    "                tasks.append(async_predict(client, text))\n",
    "            \n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            return results\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = await make_requests()\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Process results\n",
    "    successful_requests = [r for r in results if isinstance(r, dict) and r.get(\"success\", False)]\n",
    "    response_times = [r[\"response_time_ms\"] for r in successful_requests if r.get(\"response_time_ms\")]\n",
    "    \n",
    "    return {\n",
    "        \"total_requests\": num_requests,\n",
    "        \"concurrent_requests\": concurrent_requests,\n",
    "        \"total_time_seconds\": round(total_time, 2),\n",
    "        \"successful_requests\": len(successful_requests),\n",
    "        \"success_rate\": len(successful_requests) / num_requests if num_requests > 0 else 0,\n",
    "        \"avg_response_time_ms\": round(np.mean(response_times), 2) if response_times else None,\n",
    "        \"min_response_time_ms\": round(min(response_times), 2) if response_times else None,\n",
    "        \"max_response_time_ms\": round(max(response_times), 2) if response_times else None,\n",
    "        \"p95_response_time_ms\": round(np.percentile(response_times, 95), 2) if response_times else None,\n",
    "        \"requests_per_second\": round(num_requests / total_time, 2) if total_time > 0 else 0\n",
    "    }\n",
    "\n",
    "# Run async performance test\n",
    "if api_available:\n",
    "    print(\"üîÑ Async Performance Testing:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    async_results = await run_async_load_test(num_requests=50, concurrent_requests=10)\n",
    "    \n",
    "    print(f\"üìä Total Requests: {async_results['total_requests']}\")\n",
    "    print(f\"üîÑ Concurrent Requests: {async_results['concurrent_requests']}\")\n",
    "    print(f\"‚úÖ Success Rate: {async_results['success_rate']:.1%}\")\n",
    "    print(f\"‚ö° Requests/sec: {async_results['requests_per_second']:.2f}\")\n",
    "    print(f\"üìä Avg Response Time: {async_results['avg_response_time_ms']}ms\")\n",
    "    print(f\"üìà P95 Response Time: {async_results['p95_response_time_ms']}ms\")\n",
    "    \n",
    "    print(\"\\nüîç Async vs Sync Comparison:\")\n",
    "    if 'performance_results' in locals():\n",
    "        sync_avg = performance_results[1]['avg_response_time_ms']  # Medium load test\n",
    "        async_avg = async_results['avg_response_time_ms']\n",
    "        \n",
    "        print(f\"   üîÑ Sync (50 requests, 2 concurrent): {sync_avg}ms avg\")\n",
    "        print(f\"   ‚ö° Async (50 requests, 10 concurrent): {async_avg}ms avg\")\n",
    "        \n",
    "        if sync_avg and async_avg:\n",
    "            improvement = ((sync_avg - async_avg) / sync_avg) * 100\n",
    "            print(f\"   üöÄ Async improvement: {improvement:.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping async testing - API not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Automated Testing\n",
    "\n",
    "We can integrate automated tests directly into our notebooks using `pytest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test file\n",
    "test_code = \"\"\"\n",
    "import requests\n",
    "def test_health_check():\n",
    "    # Health check should be available without auth\n",
    "    response = requests.get('http://localhost:8000/health')\n",
    "    assert response.status_code == 200, f\\\"Expected 200, got {response.status_code}\\\"\n",
    "    assert response.json()[\\\"status\\\"] == \\\"healthy\\\", \\\"Service is not healthy\\\"\n",
    "\n",
    "def test_prediction():\n",
    "    # Test that a prediction can be made successfully\n",
    "    response = requests.post('http://localhost:8000/predict', json={'text': 'This is a test'})\n",
    "    assert response.status_code == 200, f\\\"Expected 200, got {response.status_code}\\\"\n",
    "    assert 'label' in response.json(), \\\"Response should contain a 'label' key\\\"\n",
    "\"\"\"\n",
    "with open(\"test_api.py\", \"w\") as f:\n",
    "    f.write(test_code)\n",
    "\n",
    "# Run pytest\n",
    "!pytest test_api.py -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Test Report Generation\n",
    "\n",
    "Let's generate a comprehensive test report summarizing all our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive test report\n",
    "def generate_test_report():\n",
    "    \"\"\"Generate a comprehensive test report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"test_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"api_endpoint\": API_BASE_URL,\n",
    "        \"api_available\": api_available,\n",
    "        \"sections\": {}\n",
    "    }\n",
    "    \n",
    "    if not api_available:\n",
    "        return report\n",
    "    \n",
    "    # Health Check Section\n",
    "    if 'health_result' in locals():\n",
    "        report[\"sections\"][\"health_check\"] = {\n",
    "            \"status\": health_result[\"success\"],\n",
    "            \"response_time_ms\": health_result[\"response_time_ms\"],\n",
    "            \"data\": health_result[\"data\"]\n",
    "        }\n",
    "    \n",
    "    # Functional Tests Section\n",
    "    if 'functional_results' in locals():\n",
    "        functional_summary = {\n",
    "            \"total_tests\": 3 + len(functional_results[\"predictions\"]),  # health, model_info, metrics + predictions\n",
    "            \"passed_tests\": sum([\n",
    "                functional_results[\"health_check\"][\"success\"],\n",
    "                functional_results[\"model_info\"][\"success\"],\n",
    "                functional_results[\"metrics\"][\"success\"],\n",
    "                sum(1 for p in functional_results[\"predictions\"] if p[\"success\"])\n",
    "            ]),\n",
    "            \"success_rate\": None\n",
    "        }\n",
    "        functional_summary[\"success_rate\"] = functional_summary[\"passed_tests\"] / functional_summary[\"total_tests\"]\n",
    "        report[\"sections\"][\"functional_tests\"] = functional_summary\n",
    "    \n",
    "    # Error Handling Section\n",
    "    if 'error_results' in locals():\n",
    "        error_summary = {\n",
    "            \"total_tests\": len(error_results),\n",
    "            \"passed_tests\": sum(1 for r in error_results if r[\"passed\"]),\n",
    "            \"success_rate\": sum(1 for r in error_results if r[\"passed\"]) / len(error_results),\n",
    "            \"details\": error_results\n",
    "        }\n",
    "        report[\"sections\"][\"error_handling\"] = error_summary\n",
    "    \n",
    "    # Performance Section\n",
    "    if 'performance_results' in locals():\n",
    "        report[\"sections\"][\"performance\"] = {\n",
    "            \"scenarios_tested\": len(performance_results),\n",
    "            \"best_throughput\": max(r[\"requests_per_second\"] for r in performance_results),\n",
    "            \"avg_response_time_ms\": np.mean([r[\"avg_response_time_ms\"] for r in performance_results if r[\"avg_response_time_ms\"]]),\n",
    "            \"overall_success_rate\": np.mean([r[\"success_rate\"] for r in performance_results]),\n",
    "            \"details\": performance_results\n",
    "        }\n",
    "    \n",
    "    # Caching Section\n",
    "    if 'caching_results' in locals():\n",
    "        report[\"sections\"][\"caching\"] = {\n",
    "            \"cache_hit_rate\": caching_results[\"cache_hit_rate\"],\n",
    "            \"time_improvement_percent\": caching_results[\"time_improvement_percent\"],\n",
    "            \"avg_cached_time_ms\": caching_results[\"avg_cached_time_ms\"],\n",
    "            \"avg_uncached_time_ms\": caching_results[\"avg_uncached_time_ms\"]\n",
    "        }\n",
    "    \n",
    "    # Async Performance Section\n",
    "    if 'async_results' in locals():\n",
    "        report[\"sections\"][\"async_performance\"] = async_results\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate and display test report\n",
    "print(\"üìã API Testing Report:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_report = generate_test_report()\n",
    "\n",
    "print(f\"üïí Test Timestamp: {test_report['test_timestamp']}\")\n",
    "print(f\"üåê API Endpoint: {test_report['api_endpoint']}\")\n",
    "print(f\"‚úÖ API Available: {test_report['api_available']}\")\n",
    "\n",
    "if test_report['api_available']:\n",
    "    for section_name, section_data in test_report['sections'].items():\n",
    "        print(f\"\\nüìä {section_name.replace('_', ' ').title()}:\")\n",
    "        \n",
    "        if section_name == \"health_check\":\n",
    "            print(f\"   Status: {'‚úÖ Healthy' if section_data['status'] else '‚ùå Unhealthy'}\")\n",
    "            print(f\"   Response Time: {section_data['response_time_ms']}ms\")\n",
    "            if section_data['data']:\n",
    "                print(f\"   Version: {section_data['data'].get('version', 'N/A')}\")\n",
    "                print(f\"   Model Status: {section_data['data'].get('model_status', 'N/A')}\")\n",
    "        \n",
    "        elif section_name in [\"functional_tests\", \"error_handling\"]:\n",
    "            print(f\"   Success Rate: {section_data['success_rate']:.1%}\")\n",
    "            print(f\"   Passed: {section_data['passed_tests']}/{section_data['total_tests']}\")\n",
    "        \n",
    "        elif section_name == \"performance\":\n",
    "            print(f\"   Best Throughput: {section_data['best_throughput']:.1f} req/sec\")\n",
    "            print(f\"   Avg Response Time: {section_data['avg_response_time_ms']:.1f}ms\")\n",
    "            print(f\"   Overall Success Rate: {section_data['overall_success_rate']:.1%}\")\n",
    "        \n",
    "        elif section_name == \"caching\":\n",
    "            print(f\"   Cache Hit Rate: {section_data['cache_hit_rate']:.1%}\")\n",
    "            if section_data['time_improvement_percent']:\n",
    "                print(f\"   Time Improvement: {section_data['time_improvement_percent']:.1f}%\")\n",
    "            print(f\"   Cached Avg Time: {section_data['avg_cached_time_ms']:.1f}ms\")\n",
    "            print(f\"   Uncached Avg Time: {section_data['avg_uncached_time_ms']:.1f}ms\")\n",
    "        \n",
    "        elif section_name == \"async_performance\":\n",
    "            print(f\"   Throughput: {section_data['requests_per_second']:.1f} req/sec\")\n",
    "            print(f\"   Avg Response Time: {section_data['avg_response_time_ms']:.1f}ms\")\n",
    "            print(f\"   Success Rate: {section_data['success_rate']:.1%}\")\n",
    "\n",
    "# Save report to JSON\n",
    "import json\n",
    "report_filename = f\"api_test_report_{int(time.time())}.json\"\n",
    "with open(report_filename, 'w') as f:\n",
    "    json.dump(test_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Test report saved to: {report_filename}\")\n",
    "\n",
    "print(\"\\nüéâ API Testing Complete!\")\n",
    "print(\"\\nüí° Next Steps:\")\n",
    "print(\"   ‚Ä¢ Review the test results above\")\n",
    "print(\"   ‚Ä¢ Check the generated JSON report for detailed metrics\")\n",
    "print(\"   ‚Ä¢ Explore other notebooks for advanced analysis\")\n",
    "print(\"   ‚Ä¢ Consider running the benchmarking suite for production testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
