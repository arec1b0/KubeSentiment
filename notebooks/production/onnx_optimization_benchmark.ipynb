{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ ONNX Optimization Benchmark\n",
    "\n",
    "This notebook benchmarks the performance of the ONNX runtime for the KubeSentiment project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the benefits of ONNX for model inference.\n",
    "2. Learn how to convert a PyTorch model to ONNX.\n",
    "3. Benchmark the performance of the ONNX runtime against the PyTorch runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Setup and Dependencies\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this notebook\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… Version Check\n",
    "Let's check the versions of the installed libraries to ensure our environment is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List installed packages to ensure reproducibility\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– What is ONNX?\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is an open format for representing machine learning models. It allows you to convert models between different frameworks, such as PyTorch, TensorFlow, and Keras. ONNX also provides a runtime that can execute models in a highly optimized way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the Model to ONNX\n",
    "\n",
    "The first step is to convert the PyTorch model to the ONNX format. We can do this using the `torch.onnx.export()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the model and tokenizer\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create a dummy input for the model\n",
    "dummy_input = tokenizer(\"This is a test sentence.\", return_tensors=\"pt\")\n",
    "\n",
    "# Export the model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "    \"sentiment_model.onnx\",\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"logits\": {0: \"batch_size\"},\n",
    "    },\n",
    "    opset_version=11,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking the ONNX Runtime\n",
    "\n",
    "Now that we have the model in ONNX format, we can benchmark its performance against the PyTorch runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "# Create an ONNX session\n",
    "ort_session = ort.InferenceSession(\"sentiment_model.onnx\")\n",
    "\n",
    "# Prepare the input\n",
    "input_ids = dummy_input[\"input_ids\"].numpy()\n",
    "attention_mask = dummy_input[\"attention_mask\"].numpy()\n",
    "\n",
    "# Benchmark the PyTorch runtime\n",
    "pytorch_times = []\n",
    "for _ in range(100):\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        model(**dummy_input)\n",
    "    pytorch_times.append(time.time() - start_time)\n",
    "\n",
    "# Benchmark the ONNX runtime\n",
    "onnx_times = []\n",
    "for _ in range(100):\n",
    "    start_time = time.time()\n",
    "    ort_session.run(None, {\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "    onnx_times.append(time.time() - start_time)\n",
    "\n",
    "# Compare the results\n",
    "print(f\"PyTorch average inference time: {np.mean(pytorch_times) * 1000:.2f} ms\")\n",
    "print(f\"ONNX average inference time: {np.mean(onnx_times) * 1000:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
