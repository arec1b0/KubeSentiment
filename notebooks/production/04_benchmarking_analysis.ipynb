{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Benchmarking Analysis: Performance & Cost Evaluation\n",
    "\n",
    "This notebook provides comprehensive analysis of KubeSentiment's benchmarking framework, including performance metrics, cost analysis, and infrastructure comparisons.\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand the benchmarking framework architecture\n",
    "2. Analyze performance metrics across different instance types\n",
    "3. Perform cost-benefit analysis for infrastructure choices\n",
    "4. Compare CPU vs GPU performance characteristics\n",
    "5. Generate performance reports and visualizations\n",
    "6. Understand scaling patterns and optimization opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Dependencies\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for this notebook\n",
    "# Note: This cell might take a few minutes to run\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Version Check\n",
    "Let's check the versions of the installed libraries to ensure our environment is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List installed packages to ensure reproducibility\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Benchmarking Framework Overview\n",
    "\n",
    "### Architecture\n",
    "\n",
    "The benchmarking framework consists of:\n",
    "\n",
    "```\n",
    "Benchmarking Framework\n",
    "‚îú‚îÄ‚îÄ Load Testing (locust/load-test.py)\n",
    "‚îú‚îÄ‚îÄ Resource Monitoring (resource-monitor.py)\n",
    "‚îú‚îÄ‚îÄ Cost Calculator (cost-calculator.py)\n",
    "‚îú‚îÄ‚îÄ Report Generator (report-generator.py)\n",
    "‚îî‚îÄ‚îÄ Configurations (configs/)\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Load Testing**: Simulates user traffic with configurable concurrency\n",
    "2. **Resource Monitoring**: Tracks CPU, memory, and network usage\n",
    "3. **Cost Analysis**: Calculates costs per prediction and total TCO\n",
    "4. **Report Generation**: Creates HTML/PDF reports with visualizations\n",
    "5. **Infrastructure Configs**: Pre-defined instance types and pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Define paths\n",
    "BENCHMARKING_DIR = Path(\"../../benchmarking\")\n",
    "CONFIGS_DIR = BENCHMARKING_DIR / \"configs\"\n",
    "SCRIPTS_DIR = BENCHMARKING_DIR / \"scripts\"\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"üìÅ Benchmarking directory: {BENCHMARKING_DIR.absolute()}\")\n",
    "print(f\"‚öôÔ∏è Configs directory: {CONFIGS_DIR.absolute()}\")\n",
    "print(f\"üîß Scripts directory: {SCRIPTS_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Loading Benchmark Configurations\n",
    "\n",
    "Let's examine the benchmarking configurations and instance types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark configurations\n",
    "def load_benchmark_configs():\n",
    "    \"\"\"Load all benchmark configuration files.\"\"\"\n",
    "    configs = {}\n",
    "    \n",
    "    # Load main benchmark config\n",
    "    main_config_path = CONFIGS_DIR / \"benchmark-config.yaml\"\n",
    "    if main_config_path.exists():\n",
    "        try:\n",
    "            import yaml\n",
    "            with open(main_config_path, 'r') as f:\n",
    "                configs['main'] = yaml.safe_load(f)\n",
    "            print(\"‚úÖ Main benchmark config loaded\")\n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è PyYAML not available, skipping YAML configs\")\n",
    "            configs['main'] = {}\n",
    "    else:\n",
    "        print(\"‚ùå Main benchmark config not found\")\n",
    "        configs['main'] = {}\n",
    "    \n",
    "    # Load CPU instances config\n",
    "    cpu_config_path = CONFIGS_DIR / \"cpu-instances.yaml\"\n",
    "    if cpu_config_path.exists():\n",
    "        try:\n",
    "            with open(cpu_config_path, 'r') as f:\n",
    "                configs['cpu_instances'] = yaml.safe_load(f)\n",
    "            print(\"‚úÖ CPU instances config loaded\")\n",
    "        except:\n",
    "            configs['cpu_instances'] = {}\n",
    "    \n",
    "    # Load GPU instances config  \n",
    "    gpu_config_path = CONFIGS_DIR / \"gpu-instances.yaml\"\n",
    "    if gpu_config_path.exists():\n",
    "        try:\n",
    "            with open(gpu_config_path, 'r') as f:\n",
    "                configs['gpu_instances'] = yaml.safe_load(f)\n",
    "            print(\"‚úÖ GPU instances config loaded\")\n",
    "        except:\n",
    "            configs['gpu_instances'] = {}\n",
    "    \n",
    "    return configs\n",
    "\n",
    "# Load configurations\n",
    "configs = load_benchmark_configs()\n",
    "\n",
    "print(\"\\nüìä Benchmark Configuration Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Main config\n",
    "if configs.get('main'):\n",
    "    main_config = configs['main']\n",
    "    print(\"üéØ Benchmark Settings:\")\n",
    "    print(f\"   Duration: {main_config.get('benchmark', {}).get('duration', 'N/A')}\")\n",
    "    print(f\"   Concurrent Users: {main_config.get('benchmark', {}).get('concurrent_users', 'N/A')}\")\n",
    "    print(f\"   Ramp-up Time: {main_config.get('benchmark', {}).get('ramp_up_time', 'N/A')}\")\n",
    "    \n",
    "    # Instance types\n",
    "    cpu_instances = main_config.get('instances', {}).get('cpu', [])\n",
    "    gpu_instances = main_config.get('instances', {}).get('gpu', [])\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è CPU Instances ({len(cpu_instances)}):\")\n",
    "    for instance in cpu_instances:\n",
    "        print(f\"   ‚Ä¢ {instance}\")\n",
    "    \n",
    "    print(f\"\\nüéÆ GPU Instances ({len(gpu_instances)}):\")\n",
    "    for instance in gpu_instances:\n",
    "        print(f\"   ‚Ä¢ {instance}\")\n",
    "    \n",
    "    # Costs\n",
    "    costs = main_config.get('costs', {})\n",
    "    print(f\"\\nüí∞ Cost Configuration ({len(costs)} instances):\")\n",
    "    for instance, cost in costs.items():\n",
    "        print(f\"   ‚Ä¢ {instance}: ${cost}/hour\")\n",
    "else:\n",
    "    print(\"‚ùå No configuration data available\")\n",
    "\n",
    "# Sample instance data for demonstration\n",
    "sample_instances = {\n",
    "    \"cpu\": [\n",
    "        {\"type\": \"t3.medium\", \"vcpu\": 2, \"memory\": \"4GB\", \"cost_per_hour\": 0.0416},\n",
    "        {\"type\": \"c5.large\", \"vcpu\": 2, \"memory\": \"4GB\", \"cost_per_hour\": 0.096},\n",
    "        {\"type\": \"c5.xlarge\", \"vcpu\": 4, \"memory\": \"8GB\", \"cost_per_hour\": 0.192}\n",
    "    ],\n",
    "    \"gpu\": [\n",
    "        {\"type\": \"p3.2xlarge\", \"vcpu\": 8, \"memory\": \"61GB\", \"gpu\": \"V100\", \"cost_per_hour\": 3.06},\n",
    "        {\"type\": \"g4dn.xlarge\", \"vcpu\": 4, \"memory\": \"16GB\", \"gpu\": \"T4\", \"cost_per_hour\": 0.526}\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Sample Instance Specifications:\")\n",
    "for category, instances in sample_instances.items():\n",
    "    print(f\"\\n{category.upper()} Instances:\")\n",
    "    for instance in instances:\n",
    "        specs = f\"{instance['type']}: {instance['vcpu']} vCPU, {instance['memory']} RAM\"\n",
    "        if 'gpu' in instance:\n",
    "            specs += f\", {instance['gpu']} GPU\"\n",
    "        specs += f\" - ${instance['cost_per_hour']}/hr\"\n",
    "        print(f\"   ‚Ä¢ {specs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Sample Benchmark Data Analysis\n",
    "\n",
    "Let's create and analyze sample benchmark data to demonstrate the analysis capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample benchmark data\n",
    "def generate_sample_benchmark_data():\n",
    "    \"\"\"Generate sample benchmark data for analysis.\"\"\"\n",
    "    \n",
    "    # Instance types\n",
    "    instances = [\n",
    "        \"t3.medium\", \"c5.large\", \"c5.xlarge\", \"c5.2xlarge\",\n",
    "        \"p3.2xlarge\", \"g4dn.xlarge\", \"g4dn.2xlarge\"\n",
    "    ]\n",
    "    \n",
    "    # User concurrency levels\n",
    "    concurrency_levels = [1, 5, 10, 20, 50, 100]\n",
    "    \n",
    "    data = []\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    for instance in instances:\n",
    "        # Base performance characteristics by instance type\n",
    "        if instance.startswith(\"t3\"):\n",
    "            base_rps = 50\n",
    "            base_latency = 80\n",
    "            cost_per_hour = 0.0416\n",
    "        elif instance.startswith(\"c5.large\"):\n",
    "            base_rps = 120\n",
    "            base_latency = 45\n",
    "            cost_per_hour = 0.096\n",
    "        elif instance.startswith(\"c5.xlarge\"):\n",
    "            base_rps = 250\n",
    "            base_latency = 30\n",
    "            cost_per_hour = 0.192\n",
    "        elif instance.startswith(\"c5.2xlarge\"):\n",
    "            base_rps = 500\n",
    "            base_latency = 25\n",
    "            cost_per_hour = 0.384\n",
    "        elif instance.startswith(\"p3.2xlarge\"):\n",
    "            base_rps = 800\n",
    "            base_latency = 15\n",
    "            cost_per_hour = 3.06\n",
    "        elif instance.startswith(\"g4dn.xlarge\"):\n",
    "            base_rps = 400\n",
    "            base_latency = 20\n",
    "            cost_per_hour = 0.526\n",
    "        elif instance.startswith(\"g4dn.2xlarge\"):\n",
    "            base_rps = 750\n",
    "            base_latency = 18\n",
    "            cost_per_hour = 0.752\n",
    "        \n",
    "        for concurrency in concurrency_levels:\n",
    "            # Performance degrades with higher concurrency\n",
    "            degradation_factor = 1 / (1 + (concurrency - 1) * 0.1)\n",
    "            \n",
    "            # Add some randomness\n",
    "            rps_noise = np.random.normal(0, base_rps * 0.1)\n",
    "            latency_noise = np.random.normal(0, base_latency * 0.2)\n",
    "            \n",
    "            actual_rps = max(1, (base_rps * degradation_factor) + rps_noise)\n",
    "            actual_latency = max(10, (base_latency / degradation_factor) + latency_noise)\n",
    "            \n",
    "            # Success rate (degrades at high concurrency)\n",
    "            success_rate = min(1.0, 0.98 - (concurrency - 10) * 0.005) if concurrency > 10 else 0.98\n",
    "            \n",
    "            # CPU and memory usage\n",
    "            cpu_usage = min(95, 20 + (concurrency * 2) + np.random.normal(0, 5))\n",
    "            memory_usage = min(90, 30 + (concurrency * 1.5) + np.random.normal(0, 3))\n",
    "            \n",
    "            data.append({\n",
    "                \"instance_type\": instance,\n",
    "                \"concurrency\": concurrency,\n",
    "                \"requests_per_second\": round(actual_rps, 2),\n",
    "                \"avg_latency_ms\": round(actual_latency, 2),\n",
    "                \"p95_latency_ms\": round(actual_latency * 1.5, 2),\n",
    "                \"p99_latency_ms\": round(actual_latency * 2.0, 2),\n",
    "                \"success_rate\": round(success_rate, 3),\n",
    "                \"cpu_usage_percent\": round(cpu_usage, 2),\n",
    "                \"memory_usage_percent\": round(memory_usage, 2),\n",
    "                \"cost_per_hour\": cost_per_hour,\n",
    "                \"cost_per_1000_predictions\": round((cost_per_hour / actual_rps) * 1000, 4)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate sample data\n",
    "benchmark_df = generate_sample_benchmark_data()\n",
    "\n",
    "print(\"üìä Sample Benchmark Data Generated:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìã Total data points: {len(benchmark_df)}\")\n",
    "print(f\"üñ•Ô∏è Instance types: {benchmark_df['instance_type'].nunique()}\")\n",
    "print(f\"üë• Concurrency levels: {benchmark_df['concurrency'].nunique()}\")\n",
    "\n",
    "print(\"\\nüîç Data Preview:\")\n",
    "display(benchmark_df.head(10))\n",
    "\n",
    "print(\"\\nüìà Summary Statistics:\")\n",
    "display(benchmark_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics across different instance types and concurrency levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "fig.suptitle('KubeSentiment Benchmark Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Throughput by instance type\n",
    "throughput_data = benchmark_df.groupby('instance_type')['requests_per_second'].mean().sort_values(ascending=True)\n",
    "axes[0, 0].barh(range(len(throughput_data)), throughput_data.values)\n",
    "axes[0, 0].set_yticks(range(len(throughput_data)))\n",
    "axes[0, 0].set_yticklabels(throughput_data.index)\n",
    "axes[0, 0].set_xlabel('Requests per Second')\n",
    "axes[0, 0].set_title('Average Throughput by Instance Type')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Latency by instance type\n",
    "latency_data = benchmark_df.groupby('instance_type')['avg_latency_ms'].mean().sort_values()\n",
    "axes[0, 1].bar(range(len(latency_data)), latency_data.values)\n",
    "axes[0, 1].set_xticks(range(len(latency_data)))\n",
    "axes[0, 1].set_xticklabels(latency_data.index, rotation=45, ha='right')\n",
    "axes[0, 1].set_ylabel('Average Latency (ms)')\n",
    "axes[0, 1].set_title('Average Latency by Instance Type')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cost per 1000 predictions\n",
    "cost_data = benchmark_df.groupby('instance_type')['cost_per_1000_predictions'].mean().sort_values()\n",
    "colors = ['green' if x < 0.01 else 'orange' if x < 0.05 else 'red' for x in cost_data.values]\n",
    "bars = axes[0, 2].bar(range(len(cost_data)), cost_data.values, color=colors)\n",
    "axes[0, 2].set_xticks(range(len(cost_data)))\n",
    "axes[0, 2].set_xticklabels(cost_data.index, rotation=45, ha='right')\n",
    "axes[0, 2].set_ylabel('Cost ($/1000 predictions)')\n",
    "axes[0, 2].set_title('Cost Efficiency by Instance Type')\n",
    "axes[0, 2].set_yscale('log')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Throughput vs Concurrency (selected instances)\n",
    "selected_instances = ['t3.medium', 'c5.xlarge', 'p3.2xlarge', 'g4dn.xlarge']\n",
    "for instance in selected_instances:\n",
    "    instance_data = benchmark_df[benchmark_df['instance_type'] == instance]\n",
    "    axes[1, 0].plot(instance_data['concurrency'], instance_data['requests_per_second'], \n",
    "                   marker='o', label=instance, linewidth=2)\n",
    "axes[1, 0].set_xlabel('Concurrent Users')\n",
    "axes[1, 0].set_ylabel('Requests per Second')\n",
    "axes[1, 0].set_title('Throughput Scaling by Instance Type')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Latency vs Concurrency\n",
    "for instance in selected_instances:\n",
    "    instance_data = benchmark_df[benchmark_df['instance_type'] == instance]\n",
    "    axes[1, 1].plot(instance_data['concurrency'], instance_data['avg_latency_ms'], \n",
    "                   marker='s', label=instance, linewidth=2)\n",
    "axes[1, 1].set_xlabel('Concurrent Users')\n",
    "axes[1, 1].set_ylabel('Average Latency (ms)')\n",
    "axes[1, 1].set_title('Latency Scaling by Instance Type')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Resource utilization\n",
    "cpu_data = benchmark_df.groupby('instance_type')['cpu_usage_percent'].mean()\n",
    "memory_data = benchmark_df.groupby('instance_type')['memory_usage_percent'].mean()\n",
    "\n",
    "x = np.arange(len(cpu_data))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 2].bar(x - width/2, cpu_data.values, width, label='CPU', alpha=0.8)\n",
    "axes[1, 2].bar(x + width/2, memory_data.values, width, label='Memory', alpha=0.8)\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(cpu_data.index, rotation=45, ha='right')\n",
    "axes[1, 2].set_ylabel('Usage (%)')\n",
    "axes[1, 2].set_title('Average Resource Utilization')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Success rate distribution\n",
    "success_data = benchmark_df.groupby('instance_type')['success_rate'].mean().sort_values(ascending=True)\n",
    "axes[2, 0].barh(range(len(success_data)), success_data.values)\n",
    "axes[2, 0].set_yticks(range(len(success_data)))\n",
    "axes[2, 0].set_yticklabels(success_data.index)\n",
    "axes[2, 0].set_xlabel('Success Rate')\n",
    "axes[2, 0].set_title('Success Rate by Instance Type')\n",
    "axes[2, 0].axvline(0.95, color='red', linestyle='--', alpha=0.7, label='95% Target')\n",
    "axes[2, 0].legend()\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Performance per dollar\n",
    "perf_per_dollar = benchmark_df.groupby('instance_type').agg({\n",
    "    'requests_per_second': 'mean',\n",
    "    'cost_per_hour': 'first'\n",
    "})\n",
    "perf_per_dollar['performance_per_dollar'] = perf_per_dollar['requests_per_second'] / perf_per_dollar['cost_per_hour']\n",
    "perf_per_dollar = perf_per_dollar.sort_values('performance_per_dollar', ascending=True)\n",
    "\n",
    "axes[2, 1].barh(range(len(perf_per_dollar)), perf_per_dollar['performance_per_dollar'].values)\n",
    "axes[2, 1].set_yticks(range(len(perf_per_dollar)))\n",
    "axes[2, 1].set_yticklabels(perf_per_dollar.index)\n",
    "axes[2, 1].set_xlabel('Requests per Second per Dollar')\n",
    "axes[2, 1].set_title('Performance per Dollar (Efficiency)')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Cost breakdown\n",
    "cost_breakdown = benchmark_df.groupby('instance_type')['cost_per_hour'].first().sort_values()\n",
    "axes[2, 2].pie(cost_breakdown.values, labels=cost_breakdown.index, autopct='%1.1f%%')\n",
    "axes[2, 2].set_title('Cost Distribution by Instance Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"üìä Performance Analysis Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Best performers\n",
    "best_throughput = benchmark_df.loc[benchmark_df['requests_per_second'].idxmax()]\n",
    "best_latency = benchmark_df.loc[benchmark_df['avg_latency_ms'].idxmin()]\n",
    "best_efficiency = perf_per_dollar.loc[perf_per_dollar['performance_per_dollar'].idxmax()]\n",
    "\n",
    "print(f\"üöÄ Best Throughput: {best_throughput['instance_type']} - {best_throughput['requests_per_second']} RPS\")\n",
    "print(f\"‚ö° Best Latency: {best_latency['instance_type']} - {best_latency['avg_latency_ms']}ms\")\n",
    "print(f\"üí∞ Best Efficiency: {best_efficiency.name} - {best_efficiency['performance_per_dollar']:.1f} RPS/$\")\n",
    "print(f\"üìà Average Success Rate: {benchmark_df['success_rate'].mean():.1%}\")\n",
    "print(f\"üí∏ Average Cost/1000 Predictions: ${benchmark_df['cost_per_1000_predictions'].mean():.4f}\")\n",
    "\n",
    "# Instance type categorization\n",
    "cpu_instances = benchmark_df[~benchmark_df['instance_type'].str.contains('p3|g4dn')]\n",
    "gpu_instances = benchmark_df[benchmark_df['instance_type'].str.contains('p3|g4dn')]\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è CPU Instances Average: {cpu_instances['requests_per_second'].mean():.1f} RPS, ${cpu_instances['cost_per_1000_predictions'].mean():.4f}/1000pred\")\n",
    "print(f\"üéÆ GPU Instances Average: {gpu_instances['requests_per_second'].mean():.1f} RPS, ${gpu_instances['cost_per_1000_predictions'].mean():.4f}/1000pred\")\n",
    "\n",
    "# Scaling analysis\n",
    "scaling_efficiency = benchmark_df.groupby('concurrency')['requests_per_second'].mean()\n",
    "print(f\"\\nüìä Scaling Efficiency: {scaling_efficiency.pct_change().mean():.1%} avg improvement per concurrency level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Cost-Benefit Analysis\n",
    "\n",
    "Let's perform a detailed cost-benefit analysis to help choose the optimal infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost-benefit analysis\n",
    "def perform_cost_benefit_analysis(benchmark_df, daily_predictions=100000, days_per_month=30):\n",
    "    \"\"\"Perform comprehensive cost-benefit analysis.\"\"\"\n",
    "    \n",
    "    # Monthly predictions\n",
    "    monthly_predictions = daily_predictions * days_per_month\n",
    "    \n",
    "    # Calculate costs for each instance\n",
    "    analysis_df = benchmark_df.groupby('instance_type').agg({\n",
    "        'requests_per_second': 'mean',\n",
    "        'avg_latency_ms': 'mean',\n",
    "        'cost_per_hour': 'first',\n",
    "        'cost_per_1000_predictions': 'mean',\n",
    "        'success_rate': 'mean',\n",
    "        'cpu_usage_percent': 'mean',\n",
    "        'memory_usage_percent': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate monthly costs and capacity\n",
    "    hours_per_month = 24 * days_per_month\n",
    "    analysis_df['monthly_cost'] = analysis_df['cost_per_hour'] * hours_per_month\n",
    "    analysis_df['monthly_capacity'] = analysis_df['requests_per_second'] * 3600 * hours_per_month\n",
    "    analysis_df['capacity_utilization'] = (monthly_predictions / analysis_df['monthly_capacity']) * 100\n",
    "    \n",
    "    # Calculate number of instances needed\n",
    "    analysis_df['instances_needed'] = np.ceil(monthly_predictions / analysis_df['monthly_capacity'])\n",
    "    analysis_df['total_monthly_cost'] = analysis_df['monthly_cost'] * analysis_df['instances_needed']\n",
    "    analysis_df['cost_per_prediction'] = analysis_df['total_monthly_cost'] / monthly_predictions\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    analysis_df['performance_efficiency'] = analysis_df['requests_per_second'] / analysis_df['cost_per_hour']\n",
    "    analysis_df['resource_efficiency'] = (analysis_df['cpu_usage_percent'] + analysis_df['memory_usage_percent']) / 2\n",
    "    \n",
    "    # Overall score (weighted combination)\n",
    "    analysis_df['overall_score'] = (\n",
    "        (1 / analysis_df['cost_per_prediction']) * 0.4 +  # 40% weight on cost efficiency\n",
    "        analysis_df['performance_efficiency'] * 0.3 +     # 30% weight on performance\n",
    "        analysis_df['success_rate'] * 0.3                  # 30% weight on reliability\n",
    "    )\n",
    "    \n",
    "    return analysis_df.sort_values('overall_score', ascending=False)\n",
    "\n",
    "# Perform analysis for different scenarios\n",
    "scenarios = [\n",
    "    {\"name\": \"Small Scale\", \"daily_predictions\": 10000, \"description\": \"10K predictions/day (startup)\"},\n",
    "    {\"name\": \"Medium Scale\", \"daily_predictions\": 100000, \"description\": \"100K predictions/day (growing company)\"},\n",
    "    {\"name\": \"Large Scale\", \"daily_predictions\": 1000000, \"description\": \"1M predictions/day (enterprise)\"}\n",
    "]\n",
    "\n",
    "cost_analysis_results = {}\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"üí∞ Analyzing {scenario['name']} Scenario: {scenario['description']}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    analysis = perform_cost_benefit_analysis(benchmark_df, \n",
    "                                           daily_predictions=scenario['daily_predictions'])\n",
    "    cost_analysis_results[scenario['name']] = analysis\n",
    "    \n",
    "    # Show top 3 recommendations\n",
    "    top_3 = analysis.head(3)[['instance_type', 'instances_needed', 'total_monthly_cost', \n",
    "                              'cost_per_prediction', 'capacity_utilization', 'overall_score']]\n",
    "    \n",
    "    print(f\"üèÜ Top 3 Recommendations for {scenario['daily_predictions']:,} daily predictions:\")\n",
    "    for idx, row in top_3.iterrows():\n",
    "        print(f\"  {idx+1}. {row['instance_type']}\")\n",
    "        print(f\"     ‚Ä¢ Instances needed: {int(row['instances_needed'])}\")\n",
    "        print(f\"     ‚Ä¢ Monthly cost: ${row['total_monthly_cost']:,.2f}\")\n",
    "        print(f\"     ‚Ä¢ Cost per prediction: ${row['cost_per_prediction']:.6f}\")\n",
    "        print(f\"     ‚Ä¢ Capacity utilization: {row['capacity_utilization']:.1f}%\")\n",
    "        print(f\"     ‚Ä¢ Overall score: {row['overall_score']:.2f}\")\n",
    "        print()\n",
    "\n",
    "# Comparative analysis visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Cost-Benefit Analysis: Different Scale Scenarios', fontsize=16, fontweight='bold')\n",
    "\n",
    "scenario_colors = ['blue', 'green', 'red']\n",
    "\n",
    "for idx, (scenario_name, analysis) in enumerate(cost_analysis_results.items()):\n",
    "    color = scenario_colors[idx]\n",
    "    \n",
    "    # Cost per prediction\n",
    "    axes[0, 0].bar(np.arange(len(analysis)) + idx*0.25, analysis['cost_per_prediction'], \n",
    "                   width=0.25, label=scenario_name, color=color, alpha=0.7)\n",
    "    \n",
    "    # Total monthly cost\n",
    "    axes[0, 1].bar(np.arange(len(analysis)) + idx*0.25, analysis['total_monthly_cost'], \n",
    "                   width=0.25, label=scenario_name, color=color, alpha=0.7)\n",
    "    \n",
    "    # Instances needed\n",
    "    axes[0, 2].bar(np.arange(len(analysis)) + idx*0.25, analysis['instances_needed'], \n",
    "                   width=0.25, label=scenario_name, color=color, alpha=0.7)\n",
    "    \n",
    "    # Capacity utilization\n",
    "    axes[1, 0].scatter(analysis['capacity_utilization'], analysis['cost_per_prediction'], \n",
    "                       label=scenario_name, color=color, s=100, alpha=0.7)\n",
    "    \n",
    "    # Performance efficiency\n",
    "    axes[1, 1].bar(np.arange(len(analysis)) + idx*0.25, analysis['performance_efficiency'], \n",
    "                   width=0.25, label=scenario_name, color=color, alpha=0.7)\n",
    "    \n",
    "    # Overall score\n",
    "    axes[1, 2].bar(np.arange(len(analysis)) + idx*0.25, analysis['overall_score'], \n",
    "                   width=0.25, label=scenario_name, color=color, alpha=0.7)\n",
    "\n",
    "# Set labels and formatting\n",
    "axes[0, 0].set_title('Cost per Prediction by Instance')\n",
    "axes[0, 0].set_ylabel('Cost ($)')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].set_xticks(np.arange(len(analysis)))\n",
    "axes[0, 0].set_xticklabels(analysis['instance_type'], rotation=45, ha='right')\n",
    "\n",
    "axes[0, 1].set_title('Total Monthly Cost')\n",
    "axes[0, 1].set_ylabel('Cost ($)')\n",
    "axes[0, 1].set_xticks(np.arange(len(analysis)))\n",
    "axes[0, 1].set_xticklabels(analysis['instance_type'], rotation=45, ha='right')\n",
    "\n",
    "axes[0, 2].set_title('Instances Needed')\n",
    "axes[0, 2].set_ylabel('Number of Instances')\n",
    "axes[0, 2].set_xticks(np.arange(len(analysis)))\n",
    "axes[0, 2].set_xticklabels(analysis['instance_type'], rotation=45, ha='right')\n",
    "\n",
    "axes[1, 0].set_title('Capacity Utilization vs Cost')\n",
    "axes[1, 0].set_xlabel('Capacity Utilization (%)')\n",
    "axes[1, 0].set_ylabel('Cost per Prediction ($)')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_title('Performance Efficiency')\n",
    "axes[1, 1].set_ylabel('RPS per Dollar')\n",
    "axes[1, 1].set_xticks(np.arange(len(analysis)))\n",
    "axes[1, 1].set_xticklabels(analysis['instance_type'], rotation=45, ha='right')\n",
    "\n",
    "axes[1, 2].set_title('Overall Recommendation Score')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].set_xticks(np.arange(len(analysis)))\n",
    "axes[1, 2].set_xticklabels(analysis['instance_type'], rotation=45, ha='right')\n",
    "\n",
    "# Add legends\n",
    "for ax in [axes[0, 0], axes[0, 1], axes[0, 2], axes[1, 1], axes[1, 2]]:\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"üí° Cost-Benefit Analysis Insights:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Best overall performers\n",
    "for scenario_name, analysis in cost_analysis_results.items():\n",
    "    best = analysis.iloc[0]\n",
    "    print(f\"üèÜ {scenario_name}: Best choice is {best['instance_type']}\")\n",
    "    print(f\"   ‚Ä¢ Cost per prediction: ${best['cost_per_prediction']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Instances needed: {int(best['instances_needed'])}\")\n",
    "    print(f\"   ‚Ä¢ Monthly cost: ${best['total_monthly_cost']:,.2f}\")\n",
    "    print(f\"   ‚Ä¢ Capacity utilization: {best['capacity_utilization']:.1f}%\")\n",
    "    print()\n",
    "\n",
    "# GPU vs CPU comparison\n",
    "medium_scale = cost_analysis_results['Medium Scale']\n",
    "gpu_instances = medium_scale[medium_scale['instance_type'].str.contains('p3|g4dn')]\n",
    "cpu_instances = medium_scale[~medium_scale['instance_type'].str.contains('p3|g4dn')]\n",
    "\n",
    "if len(gpu_instances) > 0 and len(cpu_instances) > 0:\n",
    "    print(\"üéÆ GPU vs CPU Comparison (Medium Scale):\")\n",
    "    print(f\"   ‚Ä¢ Best GPU: {gpu_instances.iloc[0]['instance_type']} (${gpu_instances.iloc[0]['cost_per_prediction']:.6f}/pred)\")\n",
    "    print(f\"   ‚Ä¢ Best CPU: {cpu_instances.iloc[0]['instance_type']} (${cpu_instances.iloc[0]['cost_per_prediction']:.6f}/pred)\")\n",
    "    \n",
    "    gpu_cpu_ratio = gpu_instances.iloc[0]['cost_per_prediction'] / cpu_instances.iloc[0]['cost_per_prediction']\n",
    "    print(f\"   ‚Ä¢ GPU cost ratio vs CPU: {gpu_cpu_ratio:.2f}x\")\n",
    "    \n",
    "    if gpu_cpu_ratio < 1:\n",
    "        print(\"   ‚úÖ GPUs are more cost-effective for this workload\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ CPUs are more cost-effective for this workload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Automated Testing\n",
    "\n",
    "We can integrate automated tests directly into our notebooks using `pytest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test file\n",
    "test_code = \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "def test_data_generation():\n",
    "    # Test that the sample data can be generated successfully\n",
    "    df = generate_sample_benchmark_data()\n",
    "    assert isinstance(df, pd.DataFrame), \\\"Should be a pandas DataFrame\\\"\n",
    "    assert not df.empty, \\\"DataFrame should not be empty\\\"\n",
    "\n",
    "def test_cost_analysis():\n",
    "    # Test that the cost analysis runs without errors\n",
    "    df = generate_sample_benchmark_data()\n",
    "    analysis = perform_cost_benefit_analysis(df)\n",
    "    assert isinstance(analysis, pd.DataFrame), \\\"Should be a pandas DataFrame\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Recommendations Engine\n",
    "\n",
    "Let's create an intelligent recommendation system that suggests optimal infrastructure based on requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intelligent recommendation engine\n",
    "def recommend_instances(requirements):\n",
    "    \"\"\"\n",
    "    Recommend optimal instances based on requirements.\n",
    "    \n",
    "    Args:\n",
    "        requirements: Dict with keys:\n",
    "            - daily_predictions: int\n",
    "            - max_latency_ms: float\n",
    "            - max_cost_per_prediction: float\n",
    "            - reliability_target: float (0-1)\n",
    "            - prefer_gpu: bool\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter instances based on requirements\n",
    "    filtered_df = benchmark_df.copy()\n",
    "    \n",
    "    # Filter by latency requirement\n",
    "    if requirements.get('max_latency_ms'):\n",
    "        filtered_df = filtered_df[filtered_df['avg_latency_ms'] <= requirements['max_latency_ms']]\n",
    "    \n",
    "    # Filter by cost requirement\n",
    "    if requirements.get('max_cost_per_prediction'):\n",
    "        filtered_df = filtered_df[filtered_df['cost_per_1000_predictions'] <= requirements['max_cost_per_prediction'] * 1000]\n",
    "    \n",
    "    # Filter by reliability requirement\n",
    "    if requirements.get('reliability_target'):\n",
    "        filtered_df = filtered_df[filtered_df['success_rate'] >= requirements['reliability_target']]\n",
    "    \n",
    "    # Filter by GPU preference\n",
    "    if requirements.get('prefer_gpu') is True:\n",
    "        filtered_df = filtered_df[filtered_df['instance_type'].str.contains('p3|g4dn')]\n",
    "    elif requirements.get('prefer_gpu') is False:\n",
    "        filtered_df = filtered_df[~filtered_df['instance_type'].str.contains('p3|g4dn')]\n",
    "    \n",
    "    if len(filtered_df) == 0:\n",
    "        return {\"error\": \"No instances meet the specified requirements\", \"requirements\": requirements}\n",
    "    \n",
    "    # Score remaining instances\n",
    "    scored_df = filtered_df.groupby('instance_type').agg({\n",
    "        'requests_per_second': 'mean',\n",
    "        'avg_latency_ms': 'mean',\n",
    "        'cost_per_1000_predictions': 'mean',\n",
    "        'success_rate': 'mean',\n",
    "        'cost_per_hour': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate capacity and cost for requirements\n",
    "    daily_predictions = requirements.get('daily_predictions', 100000)\n",
    "    monthly_predictions = daily_predictions * 30\n",
    "    hours_per_month = 24 * 30\n",
    "    \n",
    "    scored_df['monthly_capacity'] = scored_df['requests_per_second'] * 3600 * hours_per_month\n",
    "    scored_df['instances_needed'] = np.ceil(monthly_predictions / scored_df['monthly_capacity'])\n",
    "    scored_df['total_monthly_cost'] = scored_df['cost_per_hour'] * hours_per_month * scored_df['instances_needed']\n",
    "    scored_df['capacity_utilization'] = (monthly_predictions / scored_df['monthly_capacity']) * 100\n",
    "    \n",
    "    # Calculate recommendation score\n",
    "    scored_df['recommendation_score'] = (\n",
    "        (1 / scored_df['cost_per_1000_predictions']) * 0.4 +  # Cost efficiency\n",
    "        (scored_df['requests_per_second'] / scored_df['cost_per_hour']) * 0.3 +  # Performance per dollar\n",
    "        scored_df['success_rate'] * 0.3  # Reliability\n",
    "    )\n",
    "    \n",
    "    # Sort by recommendation score\n",
    "    recommendations = scored_df.sort_values('recommendation_score', ascending=False)\n",
    "    \n",
    "    # Return top recommendations\n",
    "    top_recommendations = []\n",
    "    for _, row in recommendations.head(3).iterrows():\n",
    "        top_recommendations.append({\n",
    "            \"instance_type\": row['instance_type'],\n",
    "            \"instances_needed\": int(row['instances_needed']),\n",
    "            \"total_monthly_cost\": round(row['total_monthly_cost'], 2),\n",
    "            \"cost_per_prediction\": round(row['total_monthly_cost'] / monthly_predictions, 6),\n",
    "            \"capacity_utilization\": round(row['capacity_utilization'], 1),\n",
    "            \"avg_latency_ms\": round(row['avg_latency_ms'], 1),\n",
    "            \"success_rate\": round(row['success_rate'], 3),\n",
    "            \"recommendation_score\": round(row['recommendation_score'], 2)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"requirements\": requirements,\n",
    "        \"recommendations\": top_recommendations,\n",
    "        \"total_candidates\": len(recommendations)\n",
    "    }\n",
    "\n",
    "# Test the recommendation engine with different scenarios\n",
    "recommendation_scenarios = [\n",
    "    {\n",
    "        \"name\": \"Budget-Conscious Startup\",\n",
    "        \"requirements\": {\n",
    "            \"daily_predictions\": 50000,\n",
    "            \"max_latency_ms\": 100,\n",
    "            \"max_cost_per_prediction\": 0.0001,\n",
    "            \"reliability_target\": 0.95,\n",
    "            \"prefer_gpu\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Performance-Focused Enterprise\",\n",
    "        \"requirements\": {\n",
    "            \"daily_predictions\": 500000,\n",
    "            \"max_latency_ms\": 50,\n",
    "            \"max_cost_per_prediction\": 0.001,\n",
    "            \"reliability_target\": 0.99,\n",
    "            \"prefer_gpu\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Medium Business\",\n",
    "        \"requirements\": {\n",
    "            \"daily_predictions\": 200000,\n",
    "            \"max_latency_ms\": 75,\n",
    "            \"max_cost_per_prediction\": 0.0005,\n",
    "            \"reliability_target\": 0.97,\n",
    "            \"prefer_gpu\": None  # No preference\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üéØ Intelligent Infrastructure Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for scenario in recommendation_scenarios:\n",
    "    print(f\"\\nüè¢ Scenario: {scenario['name']}\")\n",
    "    print(f\"üìä Requirements: {scenario['requirements']['daily_predictions']:,} daily predictions\")\n",
    "    print(f\"   ‚Ä¢ Max latency: {scenario['requirements']['max_latency_ms']}ms\")\n",
    "    print(f\"   ‚Ä¢ Max cost/prediction: ${scenario['requirements']['max_cost_per_prediction']:.6f}\")\n",
    "    print(f\"   ‚Ä¢ Reliability target: {scenario['requirements']['reliability_target']:.1%}\")\n",
    "    \n",
    "    if scenario['requirements'].get('prefer_gpu') is True:\n",
    "        print(\"   ‚Ä¢ GPU preferred: Yes\")\n",
    "    elif scenario['requirements'].get('prefer_gpu') is False:\n",
    "        print(\"   ‚Ä¢ GPU preferred: No\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ GPU preferred: No preference\")\n",
    "    \n",
    "    recommendation = recommend_instances(scenario['requirements'])\n",
    "    \n",
    "    if \"error\" in recommendation:\n",
    "        print(f\"‚ùå {recommendation['error']}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüèÜ Top Recommendations ({recommendation['total_candidates']} candidates considered):\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendation['recommendations'][:2], 1):\n",
    "        print(f\"\\n   {i}. {rec['instance_type']}\")\n",
    "        print(f\"      ‚Ä¢ Instances needed: {rec['instances_needed']}\")\n",
    "        print(f\"      ‚Ä¢ Monthly cost: ${rec['total_monthly_cost']:,.2f}\")\n",
    "        print(f\"      ‚Ä¢ Cost per prediction: ${rec['cost_per_prediction']:.6f}\")\n",
    "        print(f\"      ‚Ä¢ Capacity utilization: {rec['capacity_utilization']:.1f}%\")\n",
    "        print(f\"      ‚Ä¢ Average latency: {rec['avg_latency_ms']:.1f}ms\")\n",
    "        print(f\"      ‚Ä¢ Success rate: {rec['success_rate']:.1%}\")\n",
    "        print(f\"      ‚Ä¢ Recommendation score: {rec['recommendation_score']:.2f}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Interactive recommendation tool\n",
    "def interactive_recommendation():\n",
    "    \"\"\"Interactive recommendation tool.\"\"\"\n",
    "    print(\"\\nüéÆ Interactive Recommendation Tool\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        daily_preds = int(input(\"Daily predictions (e.g., 100000): \") or \"100000\")\n",
    "        max_latency = float(input(\"Max latency in ms (e.g., 100): \") or \"100\")\n",
    "        max_cost = float(input(\"Max cost per prediction (e.g., 0.0001): \") or \"0.0001\")\n",
    "        reliability = float(input(\"Reliability target (0-1, e.g., 0.95): \") or \"0.95\")\n",
    "        \n",
    "        gpu_pref_input = input(\"Prefer GPU? (yes/no/auto): \").lower().strip()\n",
    "        if gpu_pref_input == \"yes\":\n",
    "            prefer_gpu = True\n",
    "        elif gpu_pref_input == \"no\":\n",
    "            prefer_gpu = False\n",
    "        else:\n",
    "            prefer_gpu = None\n",
    "        \n",
    "        requirements = {\n",
    "            \"daily_predictions\": daily_preds,\n",
    "            \"max_latency_ms\": max_latency,\n",
    "            \"max_cost_per_prediction\": max_cost,\n",
    "            \"reliability_target\": reliability,\n",
    "            \"prefer_gpu\": prefer_gpu\n",
    "        }\n",
    "        \n",
    "        result = recommend_instances(requirements)\n",
    "        \n",
    "        if \"error\" in result:\n",
    "            print(f\"‚ùå {result['error']}\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\nüèÜ Recommendation for your requirements:\")\n",
    "        best = result['recommendations'][0]\n",
    "        print(f\"   Instance: {best['instance_type']}\")\n",
    "        print(f\"   Instances needed: {best['instances_needed']}\")\n",
    "        print(f\"   Monthly cost: ${best['total_monthly_cost']:,.2f}\")\n",
    "        print(f\"   Cost per prediction: ${best['cost_per_prediction']:.6f}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Exiting interactive mode\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")\n",
    "        print(\"üí° Try using the predefined scenarios above\")\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_recommendation()\n",
    "\n",
    "print(\"\\nüí° To use the interactive recommendation tool, uncomment the last line above!\")\n",
    "print(\"\\nüéâ Benchmarking Analysis Complete!\")\n",
    "print(\"\\nüìã Key Takeaways:\")\n",
    "print(\"   ‚Ä¢ Use the recommendation engine to find optimal infrastructure\")\n",
    "print(\"   ‚Ä¢ Consider both performance and cost when choosing instances\")\n",
    "print(\"   ‚Ä¢ GPU instances excel at high-throughput scenarios\")\n",
    "print(\"   ‚Ä¢ CPU instances are often more cost-effective for moderate loads\")\n",
    "print(\"   ‚Ä¢ Monitor capacity utilization to optimize resource usage\")\n",
    "print(\"\\nüöÄ Next: Explore monitoring metrics and alerting in the next notebook!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
