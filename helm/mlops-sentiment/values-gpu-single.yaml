# GPU Configuration Example: Single T4 GPU for Production
# Use this for medium-scale deployments (500-2000 req/s)
#
# Deploy with:
#   helm upgrade --install mlops-sentiment ./helm/mlops-sentiment \
#     -f values.yaml \
#     -f values-gpu-single.yaml \
#     --namespace mlops-prod

# Enable GPU support
gpu:
  enabled: true
  count: 1
  vendor: nvidia
  type: nvidia-tesla-t4

  # GPU resources
  resources:
    requests:
      cpu: 2000m
      memory: 8Gi
      nvidia.com/gpu: 1
    limits:
      cpu: 4000m
      memory: 16Gi
      nvidia.com/gpu: 1

  # Optimized environment for single GPU
  env:
    # CUDA settings
    CUDA_VISIBLE_DEVICES: "0"
    NVIDIA_VISIBLE_DEVICES: "all"
    NVIDIA_DRIVER_CAPABILITIES: "compute,utility"

    # GPU memory management
    GPU_MEMORY_FRACTION: "0.9"
    PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"

    # Model optimizations
    ENABLE_GPU_OPTIMIZATION: "true"
    ENABLE_TORCH_COMPILE: "false"
    ENABLE_TENSORRT: "false"

    # Batch processing optimized for T4
    GPU_BATCH_SIZE: "64"
    GPU_MAX_BATCH_SIZE: "128"
    GPU_DYNAMIC_BATCHING: "true"

  # Node selector for T4 GPUs
  nodeSelector:
    accelerator: nvidia-tesla-t4

  # GPU tolerations
  tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

  # GPU health checks (longer timeouts)
  healthChecks:
    livenessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 90
      periodSeconds: 20
      timeoutSeconds: 10
      failureThreshold: 5

    readinessProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 60
      periodSeconds: 15
      timeoutSeconds: 8
      failureThreshold: 3

    startupProbe:
      httpGet:
        path: /health
        port: http
      initialDelaySeconds: 30
      periodSeconds: 15
      timeoutSeconds: 10
      failureThreshold: 40

  # GPU-optimized HPA
  hpa:
    enabled: true
    minReplicas: 1
    maxReplicas: 4
    behavior:
      scaleUp:
        stabilizationWindowSeconds: 180
        policies:
        - type: Pods
          value: 1
          periodSeconds: 120
      scaleDown:
        stabilizationWindowSeconds: 600
        policies:
        - type: Pods
          value: 1
          periodSeconds: 180

# Reduce default replicas (GPU is more powerful)
deployment:
  replicaCount: 1

# Larger model cache for GPU
modelPersistence:
  enabled: true
  size: 10Gi

# Enable monitoring
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 15s
