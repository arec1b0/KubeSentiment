---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubesentiment-scalability-config
  namespace: default
  labels:
    app: kubesentiment
    component: config
data:
  # Redis Configuration
  MLOPS_REDIS_ENABLED: "true"
  MLOPS_REDIS_HOST: "redis"
  MLOPS_REDIS_PORT: "6379"
  MLOPS_REDIS_DB: "0"
  MLOPS_REDIS_MAX_CONNECTIONS: "50"
  MLOPS_REDIS_NAMESPACE: "kubesentiment"
  MLOPS_REDIS_PREDICTION_CACHE_TTL: "3600"
  MLOPS_REDIS_FEATURE_CACHE_TTL: "1800"

  # Anomaly Buffer Configuration
  MLOPS_ANOMALY_BUFFER_ENABLED: "true"
  MLOPS_ANOMALY_BUFFER_MAX_SIZE: "10000"
  MLOPS_ANOMALY_BUFFER_DEFAULT_TTL: "3600"
  MLOPS_ANOMALY_BUFFER_CLEANUP_INTERVAL: "300"

  # Distributed Kafka Consumer Configuration
  MLOPS_KAFKA_ENABLED: "true"
  MLOPS_KAFKA_BOOTSTRAP_SERVERS: "kafka:9092"
  MLOPS_KAFKA_CONSUMER_GROUP: "kubesentiment_consumer_group"
  MLOPS_KAFKA_TOPIC: "sentiment_requests"
  MLOPS_KAFKA_PARTITION_ASSIGNMENT_STRATEGY: "roundrobin"
  MLOPS_KAFKA_CONSUMER_THREADS: "4"
  MLOPS_KAFKA_BATCH_SIZE: "200"
  MLOPS_KAFKA_MAX_POLL_RECORDS: "1000"

  # Load Balancing Configuration
  MLOPS_MAX_CONCURRENT_REQUESTS: "100"
  MLOPS_REQUEST_QUEUE_SIZE: "1000"

  # Performance Tuning
  MLOPS_WORKERS: "2"
  MLOPS_PREDICTION_CACHE_MAX_SIZE: "5000"

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kubesentiment-hpa
  namespace: default
  labels:
    app: kubesentiment
    component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kubesentiment
  minReplicas: 2
  maxReplicas: 10
  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom metric: Active requests per pod
    # Requires metrics-server and custom metrics API
    - type: Pods
      pods:
        metric:
          name: sentiment_active_requests
        target:
          type: AverageValue
          averageValue: "50"

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # 5 minutes
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Min

    scaleUp:
      stabilizationWindowSeconds: 60  # 1 minute
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
      selectPolicy: Max

---
# Pod Disruption Budget to ensure high availability during scaling
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: kubesentiment-pdb
  namespace: default
  labels:
    app: kubesentiment
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: kubesentiment
  maxUnavailable: 1

---
# Service with load balancing
apiVersion: v1
kind: Service
metadata:
  name: kubesentiment-lb
  namespace: default
  labels:
    app: kubesentiment
    component: loadbalancer
  annotations:
    # AWS Load Balancer annotations (adjust for your cloud provider)
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    # Health check configuration
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/health"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "10"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "5"
    service.beta.kubernetes.io/aws-load-balancer-healthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-unhealthy-threshold: "2"
spec:
  type: LoadBalancer
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3 hours
  ports:
    - name: http
      port: 80
      targetPort: 8000
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 8000
      protocol: TCP
  selector:
    app: kubesentiment

---
# Network Policy for security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kubesentiment-netpol
  namespace: default
  labels:
    app: kubesentiment
spec:
  podSelector:
    matchLabels:
      app: kubesentiment
  policyTypes:
    - Ingress
    - Egress

  ingress:
    # Allow traffic from ingress controller namespace
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
      ports:
        - protocol: TCP
          port: 8000

    # Allow metrics scraping from Prometheus
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
        - podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 8000

  egress:
    # Allow DNS
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53

    # Allow Redis access
    - to:
        - podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379

    # Allow Kafka access
    - to:
        - podSelector:
            matchLabels:
              app: kafka
      ports:
        - protocol: TCP
          port: 9092

    # Allow external HTTPS (for model downloads, etc.)
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80

---
# Enhanced Deployment with scalability features
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubesentiment
  namespace: default
  labels:
    app: kubesentiment
    version: "1.0.0"
spec:
  replicas: 3  # Initial replicas (HPA will adjust)
  selector:
    matchLabels:
      app: kubesentiment
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 0  # Zero downtime deployments

  template:
    metadata:
      labels:
        app: kubesentiment
        version: "1.0.0"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      # Enable graceful shutdown
      terminationGracePeriodSeconds: 60

      # Node affinity for optimal placement
      affinity:
        # Spread pods across nodes
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - kubesentiment
                topologyKey: kubernetes.io/hostname

        # Prefer nodes with more resources
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 50
              preference:
                matchExpressions:
                  - key: node-type
                    operator: In
                    values:
                      - compute-optimized

      containers:
        - name: kubesentiment
          image: kubesentiment:latest
          imagePullPolicy: IfNotPresent

          ports:
            - containerPort: 8000
              name: http
              protocol: TCP

          env:
            # Pod information for distributed coordination
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName

          envFrom:
            - configMapRef:
                name: kubesentiment-scalability-config

          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: 2000m
              memory: 4Gi

          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /health/details
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 3
            successThreshold: 1
            failureThreshold: 3

          # Lifecycle hooks for graceful shutdown
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - sleep 15  # Give time for load balancer to deregister

          volumeMounts:
            - name: tmp
              mountPath: /tmp
            - name: cache
              mountPath: /app/.cache

      volumes:
        - name: tmp
          emptyDir: {}
        - name: cache
          emptyDir:
            sizeLimit: 1Gi

      # Security context
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
